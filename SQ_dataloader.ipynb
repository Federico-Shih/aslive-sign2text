{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP6GLo+h+jFZqwghZzwuPZ3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"nQXS6ftuSDEP"},"outputs":[],"source":["!pip install torchcodec"]},{"cell_type":"code","source":["# Mount Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"bHBfhK7CSMGA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import kagglehub\n","\n","# Download latest version\n","path = kagglehub.dataset_download(\"sttaseen/wlasl2000-resized\")\n","\n","print(\"Path to dataset files:\", path)"],"metadata":{"id":"_4D-84lOSg6x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!cp -r /root/.cache/kagglehub/datasets/sttaseen/wlasl2000-resized/versions/1 \\\n","      /content/drive/MyDrive/wlasl_resized\n","\n","!rm -rf /root/.cache/kagglehub/datasets/sttaseen/wlasl2000-resized/versions/1"],"metadata":{"id":"rdGDn7J7SmUS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import json\n","import torch\n","from torch.utils.data import Dataset\n","from torchvision import transforms\n","from torchcodec.decoders import VideoDecoder\n","\n","class WLASLTorchCodec(Dataset):\n","  def __init__(self, json_path, video_dir, split=\"train\", num_frames=32, transform=None):\n","    self.video_dir = video_dir\n","    self.num_frames = num_frames\n","    self.transform = transform\n","\n","    # Read json\n","    with open(json_path, \"r\") as f:\n","      data = json.load(f)\n","\n","    self.samples = []\n","    self.label_map = {}\n","    label_id = 0\n","\n","    for entry in data:\n","      gloss = entry[\"gloss\"]\n","\n","      if gloss not in self.label_map:\n","        self.label_map[gloss] = label_id\n","        label_id += 1\n","\n","      label = self.label_map[gloss]\n","\n","      for inst in entry[\"instances\"]:\n","        if inst[\"split\"] != split:\n","          continue\n","\n","        video_id = inst[\"video_id\"]\n","        file_path = os.path.join(video_dir, f\"{video_id}.mp4\")\n","\n","        if os.path.isfile(file_path):\n","          self.samples.append((file_path, label))\n","\n","        # change here\n","        self.num_classes = label_id\n","\n","  def __len__(self):\n","    return len(self.samples)\n","\n","  def _sample_frames(self, frames):\n","    \"\"\"frames is a list of PIL images or Tensors.\"\"\"\n","    T = len(frames)\n","    idx = torch.linspace(0, T - 1, self.num_frames).long()\n","    return [frames[i] for i in idx]\n","\n","  def __getitem__(self, idx):\n","    video_path, label = self.samples[idx]\n","\n","    decoder = VideoDecoder(video_path)\n","\n","    frames = []\n","    for chunk in decoder:\n","      # chunk is a Tensor of frames: (T, H, W) OR (T, H, W, C)\n","      for frame_tensor in chunk:\n","        # Handle grayscale frames (H, W)\n","        if frame_tensor.dim() == 2:\n","          frame_tensor = frame_tensor.unsqueeze(2)  # â†’ (H, W, 1)\n","\n","        if frame_tensor.shape[2] == 1:\n","                frame_tensor = frame_tensor.repeat(1, 1, 3)\n","\n","        # Handle RGB frames (H, W, C)\n","        ''''''\n","        if frame_tensor.dim() == 3:\n","          pass\n","        else:\n","          raise ValueError(f\"Unexpected frame shape: {frame_tensor.shape}\")\n","\n","\n","        # Convert to C x H x W\n","        frame_chw = frame_tensor.permute(2, 0, 1)\n","        # change added here\n","        frame_pil = transforms.ToPILImage()(frame_chw).convert(\"RGB\")\n","        frames.append(frame_pil)\n","\n","    # Guard for short videos\n","    if len(frames) < self.num_frames:\n","        while len(frames) < self.num_frames:\n","            frames.extend(frames)\n","        frames = frames[: self.num_frames]\n","\n","    # Uniform sampling\n","    frames = self._sample_frames(frames)\n","\n","    if self.transform:\n","      frames = torch.stack([self.transform(f) for f in frames])\n","    else:\n","      frames = torch.stack([transforms.ToTensor()(f) for f in frames])\n","\n","    return frames, label"],"metadata":{"id":"1rtrMYAFSP7z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Path to json file in drive\n","json_path = \"/content/drive/My Drive/Colab Notebooks/WLASL_v0.3.json\"\n","\n","with open(json_path, \"r\") as f:\n","  data = json.load(f)\n","\n","if \"root\" in data:\n","  data = data[\"root\"]\n","\n","# Create dataset\n","dataset = WLASLTorchCodec(\n","  json_path=json_path,\n","  video_dir=\"/content/drive/My Drive/Colab Notebooks/videos\",\n","  split=\"train\",\n","  num_frames=8\n",")\n","\n","# Test a sample\n","frames, label = dataset[0]\n","print(\"Frames shape:\", frames.shape)\n","print(\"Label:\", label)"],"metadata":{"id":"7fPED5SoScda"},"execution_count":null,"outputs":[]}]}