{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Luwl8hSq-lmd"
      },
      "source": [
        "# ASLive Sign2Text Model\n",
        "\n",
        "This notebook implements the sign language to text model following the architecture:\n",
        "- **Vision Layer (CNN)**: Extracts spatial features from each frame\n",
        "- **Positional Encoding (PE)**: Adds temporal position information\n",
        "- **Attention Layer (LSTM)**: Processes temporal sequence with attention\n",
        "- **FC Layer**: Final classification layer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kagglehub torchcodec torchvision\n",
        "!pip install git+https://github.com/facebookresearch/pytorchvideo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbsKbY7bA16g",
        "outputId": "96dc5dfc-a8a1-4dd8-ff8a-fa50af727e50"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.12/dist-packages (0.3.13)\n",
            "Requirement already satisfied: torchcodec in /usr/local/lib/python3.12/dist-packages (0.9.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kagglehub) (25.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from kagglehub) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kagglehub) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: torch==2.9.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.9.0+cu126)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.5.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.9.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.9.0->torchvision) (3.0.3)\n",
            "Collecting git+https://github.com/facebookresearch/pytorchvideo\n",
            "  Cloning https://github.com/facebookresearch/pytorchvideo to /tmp/pip-req-build-3rvnr5br\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/pytorchvideo /tmp/pip-req-build-3rvnr5br\n",
            "  Resolved https://github.com/facebookresearch/pytorchvideo to commit 0f9a5e102e4d84972b829fd30e3c3f78c7c7fd1a\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: fvcore in /usr/local/lib/python3.12/dist-packages (from pytorchvideo==0.1.5) (0.1.5.post20221221)\n",
            "Requirement already satisfied: av in /usr/local/lib/python3.12/dist-packages (from pytorchvideo==0.1.5) (16.0.1)\n",
            "Requirement already satisfied: parameterized in /usr/local/lib/python3.12/dist-packages (from pytorchvideo==0.1.5) (0.9.0)\n",
            "Requirement already satisfied: iopath in /usr/local/lib/python3.12/dist-packages (from pytorchvideo==0.1.5) (0.1.10)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from pytorchvideo==0.1.5) (3.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from fvcore->pytorchvideo==0.1.5) (2.0.2)\n",
            "Requirement already satisfied: yacs>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from fvcore->pytorchvideo==0.1.5) (0.1.8)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from fvcore->pytorchvideo==0.1.5) (6.0.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from fvcore->pytorchvideo==0.1.5) (4.67.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.12/dist-packages (from fvcore->pytorchvideo==0.1.5) (3.2.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from fvcore->pytorchvideo==0.1.5) (11.3.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from fvcore->pytorchvideo==0.1.5) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from iopath->pytorchvideo==0.1.5) (4.15.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.12/dist-packages (from iopath->pytorchvideo==0.1.5) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Before running, add everything from SQ_dataloader.ipynb into the cell below and run"
      ],
      "metadata": {
        "id": "8q1A9uXSU7qG"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtCIKfOn-lmh"
      },
      "source": [
        "## 1. Data Loading (from SQ_dataloader)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#add SQ_dataloader code here\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from torchcodec.decoders import VideoDecoder\n",
        "import kagglehub\n",
        "import os\n",
        "import json\n",
        "\n",
        "import torch # Assuming torch is imported elsewhere\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from torchcodec.decoders import VideoDecoder\n",
        "import kagglehub\n",
        "import os\n",
        "import json\n",
        "from PIL import Image # Needed for cropping if working with PIL images\n",
        "\n",
        "class WLASLTorchCodec(Dataset):\n",
        "  download_path = None\n",
        "\n",
        "  def __init__(self, json_path=None, video_dir=None, download=True, max_classes=None, split=\"train\", num_frames=32, transform=None):\n",
        "    print(\"Will download:\", download)\n",
        "    if (json_path is None or video_dir is None) and download == False:\n",
        "      raise ValueError(\"json_path and video_dir must be provided with download false\")\n",
        "    if download:\n",
        "      if WLASLTorchCodec.download_path is None:\n",
        "        path = kagglehub.dataset_download(\"sttaseen/wlasl2000-resized\")\n",
        "        WLASLTorchCodec.download_path = path\n",
        "      else:\n",
        "        path = WLASLTorchCodec.download_path\n",
        "      print(\"Downloaded at path: \", path)\n",
        "\n",
        "      self.video_dir = os.path.join(path, \"wlasl-complete\", \"videos\")\n",
        "      json_path = os.path.join(path, \"wlasl-complete\",\"WLASL_v0.3.json\")\n",
        "      downloaded = True\n",
        "    else:\n",
        "      self.video_dir = video_dir\n",
        "    self.num_frames = num_frames\n",
        "    self.transform = transform\n",
        "\n",
        "    # Read json\n",
        "    with open(json_path, \"r\") as f:\n",
        "      data = json.load(f)\n",
        "    if max_classes is not None:\n",
        "        if isinstance(max_classes, int):\n",
        "            # Keep only the first N entries (Usually the most frequent in WLASL)\n",
        "            data = data[:max_classes]\n",
        "            print(f\"Limiting dataset to top {max_classes} classes.\")\n",
        "        elif isinstance(max_classes, list):\n",
        "            # Keep only entries that match specific glosses\n",
        "            data = [entry for entry in data if entry['gloss'] in max_classes]\n",
        "            print(f\"Limiting dataset to {len(data)} specific classes.\")\n",
        "    self.samples = []\n",
        "    self.label_map = {}\n",
        "    label_id = 0\n",
        "\n",
        "    for entry in data:\n",
        "      gloss = entry[\"gloss\"]\n",
        "\n",
        "      if gloss not in self.label_map:\n",
        "        self.label_map[gloss] = label_id\n",
        "        label_id += 1\n",
        "\n",
        "      label = self.label_map[gloss]\n",
        "\n",
        "      for inst in entry[\"instances\"]:\n",
        "        if inst[\"split\"] != split:\n",
        "          continue\n",
        "\n",
        "        video_id = inst[\"video_id\"]\n",
        "        file_path = os.path.join(self.video_dir, f\"{video_id}.mp4\")\n",
        "\n",
        "        # 1. Modification in __init__: Extract and store frame/bbox info\n",
        "        frame_start = inst.get(\"frame_start\", 1) # Default to 1 if missing\n",
        "        frame_end = inst.get(\"frame_end\", -1)   # Default to -1 if missing\n",
        "        bbox = inst.get(\"bbox\", [0, 0, 1.0, 1.0]) # Default to normalized full frame if missing\n",
        "\n",
        "        if os.path.isfile(file_path):\n",
        "          # Store a tuple of (file_path, label, frame_start, frame_end, bbox)\n",
        "          self.samples.append((file_path, label, frame_start, frame_end, bbox))\n",
        "        self.num_classes = label_id\n",
        "  def __len__(self):\n",
        "    return len(self.samples)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # 2. Modification in __getitem__: Unpack all instance info\n",
        "    video_path, label, frame_start, frame_end, bbox = self.samples[idx]\n",
        "\n",
        "    # Convert WLASL 1-based indices (inclusive start, exclusive end) to\n",
        "    # torchcodec's 0-based indices (inclusive start, inclusive end).\n",
        "\n",
        "    decoder = VideoDecoder(video_path)\n",
        "    video_length = decoder.metadata.num_frames\n",
        "    end_frame = frame_end - 1 if frame_end > 0 else video_length\n",
        "    start_frame = 0\n",
        "    if end_frame > video_length:\n",
        "      end_frame = video_length\n",
        "    else:\n",
        "      end_frame = frame_end - 2 if frame_end > 0 else None\n",
        "    if frame_start > video_length:\n",
        "      start_frame = 0\n",
        "    else:\n",
        "      start_frame = frame_start - 1\n",
        "    frames = decoder[start_frame:end_frame]\n",
        "    if self.transform:\n",
        "      # Transform should handle T x C x H x W input\n",
        "      frames = self.transform(frames)\n",
        "    return frames, torch.tensor(label) # Ensure label is a tensor"
      ],
      "metadata": {
        "id": "vqaVkQmlVJE0"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorchvideo.transforms as ptv_transforms\n",
        "from torchvision.transforms import Compose, Lambda\n",
        "\n",
        "\n",
        "mean = [0.45, 0.45, 0.45]\n",
        "std = [0.225, 0.225, 0.225]\n",
        "\n",
        "# Test out dataset\n",
        "train_transform = Compose(\n",
        "    [\n",
        "        # 1. Spatial Resize: Scale the shortest edge to SIDE_SIZE\n",
        "        ptv_transforms.UniformTemporalSubsample(num_samples=24, temporal_dim=0),\n",
        "        ptv_transforms.ConvertUint8ToFloat(),\n",
        "        Lambda(lambda x: x.permute(1, 0, 2, 3)),\n",
        "        ptv_transforms.Normalize(mean, std),\n",
        "        Lambda(lambda x: x.permute(1, 0, 2, 3)),\n",
        "        ptv_transforms.ShortSideScale(size=224),\n",
        "        # ptv_transforms.RandAugment(magnitude=6, num_layers=2),\n",
        "        # ptv_transforms.AugMix(magnitude=3),\n",
        "    ]\n",
        ")\n",
        "\n",
        "def show_frame(video, frame_idx):\n",
        "  import matplotlib.pyplot as plt\n",
        "  import numpy as np\n",
        "  single_frame = video[frame_idx]\n",
        "  frame_np = single_frame.detach().cpu().numpy()\n",
        "\n",
        "  frame_np = np.transpose(frame_np, (1, 2, 0))\n",
        "  plt.figure(figsize=(5, 5))\n",
        "  plt.imshow(frame_np)\n",
        "  plt.title(f'Frame {frame_idx} from Video Batch')\n",
        "  plt.axis('off') # Hide axis ticks and labels\n",
        "  plt.show()\n",
        "\n",
        "# clip = WLASLTorchCodec(max_classes=1, transform=train_transform)\n",
        "\n",
        "# for video, label in clip:\n",
        "#   show_frame(video, 0)"
      ],
      "metadata": {
        "id": "wK-FFUttRVGu"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsdOaDCZ-lmh",
        "outputId": "ddbb9215-66e6-4798-bd4e-3f91e4cacbde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from torchcodec.decoders import VideoDecoder\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tfGCzOb-lmi"
      },
      "source": [
        "## 2. Vision Layer (CNN Backbone)\n",
        "\n",
        "The Vision Layer extracts spatial features from each video frame using a CNN. We use a pretrained ResNet-18 as the backbone and remove the final classification layer to get feature embeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "jkESoYf_-lmi"
      },
      "outputs": [],
      "source": [
        "class VisionLayer(nn.Module):\n",
        "    \"\"\"CNN backbone for extracting spatial features from video frames.\n",
        "\n",
        "    Uses pretrained ResNet-18 as feature extractor.\n",
        "    Input: (batch, T, C, H, W) - batch of T frames\n",
        "    Output: (batch, T, feature_dim) - feature vectors for each frame\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, feature_dim=512, pretrained=True, freeze_backbone=False):\n",
        "        super(VisionLayer, self).__init__()\n",
        "\n",
        "        # Load pretrained ResNet-18\n",
        "        resnet = models.resnet18(weights='IMAGENET1K_V1' if pretrained else None)\n",
        "\n",
        "        # Remove the final FC layer\n",
        "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
        "\n",
        "        # ResNet-18 outputs 512-dim features\n",
        "        self.resnet_feature_dim = 512\n",
        "\n",
        "        # Optional projection layer to adjust feature dimension\n",
        "        if feature_dim != self.resnet_feature_dim:\n",
        "            self.projection = nn.Linear(self.resnet_feature_dim, feature_dim)\n",
        "        else:\n",
        "            self.projection = None\n",
        "\n",
        "        self.feature_dim = feature_dim\n",
        "\n",
        "        # Freeze backbone if specified\n",
        "        self.set_freeze_backbone(freeze_backbone)\n",
        "\n",
        "    def set_freeze_backbone(self, is_frozen):\n",
        "      for param in self.backbone.parameters():\n",
        "          param.requires_grad = not is_frozen\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch, T, C, H, W)\n",
        "        Returns:\n",
        "            Feature tensor of shape (batch, T, feature_dim)\n",
        "        \"\"\"\n",
        "        batch_size, T, C, H, W = x.shape\n",
        "\n",
        "        # Reshape to process all frames together: (batch * T, C, H, W)\n",
        "        x = x.view(batch_size * T, C, H, W)\n",
        "\n",
        "        # Extract features: (batch * T, 512, 1, 1)\n",
        "        features = self.backbone(x)\n",
        "\n",
        "        # Flatten: (batch * T, 512)\n",
        "        features = features.view(batch_size * T, -1)\n",
        "\n",
        "        # Project features if needed\n",
        "        if self.projection is not None:\n",
        "            features = self.projection(features)\n",
        "\n",
        "        # Reshape back: (batch, T, feature_dim)\n",
        "        features = features.view(batch_size, T, self.feature_dim)\n",
        "\n",
        "        return features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIkvk_iX-lmj"
      },
      "source": [
        "## 3. Positional Encoding (PE)\n",
        "\n",
        "Sinusoidal positional encoding adds temporal position information to the frame features before feeding them to the LSTM.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "cfVsdetA-lmj"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Sinusoidal positional encoding for temporal sequences.\n",
        "\n",
        "    Adds position information to help the model understand the order of frames.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, max_len=500, dropout=0.1):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Create positional encoding matrix\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Add batch dimension: (1, max_len, d_model)\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        # Register as buffer (not a parameter, but should be saved/loaded)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch, T, d_model)\n",
        "        Returns:\n",
        "            Tensor with positional encoding added: (batch, T, d_model)\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVq1WNB1-lmj"
      },
      "source": [
        "## 4. Attention Layer (LSTM with Attention)\n",
        "\n",
        "Bidirectional LSTM processes the sequence of frame features, followed by an attention mechanism to weight the importance of different time steps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "HCrnK27J-lmj"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"Attention mechanism for weighting LSTM outputs.\n",
        "\n",
        "    Computes attention weights over the sequence and returns a weighted sum.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim // 2, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, lstm_output):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            lstm_output: LSTM outputs of shape (batch, T, hidden_dim)\n",
        "        Returns:\n",
        "            context: Weighted sum of shape (batch, hidden_dim)\n",
        "            attention_weights: Attention weights of shape (batch, T)\n",
        "        \"\"\"\n",
        "        # Compute attention scores: (batch, T, 1)\n",
        "        scores = self.attention(lstm_output)\n",
        "\n",
        "        # Apply softmax over time dimension: (batch, T, 1)\n",
        "        attention_weights = F.softmax(scores, dim=1)\n",
        "\n",
        "        # Compute weighted sum: (batch, hidden_dim)\n",
        "        context = torch.sum(attention_weights * lstm_output, dim=1)\n",
        "\n",
        "        return context, attention_weights.squeeze(-1)\n",
        "\n",
        "\n",
        "class AttentionLSTM(nn.Module):\n",
        "    \"\"\"Bidirectional LSTM with attention mechanism.\n",
        "\n",
        "    Processes temporal sequence of frame features and outputs a fixed-size representation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim=256, num_layers=2, dropout=0.3, bidirectional=True):\n",
        "        super(AttentionLSTM, self).__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.bidirectional = bidirectional\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            bidirectional=bidirectional\n",
        "        )\n",
        "\n",
        "        # Attention mechanism\n",
        "        self.attention = Attention(hidden_dim * self.num_directions)\n",
        "\n",
        "        # Output dimension\n",
        "        self.output_dim = hidden_dim * self.num_directions\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch, T, input_dim)\n",
        "        Returns:\n",
        "            output: Context vector of shape (batch, hidden_dim * num_directions)\n",
        "            attention_weights: Attention weights of shape (batch, T)\n",
        "        \"\"\"\n",
        "        # LSTM forward pass: (batch, T, hidden_dim * num_directions)\n",
        "        lstm_output, (hidden, cell) = self.lstm(x)\n",
        "\n",
        "        # Apply attention\n",
        "        context, attention_weights = self.attention(lstm_output)\n",
        "\n",
        "        return context, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXNLX5GV-lmj"
      },
      "source": [
        "## 5. Complete Sign2Text Model\n",
        "\n",
        "Combines all components: Vision Layer → Positional Encoding → Attention LSTM → FC Layer → Classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "pCKBEgo2-lmk"
      },
      "outputs": [],
      "source": [
        "class Sign2TextModel(nn.Module):\n",
        "    \"\"\"Complete Sign Language to Text model.\n",
        "\n",
        "    Architecture:\n",
        "    1. Vision Layer (CNN): Extract spatial features from each frame\n",
        "    2. Positional Encoding: Add temporal position information\n",
        "    3. Attention LSTM: Process temporal sequence with attention\n",
        "    4. FC Layer: Final classification\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, feature_dim=512, hidden_dim=256,\n",
        "                 num_lstm_layers=2, dropout=0.3, pretrained_cnn=True,\n",
        "                 classification_layers=(256, 512, 128),\n",
        "                 freeze_cnn=False, max_frames=100):\n",
        "        super(Sign2TextModel, self).__init__()\n",
        "\n",
        "        # Vision Layer (CNN)\n",
        "        self.vision_layer = VisionLayer(\n",
        "            feature_dim=feature_dim,\n",
        "            pretrained=pretrained_cnn,\n",
        "            freeze_backbone=freeze_cnn\n",
        "        )\n",
        "\n",
        "        # Positional Encoding\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            d_model=feature_dim,\n",
        "            max_len=max_frames,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        # Attention Layer (LSTM)\n",
        "        self.attention_lstm = AttentionLSTM(\n",
        "            input_dim=feature_dim,\n",
        "            hidden_dim=hidden_dim,\n",
        "            num_layers=num_lstm_layers,\n",
        "            dropout=dropout,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        layers = []\n",
        "\n",
        "        for i, dim in enumerate(classification_layers):\n",
        "            if i == 0:\n",
        "                layers.append(nn.Linear(self.attention_lstm.output_dim, dim))\n",
        "            else:\n",
        "                layers.append(nn.Linear(classification_layers[i - 1], dim))\n",
        "            layers.append(nn.ReLU())\n",
        "            nn.Dropout(dropout)\n",
        "\n",
        "        layers.append(nn.Linear(classification_layers[-1], num_classes))\n",
        "        # FC Layer (Classification)\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            *layers\n",
        "        )\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def set_freeze(self, is_frozen):\n",
        "        self.vision_layer.set_freeze_backbone(is_frozen)\n",
        "\n",
        "    def forward(self, x, return_attention=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input video frames of shape (batch, T, C, H, W)\n",
        "            return_attention: If True, also return attention weights\n",
        "        Returns:\n",
        "            logits: Classification logits of shape (batch, num_classes)\n",
        "            attention_weights (optional): Attention weights of shape (batch, T)\n",
        "        \"\"\"\n",
        "        # Vision Layer: (batch, T, C, H, W) → (batch, T, feature_dim)\n",
        "        features = self.vision_layer(x)\n",
        "\n",
        "        # Positional Encoding: (batch, T, feature_dim) → (batch, T, feature_dim)\n",
        "        features = self.positional_encoding(features)\n",
        "\n",
        "        # Attention LSTM: (batch, T, feature_dim) → (batch, hidden_dim * 2)\n",
        "        context, attention_weights = self.attention_lstm(features)\n",
        "\n",
        "        # FC Layer: (batch, hidden_dim * 2) → (batch, num_classes)\n",
        "        logits = self.fc_layer(context)\n",
        "\n",
        "        if return_attention:\n",
        "            return logits, attention_weights\n",
        "        return logits\n",
        "\n",
        "    def _init_weights(self):\n",
        "        \"\"\"Applies Xavier initialization to Linear layers and LSTM weights.\"\"\"\n",
        "\n",
        "        # Initialize LSTM weights\n",
        "        # For LSTMs, orthogonal initialization for recurrent weights and Xavier for input weights is common.\n",
        "        # However, nn.init.xavier_uniform_ is a good general starting point.\n",
        "        for name, param in self.attention_lstm.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                # Apply Xavier/Glorot for weights\n",
        "                nn.init.xavier_uniform_(param)\n",
        "            elif 'bias' in name:\n",
        "                # Initialize biases to zero (or use a specific trick like setting forget gate bias to 1)\n",
        "                nn.init.constant_(param, 0.0)\n",
        "\n",
        "        # Initialize Classification FC Layers\n",
        "        for m in self.fc_layer.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                # Use Xavier/Glorot for weights\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0.0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.1 Timesformer"
      ],
      "metadata": {
        "id": "jiW72eN0UdWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import repeat\n",
        "import collections.abc\n",
        "from enum import Enum\n",
        "from typing import *\n",
        "\n",
        "import torch.nn.functional as F\n",
        "def _ntuple(n):\n",
        "    def parse(x):\n",
        "        if isinstance(x, collections.abc.Iterable) and not isinstance(x, str):\n",
        "            return tuple(x)\n",
        "        return tuple(repeat(x, n))\n",
        "    return parse\n",
        "\n",
        "class Format(str, Enum):\n",
        "    NCHW = 'NCHW'\n",
        "    NHWC = 'NHWC'\n",
        "    NCL = 'NCL'\n",
        "    NLC = 'NLC'\n",
        "\n",
        "\n",
        "FormatT = Union[str, Format]\n",
        "\n",
        "to_1tuple = _ntuple(1)\n",
        "to_2tuple = _ntuple(2)\n",
        "to_3tuple = _ntuple(3)\n",
        "to_4tuple = _ntuple(4)\n",
        "to_ntuple = _ntuple\n",
        "\n",
        "\n",
        "# https://github.com/huggingface/pytorch-image-models/blob/main/timm/layers/patch_embed.py\n",
        "class PatchEmbedVideo(nn.Module):\n",
        "    \"\"\" Video to Patch Embedding\n",
        "        (Adapts 2D Conv PatchEmbed to handle B, F, C, H, W inputs)\n",
        "    \"\"\"\n",
        "    dynamic_img_pad: torch.jit.Final[bool]\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            img_size: Optional[Union[int, Tuple[int, int]]] = 224,\n",
        "            patch_size: int = 16,\n",
        "            in_chans: int = 3,\n",
        "            embed_dim: int = 768,\n",
        "            norm_layer: Optional[Callable] = None,\n",
        "            flatten: bool = True,\n",
        "            output_fmt: Optional[str] = None,\n",
        "            bias: bool = True,\n",
        "            strict_img_size: bool = True,\n",
        "            dynamic_img_pad: bool = False,\n",
        "            device=None,\n",
        "            dtype=None,\n",
        "    ):\n",
        "        dd = {'device': device, 'dtype': dtype}\n",
        "        super().__init__()\n",
        "        self.patch_size = to_2tuple(patch_size)\n",
        "        self.img_size, self.grid_size, self.num_patches = self._init_img_size(img_size)\n",
        "\n",
        "        if output_fmt is not None:\n",
        "            self.flatten = False\n",
        "            self.output_fmt = Format(output_fmt)\n",
        "        else:\n",
        "            self.flatten = flatten\n",
        "            self.output_fmt = Format.NCHW\n",
        "\n",
        "        self.strict_img_size = strict_img_size\n",
        "        self.dynamic_img_pad = dynamic_img_pad\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # Standard 2D Conv is sufficient for TimeSformer (frame-wise processing)\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size, bias=bias, **dd)\n",
        "        self.norm = norm_layer(embed_dim, **dd) if norm_layer else nn.Identity()\n",
        "\n",
        "    def _init_img_size(self, img_size: Union[int, Tuple[int, int]]):\n",
        "        if img_size is None:\n",
        "            return None, None, None\n",
        "        img_size = to_2tuple(img_size)\n",
        "        grid_size = tuple([s // p for s, p in zip(img_size, self.patch_size)])\n",
        "        num_patches = grid_size[0] * grid_size[1]\n",
        "        return img_size, grid_size, num_patches\n",
        "\n",
        "    def set_input_size(self, img_size=None, patch_size=None):\n",
        "        # (Same as original code, omitted for brevity)\n",
        "        pass\n",
        "\n",
        "    def feat_ratio(self, as_scalar=True):\n",
        "        return max(self.patch_size) if as_scalar else self.patch_size\n",
        "\n",
        "    def dynamic_feat_size(self, img_size: Tuple[int, int]):\n",
        "        if self.dynamic_img_pad:\n",
        "            return math.ceil(img_size[0] / self.patch_size[0]), math.ceil(img_size[1] / self.patch_size[1])\n",
        "        else:\n",
        "            return img_size[0] // self.patch_size[0], img_size[1] // self.patch_size[1]\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input shape: (Batch, Frames, Channels, Height, Width)\n",
        "        B, F, C, H, W = x.shape\n",
        "\n",
        "        # 1. Fold Frames into Batch dimension to use Conv2d\n",
        "        # (B, F, C, H, W) -> (B * F, C, H, W)\n",
        "        x = x.reshape(B * F, C, H, W)\n",
        "\n",
        "        # 2. Handle Dynamic Padding (if enabled)\n",
        "        if self.dynamic_img_pad:\n",
        "            pad_h = (self.patch_size[0] - H % self.patch_size[0]) % self.patch_size[0]\n",
        "            pad_w = (self.patch_size[1] - W % self.patch_size[1]) % self.patch_size[1]\n",
        "            x = F.pad(x, (0, pad_w, 0, pad_h))\n",
        "\n",
        "        # 3. Apply Projection\n",
        "        # Output: (B*F, Embed_Dim, Grid_H, Grid_W)\n",
        "        x = self.proj(x)\n",
        "\n",
        "        if self.flatten:\n",
        "            # Flatten spatial dims: (B*F, Embed_Dim, Grid_H * Grid_W)\n",
        "            # Transpose: (B*F, Num_Spatial_Patches, Embed_Dim)\n",
        "            x = x.flatten(2).transpose(1, 2)\n",
        "\n",
        "            # 4. Unfold Frames back out\n",
        "            # We want a continuous sequence of tokens: (B, F * Num_Spatial_Patches, Embed_Dim)\n",
        "            # This matches the TimeSformer expectation.\n",
        "            BF, N_spatial, D = x.shape\n",
        "            x = x.reshape(B, F * N_spatial, D)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "class TimeSpatialTokens(nn.Module):\n",
        "  def __init__(self, num_frames, num_patches, embed_dim):\n",
        "    super(TimeSpatialTokens, self).__init__()\n",
        "\n",
        "    self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "    self.spatial_pos_embedding = nn.Parameter(torch.zeros(1,  num_patches, embed_dim))\n",
        "    self.temporal_pos_embedding = nn.Parameter(torch.zeros(1, num_frames, embed_dim))\n",
        "\n",
        "  # Receives patch embeddings of size B, T, D\n",
        "  def forward(self, x):\n",
        "    # B, T, D = x.shape\n",
        "    # F = self.temporal_pos_embedding.shape[1] # num frames\n",
        "    # N = self.spatial_pos_embedding.shape[1] # num patches per frame\n",
        "    # cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "    # x = torch.cat((cls_tokens, x), dim=1)\n",
        "    # spatial_pe = self.spatial_pos_embedding.repeat(1, F, 1)\n",
        "    # x[:, 1:, :] = x[:, 1:, :] + spatial_pe\n",
        "    # temporal_pe = self.temporal_pos_embedding.unsqueeze(2).repeat(1, 1, N, 1)\n",
        "    # temporal_pe = temporal_pe.view(1, F*N, D)\n",
        "    # x[:, 1:, :] = x[:, 1:, :] + temporal_pe\n",
        "    B, T, D = x.shape\n",
        "    F = self.temporal_pos_embedding.shape[1] # num frames\n",
        "    N = self.spatial_pos_embedding.shape[1] # num patches per frame\n",
        "    x_reshaped = x.view(B, F, N, D)\n",
        "    x_reshaped = x_reshaped + self.spatial_pos_embedding.unsqueeze(1) # Add spatial\n",
        "    x = x_reshaped.view(B, F * N, D)\n",
        "\n",
        "    temporal_pe = self.temporal_pos_embedding.unsqueeze(2).repeat(1, 1, N, 1)\n",
        "    temporal_pe = temporal_pe.view(1, F * N, D)\n",
        "\n",
        "    x += temporal_pe\n",
        "\n",
        "    cls_pe = self.temporal_pos_embedding[:, 0:1, :] # PE for t=0, (1, 1, D)\n",
        "\n",
        "    # Add PE to the CLS token\n",
        "    cls_token_with_pe = self.cls_token + cls_pe\n",
        "\n",
        "    # Expand CLS token and prepend to sequence\n",
        "    cls_tokens = cls_token_with_pe.expand(B, -1, -1)\n",
        "    x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "    return x\n",
        "\n",
        "class DividedSpaceTimeAttention(nn.Module):\n",
        "  def __init__(self, dim, num_heads, num_frames, num_patches, dropout=0.0):\n",
        "    super(DividedSpaceTimeAttention, self).__init__()\n",
        "    self.num_frames = num_frames\n",
        "    self.num_patches = num_patches\n",
        "    self.dim = dim\n",
        "    # 1. Temporal Attention Layer\n",
        "    self.temporal_attn = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, dropout=dropout, batch_first=True)\n",
        "    # 2. Spatial Attention Layer\n",
        "    self.spatial_attn = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, dropout=dropout, batch_first=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, N_total, D = x.shape\n",
        "    # NOTE: The CLS token remains UNCHANGED throughout the attention blocks\n",
        "    # and is only updated via the residual connection *outside* this module.\n",
        "    cls_token = x[:, 0:1, :]    # (B, 1, D) -> The original CLS token\n",
        "    patch_tokens = x[:, 1:, :]  # (B, F*P, D)\n",
        "    # Reshape: (B, F*P, D) -> (B, F, P, D)\n",
        "    patch_tokens = patch_tokens.reshape(B, self.num_frames, self.num_patches, D)\n",
        "\n",
        "    # --- Temporal Attention (Across F frames, for each of the P patches) ---\n",
        "    # Shape change: (B, F, P, D) -> (B, P, F, D) -> (B * P, F, D)\n",
        "    xt = patch_tokens.permute(0, 2, 1, 3).contiguous().view(B * self.num_patches, self.num_frames, D)\n",
        "\n",
        "    # Prepare CLS token: Repeat the single CLS token for all P streams\n",
        "    cls_t = cls_token.repeat(1, self.num_patches, 1).view(B * self.num_patches, 1, D)\n",
        "    xt_with_cls = torch.cat([cls_t, xt], dim=1)\n",
        "\n",
        "    xt_out_all, _ = self.temporal_attn(xt_with_cls, xt_with_cls, xt_with_cls)\n",
        "\n",
        "    # DISCARD the attention-updated CLS token and keep the updated patch tokens\n",
        "    xt_updated_patches = xt_out_all[:, 1:, :] # (B * P, F, D)\n",
        "\n",
        "    # Recover shape for Spatial Attention input: (B, F, P, D)\n",
        "    patch_tokens = xt_updated_patches.view(B, self.num_patches, self.num_frames, D).permute(0, 2, 1, 3)\n",
        "\n",
        "    # --- Spatial Attention (Across P patches, for each of the F frames) ---\n",
        "    # Reshape: (B, F, P, D) -> (B * F, P, D)\n",
        "    xs = patch_tokens.reshape(B * self.num_frames, self.num_patches, D)\n",
        "\n",
        "    # Prepare CLS token: Repeat the single ORIGINAL CLS token for all F frames\n",
        "    cls_s = cls_token.repeat(1, self.num_frames, 1).view(B * self.num_frames, 1, D)\n",
        "    xs_with_cls = torch.cat([cls_s, xs], dim=1)\n",
        "\n",
        "    xs_out_all, _ = self.spatial_attn(xs_with_cls, xs_with_cls, xs_with_cls)\n",
        "\n",
        "    # DISCARD the attention-updated CLS token and keep the updated patch tokens\n",
        "    xs_updated_patches = xs_out_all[:, 1:, :] # (B * F, P, D)\n",
        "\n",
        "    # Recover shape: (B, F, P, D) -> flatten to (B, F*P, D)\n",
        "    patch_tokens = xs_updated_patches.view(B, self.num_frames, self.num_patches, D).flatten(1, 2)\n",
        "\n",
        "    # Final output: The ORIGINAL CLS token + all updated patch tokens\n",
        "    return torch.cat([cls_token, patch_tokens], dim=1)\n",
        "\n",
        "class TimesformerBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads, num_frames, num_patches, mlp_ratio=4., drop=0.0, attn_drop=0.0):\n",
        "        super(TimesformerBlock, self).__init__()\n",
        "\n",
        "        # --- Normalization Layers (Step 3) ---\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "\n",
        "        # --- Attention Layer (Step 4) ---\n",
        "        self.attn = DividedSpaceTimeAttention(\n",
        "            dim=dim,\n",
        "            num_heads=num_heads,\n",
        "            num_frames=num_frames,\n",
        "            num_patches=num_patches,\n",
        "            dropout=attn_drop\n",
        "        )\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, 4*dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(drop),\n",
        "            nn.Linear(4*dim, dim),\n",
        "            nn.Dropout(drop)\n",
        "        )\n",
        "\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        shortcut = x\n",
        "        x_norm = self.norm1(x)\n",
        "        attn_out = self.attn(x_norm)\n",
        "\n",
        "        # 3. Add Residual (Skip Connection)\n",
        "        x = shortcut + attn_out\n",
        "        shortcut = x\n",
        "        x_norm = self.norm2(x)\n",
        "\n",
        "        mlp_out = self.mlp(x_norm)\n",
        "\n",
        "        x = shortcut + mlp_out\n",
        "        return x\n",
        "\n",
        "class SignTimeSformer(nn.Module):\n",
        "  # L=amount of encoder blocks\n",
        "  def __init__(self, num_classes, img_size=224, num_frames=12, L=20, heads=12, dropout=0.1,):\n",
        "      super(SignTimeSformer, self).__init__()\n",
        "\n",
        "      self.patcher = PatchEmbedVideo(img_size=img_size)\n",
        "      embed_dim = self.patcher.embed_dim\n",
        "      self.time_spatial_tokens = TimeSpatialTokens(\n",
        "          num_frames=num_frames,\n",
        "          num_patches=self.patcher.num_patches,\n",
        "          embed_dim=embed_dim\n",
        "      )\n",
        "      self.blocks = nn.ModuleList(\n",
        "        [\n",
        "            TimesformerBlock(\n",
        "                dim=embed_dim,\n",
        "                num_heads=heads,\n",
        "                num_frames=num_frames,\n",
        "                num_patches=self.patcher.num_patches,\n",
        "            )\n",
        "            for _ in range(L)\n",
        "        ]\n",
        "      )\n",
        "\n",
        "      self.layer_norm = nn.LayerNorm(embed_dim)\n",
        "      self.layer_norm_mlp = nn.LayerNorm(embed_dim)\n",
        "\n",
        "      self.norm = nn.LayerNorm(embed_dim)\n",
        "      self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Patching: (batch, T, C, H, W) → (batch, T, feature_dim)\n",
        "    x = self.patcher(x) # (batch, T, C, H, W) → (batch, T, feature_dim)\n",
        "    x = self.time_spatial_tokens(x) # (batch, T, feature_dim) -> (B, 1+T, feature_dim)\n",
        "\n",
        "    for block in self.blocks:\n",
        "      x = block(x)\n",
        "    x = self.norm(x)\n",
        "    cls_token_out = x[:, 0] # extract CLS\n",
        "    logits = self.head(cls_token_out)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "mtyvIN2NUGkQ"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srRI42gq-lmk"
      },
      "source": [
        "## 6. Training Utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "6UWY8AUW-lmk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4668030c-1ef4-4c9a-9653-bef18595e714"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3634111461.py:2: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        }
      ],
      "source": [
        "from torch.cuda.amp import autocast, GradScaler\n",
        "scaler = GradScaler()\n",
        "\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    \"\"\"Train the model for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
        "    for frames, labels in progress_bar:\n",
        "        frames = frames.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # print(frames.shape)\n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        with autocast():\n",
        "          outputs = model(frames)\n",
        "          loss = criterion(outputs, labels)\n",
        "        # Backward pass\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        # loss.backward()\n",
        "        # optimizer.step()\n",
        "\n",
        "        # Statistics\n",
        "        running_loss += loss.item() * frames.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        progress_bar.set_postfix({\n",
        "            'loss': loss.item(),\n",
        "            'acc': 100 * correct / total\n",
        "        })\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = 100 * correct / total\n",
        "\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    \"\"\"Evaluate the model on a dataset.\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for frames, labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            frames = frames.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            with autocast():\n",
        "                outputs = model(frames)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * frames.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = 100 * correct / total\n",
        "\n",
        "    return epoch_loss, epoch_acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFBEBTg4-lml"
      },
      "source": [
        "## 7. Configuration and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "kPsZzTQd-lml"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# CONFIGURATION - Modify these paths and hyperparameters\n",
        "# ============================================\n",
        "\n",
        "# Data paths\n",
        "JSON_PATH = \"/content/drive/MyDrive/wlasl_resized/wlasl-complete/WLASL_v0.3.json\"  # Path to WLASL JSON\n",
        "VIDEO_DIR = \"/content/drive/MyDrive/wlasl_resized/wlasl-complete/videos\"  # Path to video directory\n",
        "\n",
        "# Model hyperparameters\n",
        "NUM_FRAMES = 32          # Number of frames to sample from each video\n",
        "FEATURE_DIM = 512        # CNN feature dimension\n",
        "HIDDEN_DIM = 256         # LSTM hidden dimension\n",
        "NUM_LSTM_LAYERS = 2      # Number of LSTM layers\n",
        "DROPOUT = 0.4            # Dropout rate\n",
        "\n",
        "# Training hyperparameters\n",
        "BATCH_SIZE = 16           # Batch size (adjust based on GPU memory)\n",
        "LEARNING_RATE = 5e-4     # Learning rate\n",
        "NUM_EPOCHS = 50          # Number of training epochs\n",
        "WEIGHT_DECAY = 1e-43     # L2 regularization\n",
        "IMG_SIZE=224\n",
        "# Options\n",
        "FREEZE_CNN = True       # Whether to freeze CNN backbone\n",
        "PRETRAINED_CNN = True    # Use pretrained CNN weights\n",
        "WORKERS = 4\n",
        "EPOCHS_UNTIL_UNFREEZE = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "l39WEIDb-lml"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import v2\n",
        "# Data transforms for training and validation\n",
        "# train_transform = transforms.Compose([\n",
        "#     v2.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "#     # v2.RandomHorizontalFlip(),\n",
        "#     # v2.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "#     v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]),\n",
        "# ])\n",
        "\n",
        "# val_transform = transforms.Compose([\n",
        "#     v2.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "#     v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]),\n",
        "# ])\n",
        "\n",
        "from torchvision.transforms import Compose\n",
        "import pytorchvideo.transforms as ptv_transforms\n",
        "from pytorchvideo.transforms import functional as ptv_functional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Note: The transforms below expect the video tensor to be in the range [0.0, 1.0]\n",
        "# and of shape (T, C, H, W). The `WLASLTorchCodec` implementation already ensures\n",
        "# the shape is (T, C, H, W), but you must ensure the pixel values are converted\n",
        "# to float and normalized to [0, 1] before applying the standard normalization.\n",
        "\n",
        "\n",
        "\n",
        "mean = [0.45, 0.45, 0.45]\n",
        "std = [0.225, 0.225, 0.225]\n",
        "\n",
        "# Test out dataset\n",
        "train_transform = Compose(\n",
        "    [\n",
        "        # 1. Spatial Resize: Scale the shortest edge to SIDE_SIZE\n",
        "        ptv_transforms.UniformTemporalSubsample(num_samples=24, temporal_dim=0),\n",
        "        ptv_transforms.ConvertUint8ToFloat(),\n",
        "        Lambda(lambda x: x.permute(1, 0, 2, 3)),\n",
        "        ptv_transforms.Normalize(mean, std),\n",
        "        Lambda(lambda x: x.permute(1, 0, 2, 3)),\n",
        "        ptv_transforms.ShortSideScale(size=224),\n",
        "        ptv_transforms.RandAugment(magnitude=4, num_layers=2),\n",
        "        # ptv_transforms.AugMix(magnitude=3),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# train_transform = Compose(\n",
        "#     [\n",
        "#         # 1. Spatial Resize: Scale the shortest edge to SIDE_SIZE\n",
        "#         ptv_transforms.UniformTemporalSubsample(num_samples=NUM_FRAMES, temporal_dim=0),\n",
        "#         ptv_transforms.ConvertUint8ToFloat(),\n",
        "#         ptv_transforms.ShortSideScale(size=IMG_SIZE),\n",
        "#         ptv_transforms.RandAugment(magnitude=15, num_layers=2),\n",
        "#         ptv_transforms.AugMix(magnitude=3),\n",
        "#     ]\n",
        "# )\n",
        "\n",
        "test_transform = Compose(\n",
        "    [\n",
        "        ptv_transforms.UniformTemporalSubsample(num_samples=24, temporal_dim=0),\n",
        "        ptv_transforms.ConvertUint8ToFloat(),\n",
        "        Lambda(lambda x: x.permute(1, 0, 2, 3)),\n",
        "        ptv_transforms.Normalize(mean, std),\n",
        "        Lambda(lambda x: x.permute(1, 0, 2, 3)),\n",
        "        ptv_transforms.ShortSideScale(size=IMG_SIZE),\n",
        "    ]\n",
        ")\n",
        "val_transform =test_transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnohaoY9-lml",
        "outputId": "53e0dce1-18c4-444f-9814-f220caafb42f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Will download: True\n",
            "Using Colab cache for faster access to the 'wlasl2000-resized' dataset.\n",
            "Downloaded at path:  /kaggle/input/wlasl2000-resized\n",
            "Limiting dataset to top 10 classes.\n",
            "Will download: True\n",
            "Downloaded at path:  /kaggle/input/wlasl2000-resized\n",
            "Limiting dataset to top 10 classes.\n",
            "Will download: True\n",
            "Downloaded at path:  /kaggle/input/wlasl2000-resized\n",
            "Limiting dataset to top 10 classes.\n",
            "Number of training samples: 200\n",
            "Number of validation samples: 45\n",
            "Number of test samples: 35\n",
            "Number of classes: 10\n"
          ]
        }
      ],
      "source": [
        "# Create datasets\n",
        "train_dataset = WLASLTorchCodec(\n",
        "    download=True,\n",
        "    split=\"train\",\n",
        "    max_classes=10,\n",
        "    num_frames=NUM_FRAMES,\n",
        "    transform=train_transform\n",
        ")\n",
        "\n",
        "val_dataset = WLASLTorchCodec(\n",
        "    download=True,\n",
        "    split=\"val\",\n",
        "    max_classes=10,\n",
        "    num_frames=NUM_FRAMES,\n",
        "    transform=val_transform\n",
        ")\n",
        "\n",
        "test_dataset = WLASLTorchCodec(\n",
        "    download=True,\n",
        "    split=\"test\",\n",
        "    max_classes=10,\n",
        "    num_frames=NUM_FRAMES,\n",
        "    transform=val_transform\n",
        ")\n",
        "\n",
        "# Get number of classes from dataset\n",
        "NUM_CLASSES = train_dataset.num_classes\n",
        "\n",
        "print(f\"Number of training samples: {len(train_dataset)}\")\n",
        "print(f\"Number of validation samples: {len(val_dataset)}\")\n",
        "print(f\"Number of test samples: {len(test_dataset)}\")\n",
        "print(f\"Number of classes: {NUM_CLASSES}\")\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=WORKERS,           # Start high. The optimal value is often 4 to 12.\n",
        "    pin_memory=True,         # Accelerates the transfer of data from CPU to GPU VRAM.\n",
        "    persistent_workers=True, # Recommended for PyTorch multi-process workers to save epoch setup time.\n",
        "    prefetch_factor=3\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=WORKERS,           # Start high. The optimal value is often 4 to 12.\n",
        "                             # Since video decoding is CPU-heavy, 8 is a good starting point.\n",
        "    pin_memory=True,         # Accelerates the transfer of data from CPU to GPU VRAM.\n",
        "    persistent_workers=True, # Recommended for PyTorch multi-process workers to save epoch setup time.\n",
        "    prefetch_factor=2\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=WORKERS,           # Start high. The optimal value is often 4 to 12.\n",
        "                             # Since video decoding is CPU-heavy, 8 is a good starting point.\n",
        "    pin_memory=True,         # Accelerates the transfer of data from CPU to GPU VRAM.\n",
        "    persistent_workers=True, # Recommended for PyTorch multi-process workers to save epoch setup time.\n",
        "    prefetch_factor=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abGiXTmk-lml",
        "outputId": "bd574fd9-324c-4b3c-ad3a-62dad0f1d72a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total parameters: 14,791,883\n",
            "Trainable parameters: 3,615,371\n"
          ]
        }
      ],
      "source": [
        "# Initialize model\n",
        "model = Sign2TextModel(\n",
        "    num_classes=NUM_CLASSES,\n",
        "    feature_dim=FEATURE_DIM,\n",
        "    hidden_dim=HIDDEN_DIM,\n",
        "    num_lstm_layers=NUM_LSTM_LAYERS,\n",
        "    dropout=DROPOUT,\n",
        "    pretrained_cnn=PRETRAINED_CNN,\n",
        "    freeze_cnn=FREEZE_CNN,\n",
        "    max_frames=NUM_FRAMES\n",
        ").to(device)\n",
        "\n",
        "# model = SignTimeSformer(\n",
        "#     num_classes=NUM_CLASSES,\n",
        "#     img_size=IMG_SIZE,\n",
        "#     num_frames=NUM_FRAMES,\n",
        "#     heads=12,\n",
        "#     L=5,\n",
        "#     dropout=DROPOUT\n",
        "# ).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW( model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY )\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau( optimizer, mode='min', factor=0.5, patience=3 )\n",
        "\n",
        "# Print model summary\n",
        "# print(model)\n",
        "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbpkZSb_-lmm"
      },
      "source": [
        "## 8. Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jbK9sCf-lmm",
        "outputId": "9b9f5d53-ee5b-44db-e47c-7f2c409f3027"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/13 [00:00<?, ?it/s]/tmp/ipython-input-3634111461.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Training: 100%|██████████| 13/13 [00:06<00:00,  1.93it/s, loss=2.36, acc=11.5]\n",
            "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]/tmp/ipython-input-3634111461.py:63: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Evaluating: 100%|██████████| 3/3 [00:02<00:00,  1.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.3095, Train Acc: 11.50%\n",
            "Val Loss: 2.2930, Val Acc: 13.33%\n",
            "✓ Saved new best model with Val Acc: 13.33%\n",
            "\n",
            "Epoch 2/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:06<00:00,  2.08it/s, loss=2.33, acc=15]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.2939, Train Acc: 15.00%\n",
            "Val Loss: 2.2724, Val Acc: 13.33%\n",
            "\n",
            "Epoch 3/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:06<00:00,  1.91it/s, loss=2.24, acc=14.5]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.2853, Train Acc: 14.50%\n",
            "Val Loss: 2.2574, Val Acc: 13.33%\n",
            "\n",
            "Epoch 4/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:06<00:00,  1.98it/s, loss=2.22, acc=14]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.2761, Train Acc: 14.00%\n",
            "Val Loss: 2.2275, Val Acc: 13.33%\n",
            "\n",
            "Epoch 5/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:05<00:00,  2.24it/s, loss=2.31, acc=18]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.2366, Train Acc: 18.00%\n",
            "Val Loss: 2.1687, Val Acc: 15.56%\n",
            "✓ Saved new best model with Val Acc: 15.56%\n",
            "\n",
            "Epoch 6/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:06<00:00,  2.05it/s, loss=2.15, acc=14.5]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.2162, Train Acc: 14.50%\n",
            "Val Loss: 2.1711, Val Acc: 22.22%\n",
            "✓ Saved new best model with Val Acc: 22.22%\n",
            "\n",
            "Epoch 7/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:06<00:00,  2.05it/s, loss=2.29, acc=20.5]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.2386, Train Acc: 20.50%\n",
            "Val Loss: 2.1011, Val Acc: 22.22%\n",
            "\n",
            "Epoch 8/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:06<00:00,  2.07it/s, loss=1.98, acc=21.5]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.1515, Train Acc: 21.50%\n",
            "Val Loss: 2.0810, Val Acc: 24.44%\n",
            "✓ Saved new best model with Val Acc: 24.44%\n",
            "\n",
            "Epoch 9/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:06<00:00,  1.92it/s, loss=2.43, acc=20.5]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.1657, Train Acc: 20.50%\n",
            "Val Loss: 2.0836, Val Acc: 22.22%\n",
            "\n",
            "Epoch 10/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:06<00:00,  2.08it/s, loss=2.59, acc=19]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.1753, Train Acc: 19.00%\n",
            "Val Loss: 2.3096, Val Acc: 13.33%\n",
            "\n",
            "Epoch 11/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:06<00:00,  2.10it/s, loss=2.06, acc=17.5]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.1731, Train Acc: 17.50%\n",
            "Val Loss: 2.0888, Val Acc: 22.22%\n",
            "\n",
            "Epoch 12/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:08<00:00,  1.56it/s, loss=2.19, acc=19.5]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.1171, Train Acc: 19.50%\n",
            "Val Loss: 2.3617, Val Acc: 20.00%\n",
            "\n",
            "Epoch 13/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:08<00:00,  1.52it/s, loss=1.86, acc=24.5]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.9613, Train Acc: 24.50%\n",
            "Val Loss: 2.2066, Val Acc: 26.67%\n",
            "✓ Saved new best model with Val Acc: 26.67%\n",
            "\n",
            "Epoch 14/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:08<00:00,  1.55it/s, loss=1.82, acc=25]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.9252, Train Acc: 25.00%\n",
            "Val Loss: 2.0306, Val Acc: 31.11%\n",
            "✓ Saved new best model with Val Acc: 31.11%\n",
            "\n",
            "Epoch 15/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:08<00:00,  1.55it/s, loss=1.87, acc=33]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.8205, Train Acc: 33.00%\n",
            "Val Loss: 2.2817, Val Acc: 31.11%\n",
            "\n",
            "Epoch 16/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:08<00:00,  1.54it/s, loss=1.78, acc=37]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.7785, Train Acc: 37.00%\n",
            "Val Loss: 2.0378, Val Acc: 33.33%\n",
            "✓ Saved new best model with Val Acc: 33.33%\n",
            "\n",
            "Epoch 17/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:08<00:00,  1.55it/s, loss=1.3, acc=37]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.7014, Train Acc: 37.00%\n",
            "Val Loss: 2.0563, Val Acc: 33.33%\n",
            "\n",
            "Epoch 18/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:08<00:00,  1.52it/s, loss=1.36, acc=42]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.5984, Train Acc: 42.00%\n",
            "Val Loss: 2.0105, Val Acc: 37.78%\n",
            "✓ Saved new best model with Val Acc: 37.78%\n",
            "\n",
            "Epoch 19/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:08<00:00,  1.56it/s, loss=2.22, acc=46.5]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.4936, Train Acc: 46.50%\n",
            "Val Loss: 1.9601, Val Acc: 40.00%\n",
            "✓ Saved new best model with Val Acc: 40.00%\n",
            "\n",
            "Epoch 20/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:08<00:00,  1.53it/s, loss=1.4, acc=45.5]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.4641, Train Acc: 45.50%\n",
            "Val Loss: 1.7753, Val Acc: 40.00%\n",
            "\n",
            "Epoch 21/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:08<00:00,  1.54it/s, loss=1.11, acc=54.5]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.2842, Train Acc: 54.50%\n",
            "Val Loss: 1.4770, Val Acc: 46.67%\n",
            "✓ Saved new best model with Val Acc: 46.67%\n",
            "\n",
            "Epoch 22/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:08<00:00,  1.50it/s, loss=0.961, acc=56.5]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.2979, Train Acc: 56.50%\n",
            "Val Loss: 1.6323, Val Acc: 40.00%\n",
            "\n",
            "Epoch 23/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:08<00:00,  1.58it/s, loss=1.26, acc=56.5]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.1741, Train Acc: 56.50%\n",
            "Val Loss: 1.4893, Val Acc: 48.89%\n",
            "✓ Saved new best model with Val Acc: 48.89%\n",
            "\n",
            "Epoch 24/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:08<00:00,  1.54it/s, loss=0.696, acc=66]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9882, Train Acc: 66.00%\n",
            "Val Loss: 1.4419, Val Acc: 42.22%\n",
            "\n",
            "Epoch 25/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:08<00:00,  1.52it/s, loss=0.659, acc=74]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.8490, Train Acc: 74.00%\n",
            "Val Loss: 1.4412, Val Acc: 51.11%\n",
            "✓ Saved new best model with Val Acc: 51.11%\n",
            "\n",
            "Epoch 26/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:08<00:00,  1.50it/s, loss=1.2, acc=77.5]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.8467, Train Acc: 77.50%\n",
            "Val Loss: 1.2366, Val Acc: 57.78%\n",
            "✓ Saved new best model with Val Acc: 57.78%\n",
            "\n",
            "Epoch 27/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:08<00:00,  1.55it/s, loss=2.12, acc=74]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.8496, Train Acc: 74.00%\n",
            "Val Loss: 1.5923, Val Acc: 51.11%\n",
            "\n",
            "Epoch 28/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:08<00:00,  1.53it/s, loss=1.43, acc=70]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.8924, Train Acc: 70.00%\n",
            "Val Loss: 1.2498, Val Acc: 57.78%\n",
            "\n",
            "Epoch 29/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:08<00:00,  1.46it/s, loss=1.01, acc=72]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.8620, Train Acc: 72.00%\n",
            "Val Loss: 1.5501, Val Acc: 48.89%\n",
            "\n",
            "Epoch 30/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:08<00:00,  1.52it/s, loss=0.507, acc=79]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.6482, Train Acc: 79.00%\n",
            "Val Loss: 1.5657, Val Acc: 44.44%\n",
            "\n",
            "Epoch 31/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:08<00:00,  1.52it/s, loss=0.245, acc=84]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5382, Train Acc: 84.00%\n",
            "Val Loss: 1.2679, Val Acc: 64.44%\n",
            "✓ Saved new best model with Val Acc: 64.44%\n",
            "\n",
            "Epoch 32/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:08<00:00,  1.52it/s, loss=0.864, acc=87]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4064, Train Acc: 87.00%\n",
            "Val Loss: 1.1065, Val Acc: 62.22%\n",
            "\n",
            "Epoch 33/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:08<00:00,  1.55it/s, loss=0.152, acc=88]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4106, Train Acc: 88.00%\n",
            "Val Loss: 1.4182, Val Acc: 53.33%\n",
            "\n",
            "Epoch 34/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:08<00:00,  1.51it/s, loss=0.493, acc=90.5]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.3793, Train Acc: 90.50%\n",
            "Val Loss: 1.3940, Val Acc: 60.00%\n",
            "\n",
            "Epoch 35/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:08<00:00,  1.55it/s, loss=0.163, acc=88.5]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.3509, Train Acc: 88.50%\n",
            "Val Loss: 1.5095, Val Acc: 55.56%\n",
            "\n",
            "Epoch 36/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:08<00:00,  1.54it/s, loss=0.274, acc=93]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2084, Train Acc: 93.00%\n",
            "Val Loss: 1.9304, Val Acc: 48.89%\n",
            "\n",
            "Epoch 37/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:08<00:00,  1.46it/s, loss=0.349, acc=91]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2880, Train Acc: 91.00%\n",
            "Val Loss: 1.5017, Val Acc: 48.89%\n",
            "\n",
            "Epoch 38/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:08<00:00,  1.55it/s, loss=0.103, acc=96.5]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1475, Train Acc: 96.50%\n",
            "Val Loss: 1.2973, Val Acc: 55.56%\n",
            "\n",
            "Epoch 39/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:08<00:00,  1.56it/s, loss=0.532, acc=97]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1346, Train Acc: 97.00%\n",
            "Val Loss: 1.5063, Val Acc: 55.56%\n",
            "\n",
            "Epoch 40/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:08<00:00,  1.54it/s, loss=0.0582, acc=96]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1305, Train Acc: 96.00%\n",
            "Val Loss: 1.6760, Val Acc: 55.56%\n",
            "\n",
            "Epoch 41/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:08<00:00,  1.58it/s, loss=0.101, acc=95]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1928, Train Acc: 95.00%\n",
            "Val Loss: 1.6030, Val Acc: 64.44%\n",
            "\n",
            "Epoch 42/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:08<00:00,  1.49it/s, loss=0.476, acc=91.5]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2343, Train Acc: 91.50%\n",
            "Val Loss: 1.5738, Val Acc: 62.22%\n",
            "\n",
            "Epoch 43/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:08<00:00,  1.55it/s, loss=0.202, acc=97.5]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0929, Train Acc: 97.50%\n",
            "Val Loss: 1.6513, Val Acc: 60.00%\n",
            "\n",
            "Epoch 44/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:08<00:00,  1.57it/s, loss=0.248, acc=96]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1498, Train Acc: 96.00%\n",
            "Val Loss: 1.5938, Val Acc: 60.00%\n",
            "\n",
            "Epoch 45/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:08<00:00,  1.48it/s, loss=0.184, acc=95]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1667, Train Acc: 95.00%\n",
            "Val Loss: 1.5842, Val Acc: 62.22%\n",
            "\n",
            "Epoch 46/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:08<00:00,  1.51it/s, loss=0.049, acc=95.5]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1100, Train Acc: 95.50%\n",
            "Val Loss: 1.5529, Val Acc: 62.22%\n",
            "\n",
            "Epoch 47/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:08<00:00,  1.54it/s, loss=0.386, acc=96.5]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1070, Train Acc: 96.50%\n",
            "Val Loss: 1.5818, Val Acc: 60.00%\n",
            "\n",
            "Epoch 48/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:08<00:00,  1.58it/s, loss=0.0759, acc=98.5]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0822, Train Acc: 98.50%\n",
            "Val Loss: 1.6336, Val Acc: 57.78%\n",
            "\n",
            "Epoch 49/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:08<00:00,  1.56it/s, loss=0.0225, acc=96]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1378, Train Acc: 96.00%\n",
            "Val Loss: 1.6288, Val Acc: 57.78%\n",
            "\n",
            "Epoch 50/50\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 13/13 [00:08<00:00,  1.49it/s, loss=0.0339, acc=99]\n",
            "Evaluating: 100%|██████████| 3/3 [00:01<00:00,  1.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0737, Train Acc: 99.00%\n",
            "Val Loss: 1.5568, Val Acc: 60.00%\n",
            "\n",
            "Training complete! Best Val Acc: 64.44%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "best_val_acc = 0.0\n",
        "history = {\n",
        "    'train_loss': [], 'train_acc': [],\n",
        "    'val_loss': [], 'val_acc': []\n",
        "}\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n",
        "    print(\"-\" * 40)\n",
        "    if epoch > EPOCHS_UNTIL_UNFREEZE and FREEZE_CNN:\n",
        "        model.set_freeze(False)\n",
        "    # Train\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "\n",
        "    # Validate\n",
        "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "    # Update scheduler\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Save history\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_acc': val_acc,\n",
        "            'label_map': train_dataset.label_map\n",
        "        }, 'best_model.pth')\n",
        "        print(f\"✓ Saved new best model with Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "print(f\"\\nTraining complete! Best Val Acc: {best_val_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0_AbauW-lmm"
      },
      "source": [
        "## 9. Evaluation and Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxMS0YFc-lmm"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training history\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Loss plot\n",
        "axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
        "axes[0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Training and Validation Loss')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True)\n",
        "\n",
        "# Accuracy plot\n",
        "axes[1].plot(history['train_acc'], label='Train Acc', marker='o')\n",
        "axes[1].plot(history['val_acc'], label='Val Acc', marker='s')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Accuracy (%)')\n",
        "axes[1].set_title('Training and Validation Accuracy')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_history.png', dpi=150)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5fEQd22-lmm"
      },
      "outputs": [],
      "source": [
        "# Load best model and evaluate on test set\n",
        "checkpoint = torch.load('best_model.pth')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
        "print(f\"\\nTest Results:\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK5qG01d-lmn"
      },
      "source": [
        "## 10. Attention Visualization\n",
        "\n",
        "Visualize which frames the model attends to most when making predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FY2JOSmH-lmn"
      },
      "outputs": [],
      "source": [
        "def visualize_attention(model, frames, true_label, label_map, device):\n",
        "    \"\"\"Visualize attention weights over video frames.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Get reverse label map\n",
        "    idx_to_label = {v: k for k, v in label_map.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Add batch dimension\n",
        "        frames_batch = frames.unsqueeze(0).to(device)\n",
        "\n",
        "        # Get predictions and attention weights\n",
        "        logits, attention_weights = model(frames_batch, return_attention=True)\n",
        "        pred_label = torch.argmax(logits, dim=1).item()\n",
        "        attention = attention_weights[0].cpu().numpy()\n",
        "\n",
        "    # Create visualization\n",
        "    num_frames = frames.shape[0]\n",
        "    fig, axes = plt.subplots(2, num_frames, figsize=(2 * num_frames, 6))\n",
        "\n",
        "    # Denormalize frames for visualization\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "\n",
        "    for i in range(num_frames):\n",
        "        frame = frames[i].cpu()\n",
        "        frame = frame * std + mean\n",
        "        frame = frame.clamp(0, 1).permute(1, 2, 0).numpy()\n",
        "\n",
        "        # Frame image\n",
        "        axes[0, i].imshow(frame)\n",
        "        axes[0, i].set_title(f\"Frame {i+1}\")\n",
        "        axes[0, i].axis('off')\n",
        "\n",
        "        # Attention weight bar\n",
        "        axes[1, i].bar([0], [attention[i]], color='blue', alpha=0.7)\n",
        "        axes[1, i].set_ylim(0, max(attention) * 1.2)\n",
        "        axes[1, i].set_title(f\"{attention[i]:.3f}\")\n",
        "        axes[1, i].axis('off')\n",
        "\n",
        "    plt.suptitle(\n",
        "        f\"True: {idx_to_label.get(true_label, true_label)} | \"\n",
        "        f\"Predicted: {idx_to_label.get(pred_label, pred_label)}\",\n",
        "        fontsize=14\n",
        "    )\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return pred_label, attention\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pOEoPik-lmn"
      },
      "outputs": [],
      "source": [
        "# Visualize attention for a sample from the test set\n",
        "sample_idx = 0\n",
        "frames, label = test_dataset[sample_idx]\n",
        "pred, attn = visualize_attention(model, frames, label, train_dataset.label_map, device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1vGI8IA-lmn"
      },
      "source": [
        "## 11. Inference Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CXDTtOc-lmn"
      },
      "outputs": [],
      "source": [
        "def predict_video(model, video_path, transform, num_frames, label_map, device):\n",
        "    \"\"\"Predict the sign language class for a video file.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Get reverse label map\n",
        "    idx_to_label = {v: k for k, v in label_map.items()}\n",
        "\n",
        "    # Decode video\n",
        "    decoder = VideoDecoder(video_path)\n",
        "    frames = []\n",
        "\n",
        "    for chunk in decoder:\n",
        "        for frame_tensor in chunk:\n",
        "            if frame_tensor.dim() == 2:\n",
        "                frame_tensor = frame_tensor.unsqueeze(2)\n",
        "            frame_chw = frame_tensor.permute(2, 0, 1)\n",
        "            frame_pil = transforms.ToPILImage()(frame_chw)\n",
        "            frames.append(frame_pil)\n",
        "\n",
        "    # Handle short videos\n",
        "    while len(frames) < num_frames:\n",
        "        frames.extend(frames)\n",
        "\n",
        "    # Sample frames\n",
        "    T = len(frames)\n",
        "    idx = torch.linspace(0, T - 1, num_frames).long()\n",
        "    frames = [frames[i] for i in idx]\n",
        "\n",
        "    # Apply transforms\n",
        "    frames = torch.stack([transform(f) for f in frames])\n",
        "\n",
        "    # Predict\n",
        "    with torch.no_grad():\n",
        "        frames_batch = frames.unsqueeze(0).to(device)\n",
        "        logits, attention = model(frames_batch, return_attention=True)\n",
        "        probabilities = F.softmax(logits, dim=1)\n",
        "        pred_idx = torch.argmax(logits, dim=1).item()\n",
        "        confidence = probabilities[0, pred_idx].item()\n",
        "\n",
        "    predicted_label = idx_to_label.get(pred_idx, f\"Unknown ({pred_idx})\")\n",
        "\n",
        "    return {\n",
        "        'prediction': predicted_label,\n",
        "        'confidence': confidence,\n",
        "        'attention_weights': attention[0].cpu().numpy(),\n",
        "        'all_probabilities': probabilities[0].cpu().numpy()\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SNQvsPo-lmn"
      },
      "outputs": [],
      "source": [
        "# Example inference (uncomment and modify path to use)\n",
        "# result = predict_video(\n",
        "#     model=model,\n",
        "#     video_path=\"/path/to/your/video.mp4\",\n",
        "#     transform=val_transform,\n",
        "#     num_frames=NUM_FRAMES,\n",
        "#     label_map=train_dataset.label_map,\n",
        "#     device=device\n",
        "# )\n",
        "# print(f\"Prediction: {result['prediction']}\")\n",
        "# print(f\"Confidence: {result['confidence']:.2%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Srt60SNL-lmn"
      },
      "source": [
        "## 12. Save Final Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpDygB68-lmo"
      },
      "outputs": [],
      "source": [
        "# Save complete model for deployment\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'label_map': train_dataset.label_map,\n",
        "    'config': {\n",
        "        'num_classes': NUM_CLASSES,\n",
        "        'feature_dim': FEATURE_DIM,\n",
        "        'hidden_dim': HIDDEN_DIM,\n",
        "        'num_lstm_layers': NUM_LSTM_LAYERS,\n",
        "        'num_frames': NUM_FRAMES,\n",
        "        'dropout': DROPOUT\n",
        "    }\n",
        "}, 'sign2text_model_final.pth')\n",
        "\n",
        "print(\"Model saved to sign2text_model_final.pth\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7qt1Yws-lmo"
      },
      "source": [
        "---\n",
        "**Note:** The cells above contain the complete implementation. Make sure to run them in order from top to bottom.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6VTtt4e-lmo"
      },
      "outputs": [],
      "source": [
        "# PositionalEncoding class is defined below cell 7 - this cell can be ignored\n",
        "# The model requires running cells in sequential order\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}