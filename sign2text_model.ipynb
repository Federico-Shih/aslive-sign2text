{"cells":[{"cell_type":"markdown","metadata":{"id":"Luwl8hSq-lmd"},"source":["# ASLive Sign2Text Model\n","\n","This notebook implements the sign language to text model following the architecture:\n","- **Vision Layer (CNN)**: Extracts spatial features from each frame\n","- **Positional Encoding (PE)**: Adds temporal position information\n","- **Attention Layer (LSTM)**: Processes temporal sequence with attention\n","- **FC Layer**: Final classification layer\n"]},{"cell_type":"code","source":["# Before running, add everything from SQ_dataloader.ipynb into the cell below and run"],"metadata":{"id":"8q1A9uXSU7qG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WtCIKfOn-lmh"},"source":["## 1. Data Loading (from SQ_dataloader)\n"]},{"cell_type":"code","source":["#add SQ_dataloader code here\n","\n","\n","\n","# once this cell successfully runs, continue with the other cells"],"metadata":{"id":"vqaVkQmlVJE0","executionInfo":{"status":"ok","timestamp":1764398302277,"user_tz":360,"elapsed":10,"user":{"displayName":"Grace Jordan","userId":"02475253457536215741"}}},"execution_count":56,"outputs":[]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bsdOaDCZ-lmh","executionInfo":{"status":"ok","timestamp":1764395722994,"user_tz":360,"elapsed":10,"user":{"displayName":"Grace Jordan","userId":"02475253457536215741"}},"outputId":"2426f9b6-e684-497d-80f2-d7ed434c8000"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n"]}],"source":["import os\n","import json\n","import math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, models\n","from torchcodec.decoders import VideoDecoder\n","import numpy as np\n","from tqdm import tqdm\n","\n","# Set device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n"]},{"cell_type":"markdown","metadata":{"id":"_tfGCzOb-lmi"},"source":["## 2. Vision Layer (CNN Backbone)\n","\n","The Vision Layer extracts spatial features from each video frame using a CNN. We use a pretrained ResNet-18 as the backbone and remove the final classification layer to get feature embeddings.\n"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"jkESoYf_-lmi","executionInfo":{"status":"ok","timestamp":1764397283618,"user_tz":360,"elapsed":5,"user":{"displayName":"Grace Jordan","userId":"02475253457536215741"}}},"outputs":[],"source":["class VisionLayer(nn.Module):\n","    \"\"\"CNN backbone for extracting spatial features from video frames.\n","\n","    Uses pretrained ResNet-18 as feature extractor.\n","    Input: (batch, T, C, H, W) - batch of T frames\n","    Output: (batch, T, feature_dim) - feature vectors for each frame\n","    \"\"\"\n","\n","    def __init__(self, feature_dim=512, pretrained=True, freeze_backbone=False):\n","        super(VisionLayer, self).__init__()\n","\n","        # Load pretrained ResNet-18\n","        resnet = models.resnet18(weights='IMAGENET1K_V1' if pretrained else None)\n","\n","        # Remove the final FC layer\n","        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n","\n","        # ResNet-18 outputs 512-dim features\n","        self.resnet_feature_dim = 512\n","\n","        # Optional projection layer to adjust feature dimension\n","        if feature_dim != self.resnet_feature_dim:\n","            self.projection = nn.Linear(self.resnet_feature_dim, feature_dim)\n","        else:\n","            self.projection = None\n","\n","        self.feature_dim = feature_dim\n","\n","        # Freeze backbone if specified\n","        if freeze_backbone:\n","            for param in self.backbone.parameters():\n","                param.requires_grad = False\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Args:\n","            x: Input tensor of shape (batch, T, C, H, W)\n","        Returns:\n","            Feature tensor of shape (batch, T, feature_dim)\n","        \"\"\"\n","        batch_size, T, C, H, W = x.shape\n","\n","        # Reshape to process all frames together: (batch * T, C, H, W)\n","        x = x.view(batch_size * T, C, H, W)\n","\n","        # Extract features: (batch * T, 512, 1, 1)\n","        features = self.backbone(x)\n","\n","        # Flatten: (batch * T, 512)\n","        features = features.view(batch_size * T, -1)\n","\n","        # Project features if needed\n","        if self.projection is not None:\n","            features = self.projection(features)\n","\n","        # Reshape back: (batch, T, feature_dim)\n","        features = features.view(batch_size, T, self.feature_dim)\n","\n","        return features\n"]},{"cell_type":"markdown","metadata":{"id":"CIkvk_iX-lmj"},"source":["## 3. Positional Encoding (PE)\n","\n","Sinusoidal positional encoding adds temporal position information to the frame features before feeding them to the LSTM.\n"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"cfVsdetA-lmj","executionInfo":{"status":"ok","timestamp":1764397286994,"user_tz":360,"elapsed":7,"user":{"displayName":"Grace Jordan","userId":"02475253457536215741"}}},"outputs":[],"source":["class PositionalEncoding(nn.Module):\n","    \"\"\"Sinusoidal positional encoding for temporal sequences.\n","\n","    Adds position information to help the model understand the order of frames.\n","    \"\"\"\n","\n","    def __init__(self, d_model, max_len=500, dropout=0.1):\n","        super(PositionalEncoding, self).__init__()\n","\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        # Create positional encoding matrix\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","\n","        # Add batch dimension: (1, max_len, d_model)\n","        pe = pe.unsqueeze(0)\n","\n","        # Register as buffer (not a parameter, but should be saved/loaded)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Args:\n","            x: Input tensor of shape (batch, T, d_model)\n","        Returns:\n","            Tensor with positional encoding added: (batch, T, d_model)\n","        \"\"\"\n","        x = x + self.pe[:, :x.size(1), :]\n","        return self.dropout(x)\n"]},{"cell_type":"markdown","metadata":{"id":"vVq1WNB1-lmj"},"source":["## 4. Attention Layer (LSTM with Attention)\n","\n","Bidirectional LSTM processes the sequence of frame features, followed by an attention mechanism to weight the importance of different time steps.\n"]},{"cell_type":"code","execution_count":46,"metadata":{"id":"HCrnK27J-lmj","executionInfo":{"status":"ok","timestamp":1764397289946,"user_tz":360,"elapsed":3,"user":{"displayName":"Grace Jordan","userId":"02475253457536215741"}}},"outputs":[],"source":["class Attention(nn.Module):\n","    \"\"\"Attention mechanism for weighting LSTM outputs.\n","\n","    Computes attention weights over the sequence and returns a weighted sum.\n","    \"\"\"\n","\n","    def __init__(self, hidden_dim):\n","        super(Attention, self).__init__()\n","\n","        self.attention = nn.Sequential(\n","            nn.Linear(hidden_dim, hidden_dim // 2),\n","            nn.Tanh(),\n","            nn.Linear(hidden_dim // 2, 1)\n","        )\n","\n","    def forward(self, lstm_output):\n","        \"\"\"\n","        Args:\n","            lstm_output: LSTM outputs of shape (batch, T, hidden_dim)\n","        Returns:\n","            context: Weighted sum of shape (batch, hidden_dim)\n","            attention_weights: Attention weights of shape (batch, T)\n","        \"\"\"\n","        # Compute attention scores: (batch, T, 1)\n","        scores = self.attention(lstm_output)\n","\n","        # Apply softmax over time dimension: (batch, T, 1)\n","        attention_weights = F.softmax(scores, dim=1)\n","\n","        # Compute weighted sum: (batch, hidden_dim)\n","        context = torch.sum(attention_weights * lstm_output, dim=1)\n","\n","        return context, attention_weights.squeeze(-1)\n","\n","\n","class AttentionLSTM(nn.Module):\n","    \"\"\"Bidirectional LSTM with attention mechanism.\n","\n","    Processes temporal sequence of frame features and outputs a fixed-size representation.\n","    \"\"\"\n","\n","    def __init__(self, input_dim, hidden_dim=256, num_layers=2, dropout=0.3, bidirectional=True):\n","        super(AttentionLSTM, self).__init__()\n","\n","        self.hidden_dim = hidden_dim\n","        self.num_layers = num_layers\n","        self.bidirectional = bidirectional\n","        self.num_directions = 2 if bidirectional else 1\n","\n","        # LSTM layer\n","        self.lstm = nn.LSTM(\n","            input_size=input_dim,\n","            hidden_size=hidden_dim,\n","            num_layers=num_layers,\n","            batch_first=True,\n","            dropout=dropout if num_layers > 1 else 0,\n","            bidirectional=bidirectional\n","        )\n","\n","        # Attention mechanism\n","        self.attention = Attention(hidden_dim * self.num_directions)\n","\n","        # Output dimension\n","        self.output_dim = hidden_dim * self.num_directions\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Args:\n","            x: Input tensor of shape (batch, T, input_dim)\n","        Returns:\n","            output: Context vector of shape (batch, hidden_dim * num_directions)\n","            attention_weights: Attention weights of shape (batch, T)\n","        \"\"\"\n","        # LSTM forward pass: (batch, T, hidden_dim * num_directions)\n","        lstm_output, (hidden, cell) = self.lstm(x)\n","\n","        # Apply attention\n","        context, attention_weights = self.attention(lstm_output)\n","\n","        return context, attention_weights\n"]},{"cell_type":"markdown","metadata":{"id":"dXNLX5GV-lmj"},"source":["## 5. Complete Sign2Text Model\n","\n","Combines all components: Vision Layer → Positional Encoding → Attention LSTM → FC Layer → Classification\n"]},{"cell_type":"code","execution_count":47,"metadata":{"id":"pCKBEgo2-lmk","executionInfo":{"status":"ok","timestamp":1764397293235,"user_tz":360,"elapsed":42,"user":{"displayName":"Grace Jordan","userId":"02475253457536215741"}}},"outputs":[],"source":["class Sign2TextModel(nn.Module):\n","    \"\"\"Complete Sign Language to Text model.\n","\n","    Architecture:\n","    1. Vision Layer (CNN): Extract spatial features from each frame\n","    2. Positional Encoding: Add temporal position information\n","    3. Attention LSTM: Process temporal sequence with attention\n","    4. FC Layer: Final classification\n","    \"\"\"\n","\n","    def __init__(self, num_classes, feature_dim=512, hidden_dim=256,\n","                 num_lstm_layers=2, dropout=0.3, pretrained_cnn=True,\n","                 freeze_cnn=False, max_frames=100):\n","        super(Sign2TextModel, self).__init__()\n","\n","        # Vision Layer (CNN)\n","        self.vision_layer = VisionLayer(\n","            feature_dim=feature_dim,\n","            pretrained=pretrained_cnn,\n","            freeze_backbone=freeze_cnn\n","        )\n","\n","        # Positional Encoding\n","        self.positional_encoding = PositionalEncoding(\n","            d_model=feature_dim,\n","            max_len=max_frames,\n","            dropout=dropout\n","        )\n","\n","        # Attention Layer (LSTM)\n","        self.attention_lstm = AttentionLSTM(\n","            input_dim=feature_dim,\n","            hidden_dim=hidden_dim,\n","            num_layers=num_lstm_layers,\n","            dropout=dropout,\n","            bidirectional=True\n","        )\n","\n","        # FC Layer (Classification)\n","        self.fc_layer = nn.Sequential(\n","            nn.Linear(self.attention_lstm.output_dim, hidden_dim),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(hidden_dim, num_classes)\n","        )\n","\n","        self.num_classes = num_classes\n","\n","    def forward(self, x, return_attention=False):\n","        \"\"\"\n","        Args:\n","            x: Input video frames of shape (batch, T, C, H, W)\n","            return_attention: If True, also return attention weights\n","        Returns:\n","            logits: Classification logits of shape (batch, num_classes)\n","            attention_weights (optional): Attention weights of shape (batch, T)\n","        \"\"\"\n","        # Vision Layer: (batch, T, C, H, W) → (batch, T, feature_dim)\n","        features = self.vision_layer(x)\n","\n","        # Positional Encoding: (batch, T, feature_dim) → (batch, T, feature_dim)\n","        features = self.positional_encoding(features)\n","\n","        # Attention LSTM: (batch, T, feature_dim) → (batch, hidden_dim * 2)\n","        context, attention_weights = self.attention_lstm(features)\n","\n","        # FC Layer: (batch, hidden_dim * 2) → (batch, num_classes)\n","        logits = self.fc_layer(context)\n","\n","        if return_attention:\n","            return logits, attention_weights\n","        return logits\n"]},{"cell_type":"markdown","metadata":{"id":"srRI42gq-lmk"},"source":["## 6. Training Utilities\n"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"6UWY8AUW-lmk","executionInfo":{"status":"ok","timestamp":1764397297111,"user_tz":360,"elapsed":11,"user":{"displayName":"Grace Jordan","userId":"02475253457536215741"}}},"outputs":[],"source":["def train_epoch(model, dataloader, criterion, optimizer, device):\n","    \"\"\"Train the model for one epoch.\"\"\"\n","    model.train()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    progress_bar = tqdm(dataloader, desc=\"Training\")\n","    for frames, labels in progress_bar:\n","        frames = frames.to(device)\n","        labels = labels.to(device)\n","\n","        # Zero gradients\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        outputs = model(frames)\n","        loss = criterion(outputs, labels)\n","\n","        # Backward pass\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Statistics\n","        running_loss += loss.item() * frames.size(0)\n","        _, predicted = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","        progress_bar.set_postfix({\n","            'loss': loss.item(),\n","            'acc': 100 * correct / total\n","        })\n","\n","    epoch_loss = running_loss / total\n","    epoch_acc = 100 * correct / total\n","\n","    return epoch_loss, epoch_acc\n","\n","\n","def evaluate(model, dataloader, criterion, device):\n","    \"\"\"Evaluate the model on a dataset.\"\"\"\n","    model.eval()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for frames, labels in tqdm(dataloader, desc=\"Evaluating\"):\n","            frames = frames.to(device)\n","            labels = labels.to(device)\n","\n","            outputs = model(frames)\n","            loss = criterion(outputs, labels)\n","\n","            running_loss += loss.item() * frames.size(0)\n","            _, predicted = torch.max(outputs, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    epoch_loss = running_loss / total\n","    epoch_acc = 100 * correct / total\n","\n","    return epoch_loss, epoch_acc\n"]},{"cell_type":"markdown","metadata":{"id":"zFBEBTg4-lml"},"source":["## 7. Configuration and Setup\n"]},{"cell_type":"code","execution_count":49,"metadata":{"id":"kPsZzTQd-lml","executionInfo":{"status":"ok","timestamp":1764397300727,"user_tz":360,"elapsed":5,"user":{"displayName":"Grace Jordan","userId":"02475253457536215741"}}},"outputs":[],"source":["# ============================================\n","# CONFIGURATION - Modify these paths and hyperparameters\n","# ============================================\n","\n","# Data paths\n","JSON_PATH = \"/content/drive/MyDrive/wlasl_resized/wlasl-complete/WLASL_v0.3.json\"  # Path to WLASL JSON\n","VIDEO_DIR = \"/content/drive/MyDrive/wlasl_resized/wlasl-complete/videos\"  # Path to video directory\n","\n","# Model hyperparameters\n","NUM_FRAMES = 16          # Number of frames to sample from each video\n","FEATURE_DIM = 512        # CNN feature dimension\n","HIDDEN_DIM = 256         # LSTM hidden dimension\n","NUM_LSTM_LAYERS = 2      # Number of LSTM layers\n","DROPOUT = 0.3            # Dropout rate\n","\n","# Training hyperparameters\n","BATCH_SIZE = 8           # Batch size (adjust based on GPU memory)\n","LEARNING_RATE = 1e-4     # Learning rate\n","NUM_EPOCHS = 30          # Number of training epochs\n","WEIGHT_DECAY = 1e-4      # L2 regularization\n","\n","# Options\n","FREEZE_CNN = False       # Whether to freeze CNN backbone\n","PRETRAINED_CNN = True    # Use pretrained CNN weights\n"]},{"cell_type":"code","execution_count":50,"metadata":{"id":"l39WEIDb-lml","executionInfo":{"status":"ok","timestamp":1764397303142,"user_tz":360,"elapsed":47,"user":{"displayName":"Grace Jordan","userId":"02475253457536215741"}}},"outputs":[],"source":["# Data transforms for training and validation\n","train_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","val_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n"]},{"cell_type":"code","execution_count":51,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VnohaoY9-lml","executionInfo":{"status":"ok","timestamp":1764397309798,"user_tz":360,"elapsed":4186,"user":{"displayName":"Grace Jordan","userId":"02475253457536215741"}},"outputId":"c6a443b4-8fb5-4ee0-9f08-a60dbd755319"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of training samples: 14289\n","Number of validation samples: 3916\n","Number of test samples: 2878\n","Number of classes: 2000\n"]}],"source":["# Create datasets\n","train_dataset = WLASLTorchCodec(\n","    json_path=JSON_PATH,\n","    video_dir=VIDEO_DIR,\n","    split=\"train\",\n","    num_frames=NUM_FRAMES,\n","    transform=train_transform\n",")\n","\n","val_dataset = WLASLTorchCodec(\n","    json_path=JSON_PATH,\n","    video_dir=VIDEO_DIR,\n","    split=\"val\",\n","    num_frames=NUM_FRAMES,\n","    transform=val_transform\n",")\n","\n","test_dataset = WLASLTorchCodec(\n","    json_path=JSON_PATH,\n","    video_dir=VIDEO_DIR,\n","    split=\"test\",\n","    num_frames=NUM_FRAMES,\n","    transform=val_transform\n",")\n","\n","# Get number of classes from dataset\n","NUM_CLASSES = train_dataset.num_classes\n","\n","print(f\"Number of training samples: {len(train_dataset)}\")\n","print(f\"Number of validation samples: {len(val_dataset)}\")\n","print(f\"Number of test samples: {len(test_dataset)}\")\n","print(f\"Number of classes: {NUM_CLASSES}\")\n"]},{"cell_type":"code","execution_count":52,"metadata":{"id":"gpPAkm72-lml","executionInfo":{"status":"ok","timestamp":1764397315557,"user_tz":360,"elapsed":3,"user":{"displayName":"Grace Jordan","userId":"02475253457536215741"}}},"outputs":[],"source":["# Create data loaders\n","train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n","    num_workers=2,\n","    pin_memory=True\n",")\n","\n","val_loader = DataLoader(\n","    val_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=False,\n","    num_workers=2,\n","    pin_memory=True\n",")\n","\n","test_loader = DataLoader(\n","    test_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=False,\n","    num_workers=2,\n","    pin_memory=True\n",")\n"]},{"cell_type":"code","execution_count":53,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"abGiXTmk-lml","executionInfo":{"status":"ok","timestamp":1764397317497,"user_tz":360,"elapsed":56,"user":{"displayName":"Grace Jordan","userId":"02475253457536215741"}},"outputId":"1297ad28-ca61-45ca-98ce-6ac98f80220a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sign2TextModel(\n","  (vision_layer): VisionLayer(\n","    (backbone): Sequential(\n","      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU(inplace=True)\n","      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","      (4): Sequential(\n","        (0): BasicBlock(\n","          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (1): BasicBlock(\n","          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (5): Sequential(\n","        (0): BasicBlock(\n","          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): BasicBlock(\n","          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (6): Sequential(\n","        (0): BasicBlock(\n","          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): BasicBlock(\n","          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (7): Sequential(\n","        (0): BasicBlock(\n","          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): BasicBlock(\n","          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (8): AdaptiveAvgPool2d(output_size=(1, 1))\n","    )\n","  )\n","  (positional_encoding): PositionalEncoding(\n","    (dropout): Dropout(p=0.3, inplace=False)\n","  )\n","  (attention_lstm): AttentionLSTM(\n","    (lstm): LSTM(512, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n","    (attention): Attention(\n","      (attention): Sequential(\n","        (0): Linear(in_features=512, out_features=256, bias=True)\n","        (1): Tanh()\n","        (2): Linear(in_features=256, out_features=1, bias=True)\n","      )\n","    )\n","  )\n","  (fc_layer): Sequential(\n","    (0): Linear(in_features=512, out_features=256, bias=True)\n","    (1): ReLU()\n","    (2): Dropout(p=0.3, inplace=False)\n","    (3): Linear(in_features=256, out_features=2000, bias=True)\n","  )\n",")\n","\n","Total parameters: 15,107,345\n","Trainable parameters: 15,107,345\n"]}],"source":["# Initialize model\n","model = Sign2TextModel(\n","    num_classes=NUM_CLASSES,\n","    feature_dim=FEATURE_DIM,\n","    hidden_dim=HIDDEN_DIM,\n","    num_lstm_layers=NUM_LSTM_LAYERS,\n","    dropout=DROPOUT,\n","    pretrained_cnn=PRETRAINED_CNN,\n","    freeze_cnn=FREEZE_CNN,\n","    max_frames=NUM_FRAMES\n",").to(device)\n","\n","# Print model summary\n","print(model)\n","print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n","print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n"]},{"cell_type":"markdown","metadata":{"id":"dXgIymLR-lmm"},"source":["# Loss function and optimizer\n","\n"]},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.AdamW( model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY )"],"metadata":{"id":"TIND-0ZnWb5m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7xrr02DMWXiQ"},"source":["# Learning rate scheduler\n","\n"]},{"cell_type":"code","source":["#no verbose\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau( optimizer, mode='min', factor=0.5, patience=3 )"],"metadata":{"id":"1zMGIp0-G9vL","executionInfo":{"status":"ok","timestamp":1764397321459,"user_tz":360,"elapsed":3,"user":{"displayName":"Grace Jordan","userId":"02475253457536215741"}}},"execution_count":54,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sbpkZSb_-lmm"},"source":["## 8. Training Loop\n"]},{"cell_type":"code","execution_count":55,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":436},"id":"0jbK9sCf-lmm","executionInfo":{"status":"error","timestamp":1764397329755,"user_tz":360,"elapsed":6443,"user":{"displayName":"Grace Jordan","userId":"02475253457536215741"}},"outputId":"96f5d1cd-1578-4578-d1bd-b24b9e85e519"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Epoch 1/30\n","----------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training:   0%|          | 0/1787 [00:06<?, ?it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3286658422.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Validate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-279887407.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-2813346075.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, return_attention)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \"\"\"\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# Vision Layer: (batch, T, C, H, W) → (batch, T, feature_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvision_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# Positional Encoding: (batch, T, feature_dim) → (batch, T, feature_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-747033383.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Extract features: (batch * T, 512, 1, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# Flatten: (batch * T, 512)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \"\"\"\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    541\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             )\n\u001b[0;32m--> 543\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    544\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         )\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Training loop\n","best_val_acc = 0.0\n","history = {\n","    'train_loss': [], 'train_acc': [],\n","    'val_loss': [], 'val_acc': []\n","}\n","\n","for epoch in range(NUM_EPOCHS):\n","    print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n","    print(\"-\" * 40)\n","\n","    # Train\n","    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n","\n","    # Validate\n","    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n","\n","    # Update scheduler\n","    scheduler.step(val_loss)\n","\n","    # Save history\n","    history['train_loss'].append(train_loss)\n","    history['train_acc'].append(train_acc)\n","    history['val_loss'].append(val_loss)\n","    history['val_acc'].append(val_acc)\n","\n","    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n","    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n","\n","    # Save best model\n","    if val_acc > best_val_acc:\n","        best_val_acc = val_acc\n","        torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'val_acc': val_acc,\n","            'label_map': train_dataset.label_map\n","        }, 'best_model.pth')\n","        print(f\"✓ Saved new best model with Val Acc: {val_acc:.2f}%\")\n","\n","print(f\"\\nTraining complete! Best Val Acc: {best_val_acc:.2f}%\")\n"]},{"cell_type":"markdown","metadata":{"id":"A0_AbauW-lmm"},"source":["## 9. Evaluation and Visualization\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MxMS0YFc-lmm"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","# Plot training history\n","fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n","\n","# Loss plot\n","axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n","axes[0].plot(history['val_loss'], label='Val Loss', marker='s')\n","axes[0].set_xlabel('Epoch')\n","axes[0].set_ylabel('Loss')\n","axes[0].set_title('Training and Validation Loss')\n","axes[0].legend()\n","axes[0].grid(True)\n","\n","# Accuracy plot\n","axes[1].plot(history['train_acc'], label='Train Acc', marker='o')\n","axes[1].plot(history['val_acc'], label='Val Acc', marker='s')\n","axes[1].set_xlabel('Epoch')\n","axes[1].set_ylabel('Accuracy (%)')\n","axes[1].set_title('Training and Validation Accuracy')\n","axes[1].legend()\n","axes[1].grid(True)\n","\n","plt.tight_layout()\n","plt.savefig('training_history.png', dpi=150)\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N5fEQd22-lmm"},"outputs":[],"source":["# Load best model and evaluate on test set\n","checkpoint = torch.load('best_model.pth')\n","model.load_state_dict(checkpoint['model_state_dict'])\n","\n","test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n","print(f\"\\nTest Results:\")\n","print(f\"Test Loss: {test_loss:.4f}\")\n","print(f\"Test Accuracy: {test_acc:.2f}%\")\n"]},{"cell_type":"markdown","metadata":{"id":"fK5qG01d-lmn"},"source":["## 10. Attention Visualization\n","\n","Visualize which frames the model attends to most when making predictions.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FY2JOSmH-lmn"},"outputs":[],"source":["def visualize_attention(model, frames, true_label, label_map, device):\n","    \"\"\"Visualize attention weights over video frames.\"\"\"\n","    model.eval()\n","\n","    # Get reverse label map\n","    idx_to_label = {v: k for k, v in label_map.items()}\n","\n","    with torch.no_grad():\n","        # Add batch dimension\n","        frames_batch = frames.unsqueeze(0).to(device)\n","\n","        # Get predictions and attention weights\n","        logits, attention_weights = model(frames_batch, return_attention=True)\n","        pred_label = torch.argmax(logits, dim=1).item()\n","        attention = attention_weights[0].cpu().numpy()\n","\n","    # Create visualization\n","    num_frames = frames.shape[0]\n","    fig, axes = plt.subplots(2, num_frames, figsize=(2 * num_frames, 6))\n","\n","    # Denormalize frames for visualization\n","    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n","    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n","\n","    for i in range(num_frames):\n","        frame = frames[i].cpu()\n","        frame = frame * std + mean\n","        frame = frame.clamp(0, 1).permute(1, 2, 0).numpy()\n","\n","        # Frame image\n","        axes[0, i].imshow(frame)\n","        axes[0, i].set_title(f\"Frame {i+1}\")\n","        axes[0, i].axis('off')\n","\n","        # Attention weight bar\n","        axes[1, i].bar([0], [attention[i]], color='blue', alpha=0.7)\n","        axes[1, i].set_ylim(0, max(attention) * 1.2)\n","        axes[1, i].set_title(f\"{attention[i]:.3f}\")\n","        axes[1, i].axis('off')\n","\n","    plt.suptitle(\n","        f\"True: {idx_to_label.get(true_label, true_label)} | \"\n","        f\"Predicted: {idx_to_label.get(pred_label, pred_label)}\",\n","        fontsize=14\n","    )\n","    plt.tight_layout()\n","    plt.show()\n","\n","    return pred_label, attention\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5pOEoPik-lmn"},"outputs":[],"source":["# Visualize attention for a sample from the test set\n","sample_idx = 0\n","frames, label = test_dataset[sample_idx]\n","pred, attn = visualize_attention(model, frames, label, train_dataset.label_map, device)\n"]},{"cell_type":"markdown","metadata":{"id":"A1vGI8IA-lmn"},"source":["## 11. Inference Function\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6CXDTtOc-lmn"},"outputs":[],"source":["def predict_video(model, video_path, transform, num_frames, label_map, device):\n","    \"\"\"Predict the sign language class for a video file.\"\"\"\n","    model.eval()\n","\n","    # Get reverse label map\n","    idx_to_label = {v: k for k, v in label_map.items()}\n","\n","    # Decode video\n","    decoder = VideoDecoder(video_path)\n","    frames = []\n","\n","    for chunk in decoder:\n","        for frame_tensor in chunk:\n","            if frame_tensor.dim() == 2:\n","                frame_tensor = frame_tensor.unsqueeze(2)\n","            frame_chw = frame_tensor.permute(2, 0, 1)\n","            frame_pil = transforms.ToPILImage()(frame_chw)\n","            frames.append(frame_pil)\n","\n","    # Handle short videos\n","    while len(frames) < num_frames:\n","        frames.extend(frames)\n","\n","    # Sample frames\n","    T = len(frames)\n","    idx = torch.linspace(0, T - 1, num_frames).long()\n","    frames = [frames[i] for i in idx]\n","\n","    # Apply transforms\n","    frames = torch.stack([transform(f) for f in frames])\n","\n","    # Predict\n","    with torch.no_grad():\n","        frames_batch = frames.unsqueeze(0).to(device)\n","        logits, attention = model(frames_batch, return_attention=True)\n","        probabilities = F.softmax(logits, dim=1)\n","        pred_idx = torch.argmax(logits, dim=1).item()\n","        confidence = probabilities[0, pred_idx].item()\n","\n","    predicted_label = idx_to_label.get(pred_idx, f\"Unknown ({pred_idx})\")\n","\n","    return {\n","        'prediction': predicted_label,\n","        'confidence': confidence,\n","        'attention_weights': attention[0].cpu().numpy(),\n","        'all_probabilities': probabilities[0].cpu().numpy()\n","    }\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1SNQvsPo-lmn"},"outputs":[],"source":["# Example inference (uncomment and modify path to use)\n","# result = predict_video(\n","#     model=model,\n","#     video_path=\"/path/to/your/video.mp4\",\n","#     transform=val_transform,\n","#     num_frames=NUM_FRAMES,\n","#     label_map=train_dataset.label_map,\n","#     device=device\n","# )\n","# print(f\"Prediction: {result['prediction']}\")\n","# print(f\"Confidence: {result['confidence']:.2%}\")\n"]},{"cell_type":"markdown","metadata":{"id":"Srt60SNL-lmn"},"source":["## 12. Save Final Model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QpDygB68-lmo"},"outputs":[],"source":["# Save complete model for deployment\n","torch.save({\n","    'model_state_dict': model.state_dict(),\n","    'label_map': train_dataset.label_map,\n","    'config': {\n","        'num_classes': NUM_CLASSES,\n","        'feature_dim': FEATURE_DIM,\n","        'hidden_dim': HIDDEN_DIM,\n","        'num_lstm_layers': NUM_LSTM_LAYERS,\n","        'num_frames': NUM_FRAMES,\n","        'dropout': DROPOUT\n","    }\n","}, 'sign2text_model_final.pth')\n","\n","print(\"Model saved to sign2text_model_final.pth\")\n"]},{"cell_type":"markdown","metadata":{"id":"Q7qt1Yws-lmo"},"source":["---\n","**Note:** The cells above contain the complete implementation. Make sure to run them in order from top to bottom.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S6VTtt4e-lmo"},"outputs":[],"source":["# PositionalEncoding class is defined below cell 7 - this cell can be ignored\n","# The model requires running cells in sequential order\n"]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":0}