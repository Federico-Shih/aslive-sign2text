{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Luwl8hSq-lmd"
      },
      "source": [
        "# ASLive Sign2Text Model\n",
        "\n",
        "This notebook implements the sign language to text model following the architecture:\n",
        "- **Vision Layer (CNN)**: Extracts spatial features from each frame\n",
        "- **Positional Encoding (PE)**: Adds temporal position information\n",
        "- **Attention Layer (LSTM)**: Processes temporal sequence with attention\n",
        "- **FC Layer**: Final classification layer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kagglehub torchcodec torchvision\n",
        "!pip install git+https://github.com/facebookresearch/pytorchvideo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbsKbY7bA16g",
        "outputId": "5ffd35db-f249-4807-86b8-2482f554c412"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.12/dist-packages (0.3.13)\n",
            "Requirement already satisfied: torchcodec in /usr/local/lib/python3.12/dist-packages (0.9.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kagglehub) (25.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from kagglehub) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kagglehub) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: torch==2.9.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.9.0+cu126)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.5.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.9.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.9.0->torchvision) (3.0.3)\n",
            "Collecting git+https://github.com/facebookresearch/pytorchvideo\n",
            "  Cloning https://github.com/facebookresearch/pytorchvideo to /tmp/pip-req-build-av8zpa0l\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/pytorchvideo /tmp/pip-req-build-av8zpa0l\n",
            "  Resolved https://github.com/facebookresearch/pytorchvideo to commit 0f9a5e102e4d84972b829fd30e3c3f78c7c7fd1a\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: fvcore in /usr/local/lib/python3.12/dist-packages (from pytorchvideo==0.1.5) (0.1.5.post20221221)\n",
            "Requirement already satisfied: av in /usr/local/lib/python3.12/dist-packages (from pytorchvideo==0.1.5) (16.0.1)\n",
            "Requirement already satisfied: parameterized in /usr/local/lib/python3.12/dist-packages (from pytorchvideo==0.1.5) (0.9.0)\n",
            "Requirement already satisfied: iopath in /usr/local/lib/python3.12/dist-packages (from pytorchvideo==0.1.5) (0.1.10)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from pytorchvideo==0.1.5) (3.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from fvcore->pytorchvideo==0.1.5) (2.0.2)\n",
            "Requirement already satisfied: yacs>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from fvcore->pytorchvideo==0.1.5) (0.1.8)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from fvcore->pytorchvideo==0.1.5) (6.0.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from fvcore->pytorchvideo==0.1.5) (4.67.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.12/dist-packages (from fvcore->pytorchvideo==0.1.5) (3.2.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from fvcore->pytorchvideo==0.1.5) (11.3.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from fvcore->pytorchvideo==0.1.5) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from iopath->pytorchvideo==0.1.5) (4.15.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.12/dist-packages (from iopath->pytorchvideo==0.1.5) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Before running, add everything from SQ_dataloader.ipynb into the cell below and run"
      ],
      "metadata": {
        "id": "8q1A9uXSU7qG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtCIKfOn-lmh"
      },
      "source": [
        "## 1. Data Loading (from SQ_dataloader)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#add SQ_dataloader code here\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from torchcodec.decoders import VideoDecoder\n",
        "import kagglehub\n",
        "import os\n",
        "import json\n",
        "\n",
        "import torch # Assuming torch is imported elsewhere\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from torchcodec.decoders import VideoDecoder\n",
        "import kagglehub\n",
        "import os\n",
        "import json\n",
        "from PIL import Image # Needed for cropping if working with PIL images\n",
        "\n",
        "class WLASLTorchCodec(Dataset):\n",
        "  download_path = None\n",
        "\n",
        "  def __init__(self, json_path=None, video_dir=None, download=True, max_classes=None, split=\"train\", num_frames=32, transform=None):\n",
        "    print(\"Will download:\", download)\n",
        "    if (json_path is None or video_dir is None) and download == False:\n",
        "      raise ValueError(\"json_path and video_dir must be provided with download false\")\n",
        "    if download:\n",
        "      if WLASLTorchCodec.download_path is None:\n",
        "        path = kagglehub.dataset_download(\"sttaseen/wlasl2000-resized\")\n",
        "        WLASLTorchCodec.download_path = path\n",
        "      else:\n",
        "        path = WLASLTorchCodec.download_path\n",
        "      print(\"Downloaded at path: \", path)\n",
        "\n",
        "      self.video_dir = os.path.join(path, \"wlasl-complete\", \"videos\")\n",
        "      json_path = os.path.join(path, \"wlasl-complete\",\"WLASL_v0.3.json\")\n",
        "      downloaded = True\n",
        "    else:\n",
        "      self.video_dir = video_dir\n",
        "    self.num_frames = num_frames\n",
        "    self.transform = transform\n",
        "\n",
        "    # Read json\n",
        "    with open(json_path, \"r\") as f:\n",
        "      data = json.load(f)\n",
        "    if max_classes is not None:\n",
        "        if isinstance(max_classes, int):\n",
        "            # Keep only the first N entries (Usually the most frequent in WLASL)\n",
        "            data = data[:max_classes]\n",
        "            print(f\"Limiting dataset to top {max_classes} classes.\")\n",
        "        elif isinstance(max_classes, list):\n",
        "            # Keep only entries that match specific glosses\n",
        "            data = [entry for entry in data if entry['gloss'] in max_classes]\n",
        "            print(f\"Limiting dataset to {len(data)} specific classes.\")\n",
        "    self.samples = []\n",
        "    self.label_map = {}\n",
        "    label_id = 0\n",
        "\n",
        "    for entry in data:\n",
        "      gloss = entry[\"gloss\"]\n",
        "\n",
        "      if gloss not in self.label_map:\n",
        "        self.label_map[gloss] = label_id\n",
        "        label_id += 1\n",
        "\n",
        "      label = self.label_map[gloss]\n",
        "\n",
        "      for inst in entry[\"instances\"]:\n",
        "        if inst[\"split\"] != split:\n",
        "          continue\n",
        "\n",
        "        video_id = inst[\"video_id\"]\n",
        "        file_path = os.path.join(self.video_dir, f\"{video_id}.mp4\")\n",
        "\n",
        "        # 1. Modification in __init__: Extract and store frame/bbox info\n",
        "        frame_start = inst.get(\"frame_start\", 1) # Default to 1 if missing\n",
        "        frame_end = inst.get(\"frame_end\", -1)   # Default to -1 if missing\n",
        "        bbox = inst.get(\"bbox\", [0, 0, 1.0, 1.0]) # Default to normalized full frame if missing\n",
        "\n",
        "        if os.path.isfile(file_path):\n",
        "          # Store a tuple of (file_path, label, frame_start, frame_end, bbox)\n",
        "          self.samples.append((file_path, label, frame_start, frame_end, bbox))\n",
        "        self.num_classes = label_id\n",
        "  def __len__(self):\n",
        "    return len(self.samples)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # 2. Modification in __getitem__: Unpack all instance info\n",
        "    video_path, label, frame_start, frame_end, bbox = self.samples[idx]\n",
        "\n",
        "    # Convert WLASL 1-based indices (inclusive start, exclusive end) to\n",
        "    # torchcodec's 0-based indices (inclusive start, inclusive end).\n",
        "\n",
        "    decoder = VideoDecoder(video_path)\n",
        "    video_length = decoder.metadata.num_frames\n",
        "    end_frame = frame_end - 1 if frame_end > 0 else video_length\n",
        "    start_frame = 0\n",
        "    if end_frame > video_length:\n",
        "      end_frame = video_length\n",
        "    else:\n",
        "      end_frame = frame_end - 2 if frame_end > 0 else None\n",
        "    if frame_start > video_length:\n",
        "      start_frame = 0\n",
        "    else:\n",
        "      start_frame = frame_start - 1\n",
        "    frames = decoder[start_frame:end_frame]\n",
        "    if self.transform:\n",
        "      # Transform should handle T x C x H x W input\n",
        "      frames = self.transform(frames)\n",
        "    return frames, torch.tensor(label) # Ensure label is a tensor"
      ],
      "metadata": {
        "id": "vqaVkQmlVJE0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorchvideo.transforms as ptv_transforms\n",
        "from torchvision.transforms import Compose, Lambda\n",
        "\n",
        "\n",
        "mean = [0.45, 0.45, 0.45]\n",
        "std = [0.225, 0.225, 0.225]\n",
        "\n",
        "# Test out dataset\n",
        "train_transform = Compose(\n",
        "    [\n",
        "        # 1. Spatial Resize: Scale the shortest edge to SIDE_SIZE\n",
        "        ptv_transforms.UniformTemporalSubsample(num_samples=24, temporal_dim=0),\n",
        "        ptv_transforms.ConvertUint8ToFloat(),\n",
        "        Lambda(lambda x: x.permute(1, 0, 2, 3)),\n",
        "        ptv_transforms.Normalize(mean, std),\n",
        "        Lambda(lambda x: x.permute(1, 0, 2, 3)),\n",
        "        ptv_transforms.ShortSideScale(size=224),\n",
        "        # ptv_transforms.RandAugment(magnitude=6, num_layers=2),\n",
        "        # ptv_transforms.AugMix(magnitude=3),\n",
        "    ]\n",
        ")\n",
        "\n",
        "def show_frame(video, frame_idx):\n",
        "  import matplotlib.pyplot as plt\n",
        "  import numpy as np\n",
        "  single_frame = video[frame_idx]\n",
        "  frame_np = single_frame.detach().cpu().numpy()\n",
        "\n",
        "  frame_np = np.transpose(frame_np, (1, 2, 0))\n",
        "  plt.figure(figsize=(5, 5))\n",
        "  plt.imshow(frame_np)\n",
        "  plt.title(f'Frame {frame_idx} from Video Batch')\n",
        "  plt.axis('off') # Hide axis ticks and labels\n",
        "  plt.show()\n",
        "\n",
        "# clip = WLASLTorchCodec(max_classes=1, transform=train_transform)\n",
        "\n",
        "# for video, label in clip:\n",
        "#   show_frame(video, 0)"
      ],
      "metadata": {
        "id": "wK-FFUttRVGu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsdOaDCZ-lmh",
        "outputId": "f6a63978-2356-46c6-b2cb-7a7e6b3d65a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from torchcodec.decoders import VideoDecoder\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tfGCzOb-lmi"
      },
      "source": [
        "## 2. Vision Layer (CNN Backbone)\n",
        "\n",
        "The Vision Layer extracts spatial features from each video frame using a CNN. We use a pretrained ResNet-18 as the backbone and remove the final classification layer to get feature embeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jkESoYf_-lmi"
      },
      "outputs": [],
      "source": [
        "class VisionLayer(nn.Module):\n",
        "    \"\"\"CNN backbone for extracting spatial features from video frames.\n",
        "\n",
        "    Uses pretrained ResNet-18 as feature extractor.\n",
        "    Input: (batch, T, C, H, W) - batch of T frames\n",
        "    Output: (batch, T, feature_dim) - feature vectors for each frame\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, feature_dim=512, pretrained=True, freeze_backbone=False):\n",
        "        super(VisionLayer, self).__init__()\n",
        "\n",
        "        # Load pretrained ResNet-18\n",
        "        resnet = models.resnet18(weights='IMAGENET1K_V1' if pretrained else None)\n",
        "\n",
        "        # Remove the final FC layer\n",
        "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
        "\n",
        "        # ResNet-18 outputs 512-dim features\n",
        "        self.resnet_feature_dim = 512\n",
        "\n",
        "        # Optional projection layer to adjust feature dimension\n",
        "        if feature_dim != self.resnet_feature_dim:\n",
        "            self.projection = nn.Linear(self.resnet_feature_dim, feature_dim)\n",
        "        else:\n",
        "            self.projection = None\n",
        "\n",
        "        self.feature_dim = feature_dim\n",
        "\n",
        "        # Freeze backbone if specified\n",
        "        self.set_freeze_backbone(freeze_backbone)\n",
        "\n",
        "    def set_freeze_backbone(self, is_frozen):\n",
        "      for param in self.backbone.parameters():\n",
        "          param.requires_grad = not is_frozen\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch, T, C, H, W)\n",
        "        Returns:\n",
        "            Feature tensor of shape (batch, T, feature_dim)\n",
        "        \"\"\"\n",
        "        batch_size, T, C, H, W = x.shape\n",
        "\n",
        "        # Reshape to process all frames together: (batch * T, C, H, W)\n",
        "        x = x.view(batch_size * T, C, H, W)\n",
        "\n",
        "        # Extract features: (batch * T, 512, 1, 1)\n",
        "        features = self.backbone(x)\n",
        "\n",
        "        # Flatten: (batch * T, 512)\n",
        "        features = features.view(batch_size * T, -1)\n",
        "\n",
        "        # Project features if needed\n",
        "        if self.projection is not None:\n",
        "            features = self.projection(features)\n",
        "\n",
        "        # Reshape back: (batch, T, feature_dim)\n",
        "        features = features.view(batch_size, T, self.feature_dim)\n",
        "\n",
        "        return features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIkvk_iX-lmj"
      },
      "source": [
        "## 3. Positional Encoding (PE)\n",
        "\n",
        "Sinusoidal positional encoding adds temporal position information to the frame features before feeding them to the LSTM.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cfVsdetA-lmj"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Sinusoidal positional encoding for temporal sequences.\n",
        "\n",
        "    Adds position information to help the model understand the order of frames.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, max_len=500, dropout=0.1):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Create positional encoding matrix\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Add batch dimension: (1, max_len, d_model)\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        # Register as buffer (not a parameter, but should be saved/loaded)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch, T, d_model)\n",
        "        Returns:\n",
        "            Tensor with positional encoding added: (batch, T, d_model)\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVq1WNB1-lmj"
      },
      "source": [
        "## 4. Attention Layer (LSTM with Attention)\n",
        "\n",
        "Bidirectional LSTM processes the sequence of frame features, followed by an attention mechanism to weight the importance of different time steps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HCrnK27J-lmj"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"Attention mechanism for weighting LSTM outputs.\n",
        "\n",
        "    Computes attention weights over the sequence and returns a weighted sum.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim // 2, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, lstm_output):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            lstm_output: LSTM outputs of shape (batch, T, hidden_dim)\n",
        "        Returns:\n",
        "            context: Weighted sum of shape (batch, hidden_dim)\n",
        "            attention_weights: Attention weights of shape (batch, T)\n",
        "        \"\"\"\n",
        "        # Compute attention scores: (batch, T, 1)\n",
        "        scores = self.attention(lstm_output)\n",
        "\n",
        "        # Apply softmax over time dimension: (batch, T, 1)\n",
        "        attention_weights = F.softmax(scores, dim=1)\n",
        "\n",
        "        # Compute weighted sum: (batch, hidden_dim)\n",
        "        context = torch.sum(attention_weights * lstm_output, dim=1)\n",
        "\n",
        "        return context, attention_weights.squeeze(-1)\n",
        "\n",
        "\n",
        "class AttentionLSTM(nn.Module):\n",
        "    \"\"\"Bidirectional LSTM with attention mechanism.\n",
        "\n",
        "    Processes temporal sequence of frame features and outputs a fixed-size representation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim=256, num_layers=2, dropout=0.3, bidirectional=True):\n",
        "        super(AttentionLSTM, self).__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.bidirectional = bidirectional\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            bidirectional=bidirectional\n",
        "        )\n",
        "\n",
        "        # Attention mechanism\n",
        "        self.attention = Attention(hidden_dim * self.num_directions)\n",
        "\n",
        "        # Output dimension\n",
        "        self.output_dim = hidden_dim * self.num_directions\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch, T, input_dim)\n",
        "        Returns:\n",
        "            output: Context vector of shape (batch, hidden_dim * num_directions)\n",
        "            attention_weights: Attention weights of shape (batch, T)\n",
        "        \"\"\"\n",
        "        # LSTM forward pass: (batch, T, hidden_dim * num_directions)\n",
        "        lstm_output, (hidden, cell) = self.lstm(x)\n",
        "\n",
        "        # Apply attention\n",
        "        context, attention_weights = self.attention(lstm_output)\n",
        "\n",
        "        return context, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXNLX5GV-lmj"
      },
      "source": [
        "## 5. Complete Sign2Text Model\n",
        "\n",
        "Combines all components: Vision Layer → Positional Encoding → Attention LSTM → FC Layer → Classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "pCKBEgo2-lmk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models.video import r3d_18, R3D_18_Weights\n",
        "from torchvision.models.video import mc3_18, MC3_18_Weights\n",
        "\n",
        "class SlowFast(nn.Module):\n",
        "    def __init__(self, num_classes=100, pretrained=True, dropout=0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        # === 1. Backbones ===\n",
        "        slow_weights = R3D_18_Weights.KINETICS400_V1 if pretrained else None\n",
        "        slow_net = r3d_18(weights=slow_weights)\n",
        "\n",
        "        fast_weights = MC3_18_Weights.KINETICS400_V1 if pretrained else None\n",
        "        fast_net = mc3_18(weights=fast_weights)\n",
        "\n",
        "        # Split backbones\n",
        "        self.slow_path = nn.ModuleList([\n",
        "            slow_net.stem, slow_net.layer1, slow_net.layer2, slow_net.layer3, slow_net.layer4\n",
        "        ])\n",
        "        self.fast_path = nn.ModuleList([\n",
        "            fast_net.stem, fast_net.layer1, fast_net.layer2, fast_net.layer3, fast_net.layer4\n",
        "        ])\n",
        "\n",
        "        # === 2. Lateral Connections (Fixed Channels) ===\n",
        "        # Output channels must match the Slow pathway at that layer for summation\n",
        "        # Slow Layer1: 64, Layer2: 128, Layer3: 256, Layer4: 512\n",
        "        # We assume Fast path has same channel progression (64, 128, 256, 512)\n",
        "\n",
        "        # Note: We use stride=(8,1,1) assuming Fast input has 8x frames and we need to match Slow T.\n",
        "        # Padding ensures we don't lose frames excessively.\n",
        "        self.lateral_p3 = nn.Conv3d(64,  64,  kernel_size=(5,1,1), stride=(8,1,1), padding=(2,0,0), bias=False)\n",
        "        self.lateral_p4 = nn.Conv3d(128, 128, kernel_size=(5,1,1), stride=(8,1,1), padding=(2,0,0), bias=False)\n",
        "        self.lateral_p5 = nn.Conv3d(256, 256, kernel_size=(5,1,1), stride=(8,1,1), padding=(2,0,0), bias=False)\n",
        "\n",
        "        # === 3. Fusion + Head ===\n",
        "        slow_feat_dim = 512\n",
        "        fast_proj_dim = 64\n",
        "\n",
        "        # Fixed: Defined in __init__, not forward\n",
        "        self.fast_proj = nn.Conv3d(512, fast_proj_dim, kernel_size=1, bias=False)\n",
        "\n",
        "        # Fixed: Input dim accounts for concatenation (512 + 64)\n",
        "        fusion_dim = slow_feat_dim + fast_proj_dim\n",
        "\n",
        "        self.fusion = nn.Conv3d(fusion_dim, fusion_dim, kernel_size=1, bias=False)\n",
        "        self.bn_fusion = nn.BatchNorm3d(fusion_dim)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool3d(1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(fusion_dim, num_classes)\n",
        "\n",
        "        # === 4. Fixed Initialization ===\n",
        "        # Initialize ONLY new layers. Do NOT wipe out backbone weights.\n",
        "        new_layers = [self.lateral_p3, self.lateral_p4, self.lateral_p5,\n",
        "                      self.fast_proj, self.fusion, self.bn_fusion, self.fc]\n",
        "\n",
        "        for module in new_layers:\n",
        "            for m in module.modules():\n",
        "                if isinstance(m, nn.Conv3d):\n",
        "                    nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                elif isinstance(m, nn.BatchNorm3d):\n",
        "                    nn.init.constant_(m.weight, 1)\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _upsample_add(self, fast_feat, slow_feat):\n",
        "        # Match Temporal, Height, Width\n",
        "        target_shape = slow_feat.shape[2:] # (T, H, W)\n",
        "\n",
        "        fast_up = F.interpolate(\n",
        "            fast_feat,\n",
        "            size=target_shape,\n",
        "            mode='trilinear',\n",
        "            align_corners=False\n",
        "        )\n",
        "        return slow_feat + fast_up\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, C, H, W) -> (B, C, T, H, W)\n",
        "        x = x.permute(0, 2, 1, 3, 4)\n",
        "\n",
        "        # Slow: sample every 8th frame\n",
        "        slow = x[:, :, ::8, :, :]\n",
        "        # Fast: all frames\n",
        "        fast = x\n",
        "\n",
        "        # Block 0 (Stem)\n",
        "        slow = self.slow_path[0](slow)\n",
        "        fast = self.fast_path[0](fast)\n",
        "\n",
        "        # Block 1 + Lateral\n",
        "        slow = self.slow_path[1](slow)\n",
        "        fast = self.fast_path[1](fast)\n",
        "        lateral = self.lateral_p3(fast)\n",
        "        slow = self._upsample_add(lateral, slow)\n",
        "\n",
        "        # Block 2 + Lateral\n",
        "        slow = self.slow_path[2](slow)\n",
        "        fast = self.fast_path[2](fast)\n",
        "        lateral = self.lateral_p4(fast)\n",
        "        slow = self._upsample_add(lateral, slow)\n",
        "\n",
        "        # Block 3 + Lateral\n",
        "        slow = self.slow_path[3](slow)\n",
        "        fast = self.fast_path[3](fast)\n",
        "        lateral = self.lateral_p5(fast)\n",
        "        slow = self._upsample_add(lateral, slow)\n",
        "\n",
        "        # Block 4\n",
        "        slow = self.slow_path[4](slow)\n",
        "        fast = self.fast_path[4](fast)\n",
        "\n",
        "        # Final Fusion\n",
        "        # Project Fast (512 -> 64)\n",
        "        fast_final = self.fast_proj(fast)\n",
        "\n",
        "        # Match Slow spatial/temporal dims for concatenation\n",
        "        fast_final = F.interpolate(fast_final, size=slow.shape[2:], mode='trilinear', align_corners=False)\n",
        "\n",
        "        # Concatenate\n",
        "        x = torch.cat([slow, fast_final], dim=1) # (B, 576, T, H, W)\n",
        "\n",
        "        # Head\n",
        "        x = self.fusion(x)\n",
        "        x = self.bn_fusion(x)\n",
        "        x = F.relu_(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models.video import r3d_18, R3D_18_Weights\n",
        "from torchvision.models.video import mc3_18, MC3_18_Weights\n",
        "\n",
        "\n",
        "class SlowFast(nn.Module):\n",
        "    \"\"\"\n",
        "    Accepts: (B, T, C, H, W)\n",
        "    Returns: (B, num_classes)\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=100, pretrained=True, dropout=0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        # === Slow pathway (R3D-18) ===\n",
        "        slow_weights = R3D_18_Weights.KINETICS400_V1 if pretrained else None\n",
        "        slow_net = r3d_18(weights=slow_weights)\n",
        "\n",
        "        self.slow_path = nn.Sequential(\n",
        "            slow_net.stem,\n",
        "            slow_net.layer1,\n",
        "            slow_net.layer2,\n",
        "            slow_net.layer3,\n",
        "            slow_net.layer4\n",
        "        )\n",
        "        slow_feat_dim = 512\n",
        "\n",
        "        # === Fast pathway (MC3-18) ===\n",
        "        fast_weights = MC3_18_Weights.KINETICS400_V1 if pretrained else None\n",
        "        fast_net = mc3_18(weights=fast_weights)\n",
        "\n",
        "        self.fast_path = nn.Sequential(\n",
        "            fast_net.stem,\n",
        "            fast_net.layer1,\n",
        "            fast_net.layer2,\n",
        "            fast_net.layer3,\n",
        "            fast_net.layer4\n",
        "        )\n",
        "\n",
        "        # === Lateral connections (Fast → Slow) ===\n",
        "        # Match channel sizes exactly:\n",
        "        self.lateral_p3 = nn.Conv3d(64, 64, kernel_size=(5,1,1), stride=(8,1,1), padding=(2,0,0), bias=False)\n",
        "        self.lateral_p4 = nn.Conv3d(128, 128, kernel_size=(5,1,1), stride=(8,1,1), padding=(2,0,0), bias=False)\n",
        "        self.lateral_p5 = nn.Conv3d(256, 256, kernel_size=(5,1,1), stride=(8,1,1), padding=(2,0,0), bias=False)\n",
        "\n",
        "        # === Final fast projection (512 → 64) ===\n",
        "        self.fast_proj = nn.Conv3d(512, 64, kernel_size=1, bias=False)\n",
        "\n",
        "        # === Fusion + head ===\n",
        "        fusion_dim = slow_feat_dim + 64  # 512 + 64 = 576\n",
        "        self.fusion = nn.Conv3d(fusion_dim, fusion_dim, kernel_size=1, bias=False)\n",
        "        self.bn_fusion = nn.BatchNorm3d(fusion_dim)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool3d(1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(fusion_dim, num_classes)\n",
        "\n",
        "        # Weight initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv3d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm3d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _upsample_add(self, fast_feat, slow_feat):\n",
        "        _, _, T_f, H_f, W_f = fast_feat.shape\n",
        "        target_T, target_H, target_W = slow_feat.shape[2:]\n",
        "        fast_up = F.interpolate(\n",
        "            fast_feat,\n",
        "            size=(target_T, target_H, target_W),\n",
        "            mode='trilinear',\n",
        "            align_corners=False\n",
        "        )\n",
        "        return slow_feat + fast_up\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, C, H, W) → (B, C, T, H, W)\n",
        "        x = x.permute(0, 2, 1, 3, 4).contiguous()\n",
        "\n",
        "        # Slow pathway (sample 1/8)\n",
        "        slow_x = x[:, :, ::8]\n",
        "        slow = self.slow_path[0](slow_x)\n",
        "        slow = self.slow_path[1](slow)\n",
        "\n",
        "        # Fast pathway\n",
        "        fast = self.fast_path[0](x)\n",
        "        fast = self.fast_path[1](fast)\n",
        "\n",
        "        # Lateral p3\n",
        "        lateral = self.lateral_p3(fast)\n",
        "        slow = self._upsample_add(lateral, slow)\n",
        "\n",
        "        # layer2\n",
        "        slow = self.slow_path[2](slow)\n",
        "        fast = self.fast_path[2](fast)\n",
        "\n",
        "        lateral = self.lateral_p4(fast)\n",
        "        slow = self._upsample_add(lateral, slow)\n",
        "\n",
        "        # layer3\n",
        "        slow = self.slow_path[3](slow)\n",
        "        fast = self.fast_path[3](fast)\n",
        "\n",
        "        lateral = self.lateral_p5(fast)\n",
        "        slow = self._upsample_add(lateral, slow)\n",
        "\n",
        "        # layer4\n",
        "        slow = self.slow_path[4](slow)\n",
        "        fast = self.fast_path[4](fast)\n",
        "\n",
        "        # Final fast projection\n",
        "        fast_final = F.adaptive_avg_pool3d(fast, slow.shape[2:])\n",
        "        fast_final = self.fast_proj(fast_final)\n",
        "\n",
        "        # Concatenate\n",
        "        slow = torch.cat([slow, fast_final], dim=1)  # 512+64 = 576\n",
        "\n",
        "        # Fusion head\n",
        "        x = self.fusion(slow)\n",
        "        x = self.bn_fusion(x)\n",
        "        x = F.relu_(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "VdpKSoUFhmo9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srRI42gq-lmk"
      },
      "source": [
        "## 6. Training Utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6UWY8AUW-lmk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d05b22dd-6f4c-4689-fe39-3df5f39f2bcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3634111461.py:2: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        }
      ],
      "source": [
        "from torch.cuda.amp import autocast, GradScaler\n",
        "scaler = GradScaler()\n",
        "\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    \"\"\"Train the model for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
        "    for frames, labels in progress_bar:\n",
        "        frames = frames.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # print(frames.shape)\n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        with autocast():\n",
        "          outputs = model(frames)\n",
        "          loss = criterion(outputs, labels)\n",
        "        # Backward pass\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        # loss.backward()\n",
        "        # optimizer.step()\n",
        "\n",
        "        # Statistics\n",
        "        running_loss += loss.item() * frames.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        progress_bar.set_postfix({\n",
        "            'loss': loss.item(),\n",
        "            'acc': 100 * correct / total\n",
        "        })\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = 100 * correct / total\n",
        "\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    \"\"\"Evaluate the model on a dataset.\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for frames, labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            frames = frames.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            with autocast():\n",
        "                outputs = model(frames)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * frames.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = 100 * correct / total\n",
        "\n",
        "    return epoch_loss, epoch_acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFBEBTg4-lml"
      },
      "source": [
        "## 7. Configuration and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "kPsZzTQd-lml"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# CONFIGURATION - Modify these paths and hyperparameters\n",
        "# ============================================\n",
        "\n",
        "# Data paths\n",
        "JSON_PATH = \"/content/drive/MyDrive/wlasl_resized/wlasl-complete/WLASL_v0.3.json\"  # Path to WLASL JSON\n",
        "VIDEO_DIR = \"/content/drive/MyDrive/wlasl_resized/wlasl-complete/videos\"  # Path to video directory\n",
        "\n",
        "# Model hyperparameters\n",
        "NUM_FRAMES = 16           # Number of frames to sample from each video\n",
        "FEATURE_DIM = 512        # CNN feature dimension\n",
        "HIDDEN_DIM = 256         # LSTM hidden dimension\n",
        "NUM_LSTM_LAYERS = 2      # Number of LSTM layers\n",
        "DROPOUT = 0.0            # Dropout rate\n",
        "\n",
        "# Training hyperparameters\n",
        "BATCH_SIZE = 12           # Batch size (adjust based on GPU memory)\n",
        "LEARNING_RATE = 1e-4     # Learning rate\n",
        "NUM_EPOCHS = 30          # Number of training epochs\n",
        "WEIGHT_DECAY = 1e-4     # L2 regularization\n",
        "IMG_SIZE=168\n",
        "# Options\n",
        "FREEZE_CNN = True       # Whether to freeze CNN backbone\n",
        "PRETRAINED_CNN = True    # Use pretrained CNN weights\n",
        "WORKERS = 6\n",
        "EPOCHS_UNTIL_UNFREEZE = 50\n",
        "CLASSES_COUNT = 50\n",
        "PREFETCH = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "l39WEIDb-lml"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import v2\n",
        "# Data transforms for training and validation\n",
        "# train_transform = transforms.Compose([\n",
        "#     v2.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "#     # v2.RandomHorizontalFlip(),\n",
        "#     # v2.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "#     v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]),\n",
        "# ])\n",
        "\n",
        "# val_transform = transforms.Compose([\n",
        "#     v2.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "#     v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]),\n",
        "# ])\n",
        "\n",
        "from torchvision.transforms import Compose\n",
        "import pytorchvideo.transforms as ptv_transforms\n",
        "from pytorchvideo.transforms import functional as ptv_functional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Note: The transforms below expect the video tensor to be in the range [0.0, 1.0]\n",
        "# and of shape (T, C, H, W). The `WLASLTorchCodec` implementation already ensures\n",
        "# the shape is (T, C, H, W), but you must ensure the pixel values are converted\n",
        "# to float and normalized to [0, 1] before applying the standard normalization.\n",
        "\n",
        "\n",
        "\n",
        "mean = [0.45, 0.45, 0.45]\n",
        "std = [0.225, 0.225, 0.225]\n",
        "\n",
        "# Test out dataset\n",
        "train_transform = Compose(\n",
        "    [\n",
        "        # 1. Spatial Resize: Scale the shortest edge to SIDE_SIZE\n",
        "        ptv_transforms.UniformTemporalSubsample(num_samples=24, temporal_dim=0),\n",
        "        ptv_transforms.ConvertUint8ToFloat(),\n",
        "        Lambda(lambda x: x.permute(1, 0, 2, 3)),\n",
        "        ptv_transforms.Normalize(mean, std),\n",
        "        Lambda(lambda x: x.permute(1, 0, 2, 3)),\n",
        "        ptv_transforms.ShortSideScale(size=224),\n",
        "        ptv_transforms.RandAugment(magnitude=4, num_layers=2),\n",
        "        # ptv_transforms.AugMix(magnitude=3),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# train_transform = Compose(\n",
        "#     [\n",
        "#         # 1. Spatial Resize: Scale the shortest edge to SIDE_SIZE\n",
        "#         ptv_transforms.UniformTemporalSubsample(num_samples=NUM_FRAMES, temporal_dim=0),\n",
        "#         ptv_transforms.ConvertUint8ToFloat(),\n",
        "#         ptv_transforms.ShortSideScale(size=IMG_SIZE),\n",
        "#         ptv_transforms.RandAugment(magnitude=15, num_layers=2),\n",
        "#         ptv_transforms.AugMix(magnitude=3),\n",
        "#     ]\n",
        "# )\n",
        "\n",
        "test_transform = Compose(\n",
        "    [\n",
        "        ptv_transforms.UniformTemporalSubsample(num_samples=24, temporal_dim=0),\n",
        "        ptv_transforms.ConvertUint8ToFloat(),\n",
        "        Lambda(lambda x: x.permute(1, 0, 2, 3)),\n",
        "        ptv_transforms.Normalize(mean, std),\n",
        "        Lambda(lambda x: x.permute(1, 0, 2, 3)),\n",
        "        ptv_transforms.ShortSideScale(size=IMG_SIZE),\n",
        "    ]\n",
        ")\n",
        "val_transform =test_transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnohaoY9-lml",
        "outputId": "f3cc9906-e427-4115-a2ac-f9bdc12ea82b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Will download: True\n",
            "Downloaded at path:  /kaggle/input/wlasl2000-resized\n",
            "Limiting dataset to top 50 classes.\n",
            "Will download: True\n",
            "Downloaded at path:  /kaggle/input/wlasl2000-resized\n",
            "Limiting dataset to top 50 classes.\n",
            "Will download: True\n",
            "Downloaded at path:  /kaggle/input/wlasl2000-resized\n",
            "Limiting dataset to top 50 classes.\n",
            "Number of training samples: 785\n",
            "Number of validation samples: 183\n",
            "Number of test samples: 143\n",
            "Number of classes: 50\n"
          ]
        }
      ],
      "source": [
        "# Create datasets\n",
        "train_dataset = WLASLTorchCodec(\n",
        "    download=True,\n",
        "    split=\"train\",\n",
        "    max_classes=CLASSES_COUNT,\n",
        "    num_frames=NUM_FRAMES,\n",
        "    transform=train_transform\n",
        ")\n",
        "\n",
        "val_dataset = WLASLTorchCodec(\n",
        "    download=True,\n",
        "    split=\"val\",\n",
        "    max_classes=CLASSES_COUNT,\n",
        "    num_frames=NUM_FRAMES,\n",
        "    transform=val_transform\n",
        ")\n",
        "\n",
        "test_dataset = WLASLTorchCodec(\n",
        "    download=True,\n",
        "    split=\"test\",\n",
        "    max_classes=CLASSES_COUNT,\n",
        "    num_frames=NUM_FRAMES,\n",
        "    transform=val_transform\n",
        ")\n",
        "\n",
        "# Get number of classes from dataset\n",
        "NUM_CLASSES = train_dataset.num_classes\n",
        "\n",
        "print(f\"Number of training samples: {len(train_dataset)}\")\n",
        "print(f\"Number of validation samples: {len(val_dataset)}\")\n",
        "print(f\"Number of test samples: {len(test_dataset)}\")\n",
        "print(f\"Number of classes: {NUM_CLASSES}\")\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=WORKERS,           # Start high. The optimal value is often 4 to 12.\n",
        "    pin_memory=True,         # Accelerates the transfer of data from CPU to GPU VRAM.\n",
        "    persistent_workers=True, # Recommended for PyTorch multi-process workers to save epoch setup time.\n",
        "    prefetch_factor=PREFETCH\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=WORKERS,           # Start high. The optimal value is often 4 to 12.\n",
        "                             # Since video decoding is CPU-heavy, 8 is a good starting point.\n",
        "    pin_memory=True,         # Accelerates the transfer of data from CPU to GPU VRAM.\n",
        "    persistent_workers=True, # Recommended for PyTorch multi-process workers to save epoch setup time.\n",
        "    prefetch_factor=PREFETCH\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=WORKERS,           # Start high. The optimal value is often 4 to 12.\n",
        "                             # Since video decoding is CPU-heavy, 8 is a good starting point.\n",
        "    pin_memory=True,         # Accelerates the transfer of data from CPU to GPU VRAM.\n",
        "    persistent_workers=True, # Recommended for PyTorch multi-process workers to save epoch setup time.\n",
        "    prefetch_factor=PREFETCH\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abGiXTmk-lml",
        "outputId": "4047bb65-bb84-496c-9dba-7c21c37df120"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total parameters: 45,481,138\n",
            "Trainable parameters: 45,481,138\n"
          ]
        }
      ],
      "source": [
        "# Initialize model\n",
        "# model = Sign2TextModel(\n",
        "#     num_classes=NUM_CLASSES,\n",
        "#     feature_dim=FEATURE_DIM,\n",
        "#     hidden_dim=HIDDEN_DIM,\n",
        "#     num_lstm_layers=NUM_LSTM_LAYERS,\n",
        "#     dropout=DROPOUT,\n",
        "#     pretrained_cnn=PRETRAINED_CNN,\n",
        "#     freeze_cnn=FREEZE_CNN,\n",
        "#     max_frames=NUM_FRAMES\n",
        "# ).to(device)\n",
        "\n",
        "model = SlowFast(num_classes=NUM_CLASSES, dropout=DROPOUT).to(device)\n",
        "\n",
        "# model = SignTimeSformer(\n",
        "#     num_classes=NUM_CLASSES,\n",
        "#     img_size=IMG_SIZE,\n",
        "#     num_frames=NUM_FRAMES,\n",
        "#     heads=12,\n",
        "#     L=5,\n",
        "#     dropout=DROPOUT\n",
        "# ).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW( model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY )\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau( optimizer, mode='min', factor=0.5, patience=3 )\n",
        "\n",
        "# Print model summary\n",
        "# print(model)\n",
        "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbpkZSb_-lmm"
      },
      "source": [
        "## 8. Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jbK9sCf-lmm",
        "outputId": "aa4d7851-f178-43eb-c8c5-5ff8f96c2e69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/30\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/66 [00:00<?, ?it/s]/tmp/ipython-input-3634111461.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Training: 100%|██████████| 66/66 [04:55<00:00,  4.48s/it, loss=4.02, acc=1.78]\n",
            "Evaluating:   0%|          | 0/16 [00:00<?, ?it/s]/tmp/ipython-input-3634111461.py:63: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Evaluating: 100%|██████████| 16/16 [00:11<00:00,  1.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 3.9655, Train Acc: 1.78%\n",
            "Val Loss: 3.9505, Val Acc: 2.73%\n",
            "✓ Saved new best model with Val Acc: 2.73%\n",
            "\n",
            "Epoch 2/30\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 66/66 [04:52<00:00,  4.44s/it, loss=4.03, acc=2.55]\n",
            "Evaluating: 100%|██████████| 16/16 [00:10<00:00,  1.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 3.9399, Train Acc: 2.55%\n",
            "Val Loss: 3.9272, Val Acc: 3.28%\n",
            "✓ Saved new best model with Val Acc: 3.28%\n",
            "\n",
            "Epoch 3/30\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 66/66 [04:52<00:00,  4.43s/it, loss=3.82, acc=3.31]\n",
            "Evaluating: 100%|██████████| 16/16 [00:10<00:00,  1.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 3.9274, Train Acc: 3.31%\n",
            "Val Loss: 3.9735, Val Acc: 4.37%\n",
            "✓ Saved new best model with Val Acc: 4.37%\n",
            "\n",
            "Epoch 4/30\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 66/66 [04:52<00:00,  4.43s/it, loss=4.04, acc=4.59]\n",
            "Evaluating: 100%|██████████| 16/16 [00:10<00:00,  1.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 3.9087, Train Acc: 4.59%\n",
            "Val Loss: 4.0103, Val Acc: 3.28%\n",
            "\n",
            "Epoch 5/30\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 66/66 [04:52<00:00,  4.43s/it, loss=3.73, acc=4.2]\n",
            "Evaluating: 100%|██████████| 16/16 [00:10<00:00,  1.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 3.9067, Train Acc: 4.20%\n",
            "Val Loss: 3.9655, Val Acc: 3.83%\n",
            "\n",
            "Epoch 6/30\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  38%|███▊      | 25/66 [01:53<03:02,  4.44s/it, loss=3.74, acc=4.67]"
          ]
        }
      ],
      "source": [
        "from typing import *\n",
        "# Training loop\n",
        "best_val_acc = 0.0\n",
        "history = {\n",
        "    'train_loss': [], 'train_acc': [],\n",
        "    'val_loss': [], 'val_acc': []\n",
        "}\n",
        "\n",
        "from torch.utils.flop_counter import FlopCounterMode\n",
        "\n",
        "def get_flops(model, inp: Union[torch.Tensor, Tuple], with_backward=False):\n",
        "\n",
        "    istrain = model.training\n",
        "    model.eval()\n",
        "\n",
        "    inp = inp if isinstance(inp, torch.Tensor) else torch.randn(inp)\n",
        "\n",
        "    flop_counter = FlopCounterMode(mods=model, display=False, depth=None)\n",
        "    with flop_counter:\n",
        "        if with_backward:\n",
        "            model(inp).sum().backward()\n",
        "        else:\n",
        "            model(inp)\n",
        "    total_flops =  flop_counter.get_total_flops()\n",
        "    if istrain:\n",
        "        model.train()\n",
        "    return total_flops\n",
        "\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n",
        "    print(\"-\" * 40)\n",
        "    # if epoch > EPOCHS_UNTIL_UNFREEZE and FREEZE_CNN:\n",
        "    #     model.set_freeze(False)\n",
        "    # Train\n",
        "\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    # train_flops = get_flops(model, )\n",
        "    # Validate\n",
        "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "    # Update scheduler\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Save history\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_acc': val_acc,\n",
        "            'label_map': train_dataset.label_map\n",
        "        }, 'best_model.pth')\n",
        "        print(f\"✓ Saved new best model with Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "print(f\"\\nTraining complete! Best Val Acc: {best_val_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0_AbauW-lmm"
      },
      "source": [
        "## 9. Evaluation and Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxMS0YFc-lmm"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training history\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Loss plot\n",
        "axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
        "axes[0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Training and Validation Loss')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True)\n",
        "\n",
        "# Accuracy plot\n",
        "axes[1].plot(history['train_acc'], label='Train Acc', marker='o')\n",
        "axes[1].plot(history['val_acc'], label='Val Acc', marker='s')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Accuracy (%)')\n",
        "axes[1].set_title('Training and Validation Accuracy')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_history.png', dpi=150)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5fEQd22-lmm"
      },
      "outputs": [],
      "source": [
        "# Load best model and evaluate on test set\n",
        "checkpoint = torch.load('best_model.pth')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
        "print(f\"\\nTest Results:\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK5qG01d-lmn"
      },
      "source": [
        "## 10. Attention Visualization\n",
        "\n",
        "Visualize which frames the model attends to most when making predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FY2JOSmH-lmn"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def visualize_attention(model, frames, true_label, label_map, device):\n",
        "    \"\"\"Visualize attention weights over video frames.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Get reverse label map\n",
        "    idx_to_label = {v: k for k, v in label_map.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Add batch dimension\n",
        "        frames_batch = frames.unsqueeze(0).to(device)\n",
        "\n",
        "        # Get predictions and attention weights\n",
        "        logits, attention_weights = model(frames_batch, return_attention=True)\n",
        "        pred_label = torch.argmax(logits, dim=1).item()\n",
        "        attention = attention_weights[0].cpu().numpy()\n",
        "\n",
        "    # Create visualization\n",
        "    num_frames = frames.shape[0]\n",
        "    fig, axes = plt.subplots(2, num_frames, figsize=(2 * num_frames, 6))\n",
        "\n",
        "    # Denormalize frames for visualization\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "\n",
        "    for i in range(num_frames):\n",
        "        frame = frames[i].cpu()\n",
        "        frame = frame * std + mean\n",
        "        frame = frame.clamp(0, 1).permute(1, 2, 0).numpy()\n",
        "\n",
        "        # Frame image\n",
        "        axes[0, i].imshow(frame)\n",
        "        axes[0, i].set_title(f\"Frame {i+1}\")\n",
        "        axes[0, i].axis('off')\n",
        "\n",
        "        # Attention weight bar\n",
        "        axes[1, i].bar([0], [attention[i]], color='blue', alpha=0.7)\n",
        "        axes[1, i].set_ylim(0, max(attention) * 1.2)\n",
        "        axes[1, i].set_title(f\"{attention[i]:.3f}\")\n",
        "        axes[1, i].axis('off')\n",
        "\n",
        "    plt.suptitle(\n",
        "        f\"True: {idx_to_label.get(true_label, true_label)} | \"\n",
        "        f\"Predicted: {idx_to_label.get(pred_label, pred_label)}\",\n",
        "        fontsize=14\n",
        "    )\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return pred_label, attention\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pOEoPik-lmn"
      },
      "outputs": [],
      "source": [
        "# Visualize attention for a sample from the test set\n",
        "sample_idx = 0\n",
        "frames, label = test_dataset[sample_idx]\n",
        "pred, attn = visualize_attention(model, frames, label, train_dataset.label_map, device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1vGI8IA-lmn"
      },
      "source": [
        "## 11. Inference Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CXDTtOc-lmn"
      },
      "outputs": [],
      "source": [
        "def predict_video(model, video_path, transform, num_frames, label_map, device):\n",
        "    \"\"\"Predict the sign language class for a video file.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Get reverse label map\n",
        "    idx_to_label = {v: k for k, v in label_map.items()}\n",
        "\n",
        "    # Decode video\n",
        "    decoder = VideoDecoder(video_path)\n",
        "    frames = []\n",
        "\n",
        "    for chunk in decoder:\n",
        "        for frame_tensor in chunk:\n",
        "            if frame_tensor.dim() == 2:\n",
        "                frame_tensor = frame_tensor.unsqueeze(2)\n",
        "            frame_chw = frame_tensor.permute(2, 0, 1)\n",
        "            frame_pil = transforms.ToPILImage()(frame_chw)\n",
        "            frames.append(frame_pil)\n",
        "\n",
        "    # Handle short videos\n",
        "    while len(frames) < num_frames:\n",
        "        frames.extend(frames)\n",
        "\n",
        "    # Sample frames\n",
        "    T = len(frames)\n",
        "    idx = torch.linspace(0, T - 1, num_frames).long()\n",
        "    frames = [frames[i] for i in idx]\n",
        "\n",
        "    # Apply transforms\n",
        "    frames = torch.stack([transform(f) for f in frames])\n",
        "\n",
        "    # Predict\n",
        "    with torch.no_grad():\n",
        "        frames_batch = frames.unsqueeze(0).to(device)\n",
        "        logits, attention = model(frames_batch, return_attention=True)\n",
        "        probabilities = F.softmax(logits, dim=1)\n",
        "        pred_idx = torch.argmax(logits, dim=1).item()\n",
        "        confidence = probabilities[0, pred_idx].item()\n",
        "\n",
        "    predicted_label = idx_to_label.get(pred_idx, f\"Unknown ({pred_idx})\")\n",
        "\n",
        "    return {\n",
        "        'prediction': predicted_label,\n",
        "        'confidence': confidence,\n",
        "        'attention_weights': attention[0].cpu().numpy(),\n",
        "        'all_probabilities': probabilities[0].cpu().numpy()\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SNQvsPo-lmn"
      },
      "outputs": [],
      "source": [
        "# Example inference (uncomment and modify path to use)\n",
        "# result = predict_video(\n",
        "#     model=model,\n",
        "#     video_path=\"/path/to/your/video.mp4\",\n",
        "#     transform=val_transform,\n",
        "#     num_frames=NUM_FRAMES,\n",
        "#     label_map=train_dataset.label_map,\n",
        "#     device=device\n",
        "# )\n",
        "# print(f\"Prediction: {result['prediction']}\")\n",
        "# print(f\"Confidence: {result['confidence']:.2%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Srt60SNL-lmn"
      },
      "source": [
        "## 12. Save Final Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpDygB68-lmo"
      },
      "outputs": [],
      "source": [
        "# Save complete model for deployment\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'label_map': train_dataset.label_map,\n",
        "    'config': {\n",
        "        'num_classes': NUM_CLASSES,\n",
        "        'feature_dim': FEATURE_DIM,\n",
        "        'hidden_dim': HIDDEN_DIM,\n",
        "        'num_lstm_layers': NUM_LSTM_LAYERS,\n",
        "        'num_frames': NUM_FRAMES,\n",
        "        'dropout': DROPOUT\n",
        "    }\n",
        "}, 'sign2text_model_final.pth')\n",
        "\n",
        "print(\"Model saved to sign2text_model_final.pth\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7qt1Yws-lmo"
      },
      "source": [
        "---\n",
        "**Note:** The cells above contain the complete implementation. Make sure to run them in order from top to bottom.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6VTtt4e-lmo"
      },
      "outputs": [],
      "source": [
        "# PositionalEncoding class is defined below cell 7 - this cell can be ignored\n",
        "# The model requires running cells in sequential order\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}