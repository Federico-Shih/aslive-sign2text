{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Luwl8hSq-lmd"
      },
      "source": [
        "# ASLive Sign2Text Model\n",
        "\n",
        "This notebook implements the sign language to text model following the architecture:\n",
        "- **Vision Layer (CNN)**: Extracts spatial features from each frame\n",
        "- **Positional Encoding (PE)**: Adds temporal position information\n",
        "- **Attention Layer (LSTM)**: Processes temporal sequence with attention\n",
        "- **FC Layer**: Final classification layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbsKbY7bA16g",
        "outputId": "6104cad8-c56a-410a-d3e0-0782f3c61b54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kagglehub in c:\\users\\zach\\anaconda3\\lib\\site-packages (0.3.13)\n",
            "Requirement already satisfied: torchcodec in c:\\users\\zach\\anaconda3\\lib\\site-packages (0.8.1)\n",
            "Requirement already satisfied: torchvision in c:\\users\\zach\\anaconda3\\lib\\site-packages (0.24.1)\n",
            "Requirement already satisfied: packaging in c:\\users\\zach\\anaconda3\\lib\\site-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: pyyaml in c:\\users\\zach\\anaconda3\\lib\\site-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: requests in c:\\users\\zach\\anaconda3\\lib\\site-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\zach\\anaconda3\\lib\\site-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: numpy in c:\\users\\zach\\anaconda3\\lib\\site-packages (from torchvision) (2.1.3)\n",
            "Requirement already satisfied: torch==2.9.1 in c:\\users\\zach\\anaconda3\\lib\\site-packages (from torchvision) (2.9.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\zach\\anaconda3\\lib\\site-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\zach\\anaconda3\\lib\\site-packages (from torch==2.9.1->torchvision) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\zach\\anaconda3\\lib\\site-packages (from torch==2.9.1->torchvision) (4.12.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\zach\\anaconda3\\lib\\site-packages (from torch==2.9.1->torchvision) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\zach\\anaconda3\\lib\\site-packages (from torch==2.9.1->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\zach\\anaconda3\\lib\\site-packages (from torch==2.9.1->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\zach\\anaconda3\\lib\\site-packages (from torch==2.9.1->torchvision) (2025.3.2)\n",
            "Requirement already satisfied: setuptools in c:\\users\\zach\\anaconda3\\lib\\site-packages (from torch==2.9.1->torchvision) (72.1.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\zach\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch==2.9.1->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\zach\\anaconda3\\lib\\site-packages (from jinja2->torch==2.9.1->torchvision) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\zach\\anaconda3\\lib\\site-packages (from requests->kagglehub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\zach\\anaconda3\\lib\\site-packages (from requests->kagglehub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\zach\\anaconda3\\lib\\site-packages (from requests->kagglehub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\zach\\anaconda3\\lib\\site-packages (from requests->kagglehub) (2025.11.12)\n",
            "Requirement already satisfied: colorama in c:\\users\\zach\\anaconda3\\lib\\site-packages (from tqdm->kagglehub) (0.4.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install kagglehub torchcodec torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "8q1A9uXSU7qG"
      },
      "outputs": [],
      "source": [
        "# Before running, add everything from SQ_dataloader.ipynb into the cell below and run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtCIKfOn-lmh"
      },
      "source": [
        "## 1. Data Loading (from SQ_dataloader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "vqaVkQmlVJE0"
      },
      "outputs": [],
      "source": [
        "#add SQ_dataloader code here\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from torchcodec.decoders import VideoDecoder\n",
        "import kagglehub\n",
        "\n",
        "class WLASLTorchCodec(Dataset):\n",
        "  def __init__(self, json_path=None, video_dir=None, download=True, split=\"train\", num_frames=32, transform=None, max_samples=None):\n",
        "    if (json_path is None or video_dir is None) and not download:\n",
        "      raise ValueError(\"json_path and video_dir must be provided when download=False\")\n",
        "    if download:\n",
        "      path = kagglehub.dataset_download(\"sttaseen/wlasl2000-resized\")\n",
        "      print(\"Downloaded at path: \", path)\n",
        "      self.video_dir = os.path.join(path, \"wlasl-complete\", \"videos\")\n",
        "      json_path = os.path.join(path, \"wlasl-complete\", \"WLASL_v0.3.json\")\n",
        "    else:\n",
        "      self.video_dir = video_dir\n",
        "    self.num_frames = num_frames\n",
        "    self.transform = transform\n",
        "\n",
        "    # Read json\n",
        "    with open(json_path, \"r\") as f:\n",
        "      data = json.load(f)\n",
        "\n",
        "    self.samples = []\n",
        "    self.label_map = {}\n",
        "    label_id = 0\n",
        "\n",
        "    for entry in data:\n",
        "      gloss = entry[\"gloss\"]\n",
        "\n",
        "      if gloss not in self.label_map:\n",
        "        self.label_map[gloss] = label_id\n",
        "        label_id += 1\n",
        "\n",
        "      label = self.label_map[gloss]\n",
        "\n",
        "      for inst in entry[\"instances\"]:\n",
        "        if inst[\"split\"] != split:\n",
        "          continue\n",
        "\n",
        "        video_id = inst[\"video_id\"]\n",
        "        file_path = os.path.join(self.video_dir, f\"{video_id}.mp4\")\n",
        "\n",
        "        if os.path.isfile(file_path):\n",
        "          self.samples.append((file_path, label))\n",
        "    \n",
        "    # Limit samples for quick testing\n",
        "    if max_samples is not None and len(self.samples) > max_samples:\n",
        "      self.samples = self.samples[:max_samples]\n",
        "    \n",
        "    self.num_classes = label_id\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.samples)\n",
        "\n",
        "  def _sample_frames(self, frames):\n",
        "    \"\"\"frames is a list of PIL images or Tensors.\"\"\"\n",
        "    T = len(frames)\n",
        "    idx = torch.linspace(0, T - 1, self.num_frames).long()\n",
        "    return [frames[i] for i in idx]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    video_path, label = self.samples[idx]\n",
        "\n",
        "    decoder = VideoDecoder(video_path)\n",
        "\n",
        "    frames = []\n",
        "    for chunk in decoder:\n",
        "      # chunk is a Tensor of frames: (T, H, W) OR (T, H, W, C)\n",
        "      for frame_tensor in chunk:\n",
        "        # Handle grayscale frames (H, W)\n",
        "        if frame_tensor.dim() == 2:\n",
        "          frame_tensor = frame_tensor.unsqueeze(2)  # → (H, W, 1)\n",
        "\n",
        "        # Handle RGB frames (H, W, C)\n",
        "        if frame_tensor.dim() == 3:\n",
        "          pass\n",
        "        else:\n",
        "          raise ValueError(f\"Unexpected frame shape: {frame_tensor.shape}\")\n",
        "\n",
        "        # Convert to C x H x W\n",
        "        frame_chw = frame_tensor.permute(2, 0, 1)\n",
        "\n",
        "        frame_pil = transforms.ToPILImage()(frame_chw)\n",
        "        # Ensure RGB (some videos are grayscale)\n",
        "        frame_pil = frame_pil.convert('RGB')\n",
        "        frames.append(frame_pil)\n",
        "\n",
        "    # Guard for short videos\n",
        "    if len(frames) < self.num_frames:\n",
        "        while len(frames) < self.num_frames:\n",
        "            frames.extend(frames)\n",
        "        frames = frames[: self.num_frames]\n",
        "\n",
        "    # Uniform sampling\n",
        "    frames = self._sample_frames(frames)\n",
        "\n",
        "    if self.transform:\n",
        "      frames = torch.stack([self.transform(f) for f in frames])\n",
        "    else:\n",
        "      frames = torch.stack([transforms.ToTensor()(f) for f in frames])\n",
        "\n",
        "    return frames, label\n",
        "\n",
        "\n",
        "\n",
        "# once this cell successfully runs, continue with the other cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsdOaDCZ-lmh",
        "outputId": "2426f9b6-e684-497d-80f2-d7ed434c8000"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from torchcodec.decoders import VideoDecoder\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tfGCzOb-lmi"
      },
      "source": [
        "## 2. Vision Layer (CNN Backbone)\n",
        "\n",
        "The Vision Layer extracts spatial features from each video frame using a CNN. We use a pretrained ResNet-18 as the backbone and remove the final classification layer to get feature embeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "jkESoYf_-lmi"
      },
      "outputs": [],
      "source": [
        "class VisionLayer(nn.Module):\n",
        "    \"\"\"CNN backbone for extracting spatial features from video frames.\n",
        "\n",
        "    Uses pretrained ResNet-18 as feature extractor.\n",
        "    Input: (batch, T, C, H, W) - batch of T frames\n",
        "    Output: (batch, T, feature_dim) - feature vectors for each frame\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, feature_dim=512, pretrained=True, freeze_backbone=False):\n",
        "        super(VisionLayer, self).__init__()\n",
        "\n",
        "        # Load pretrained ResNet-18\n",
        "        resnet = models.resnet18(weights='IMAGENET1K_V1' if pretrained else None)\n",
        "\n",
        "        # Remove the final FC layer\n",
        "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
        "\n",
        "        # ResNet-18 outputs 512-dim features\n",
        "        self.resnet_feature_dim = 512\n",
        "\n",
        "        # Optional projection layer to adjust feature dimension\n",
        "        if feature_dim != self.resnet_feature_dim:\n",
        "            self.projection = nn.Linear(self.resnet_feature_dim, feature_dim)\n",
        "        else:\n",
        "            self.projection = None\n",
        "\n",
        "        self.feature_dim = feature_dim\n",
        "\n",
        "        # Freeze backbone if specified\n",
        "        if freeze_backbone:\n",
        "            for param in self.backbone.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch, T, C, H, W)\n",
        "        Returns:\n",
        "            Feature tensor of shape (batch, T, feature_dim)\n",
        "        \"\"\"\n",
        "        batch_size, T, C, H, W = x.shape\n",
        "\n",
        "        # Reshape to process all frames together: (batch * T, C, H, W)\n",
        "        x = x.view(batch_size * T, C, H, W)\n",
        "\n",
        "        # Extract features: (batch * T, 512, 1, 1)\n",
        "        features = self.backbone(x)\n",
        "\n",
        "        # Flatten: (batch * T, 512)\n",
        "        features = features.view(batch_size * T, -1)\n",
        "\n",
        "        # Project features if needed\n",
        "        if self.projection is not None:\n",
        "            features = self.projection(features)\n",
        "\n",
        "        # Reshape back: (batch, T, feature_dim)\n",
        "        features = features.view(batch_size, T, self.feature_dim)\n",
        "\n",
        "        return features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIkvk_iX-lmj"
      },
      "source": [
        "## 3. Positional Encoding (PE)\n",
        "\n",
        "Sinusoidal positional encoding adds temporal position information to the frame features before feeding them to the LSTM.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "cfVsdetA-lmj"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Sinusoidal positional encoding for temporal sequences.\n",
        "\n",
        "    Adds position information to help the model understand the order of frames.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, max_len=500, dropout=0.1):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Create positional encoding matrix\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Add batch dimension: (1, max_len, d_model)\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        # Register as buffer (not a parameter, but should be saved/loaded)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch, T, d_model)\n",
        "        Returns:\n",
        "            Tensor with positional encoding added: (batch, T, d_model)\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVq1WNB1-lmj"
      },
      "source": [
        "## 4. Attention Layer (LSTM with Attention)\n",
        "\n",
        "Bidirectional LSTM processes the sequence of frame features, followed by an attention mechanism to weight the importance of different time steps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "HCrnK27J-lmj"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    \"\"\"Attention mechanism for weighting LSTM outputs.\n",
        "\n",
        "    Computes attention weights over the sequence and returns a weighted sum.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim // 2, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, lstm_output):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            lstm_output: LSTM outputs of shape (batch, T, hidden_dim)\n",
        "        Returns:\n",
        "            context: Weighted sum of shape (batch, hidden_dim)\n",
        "            attention_weights: Attention weights of shape (batch, T)\n",
        "        \"\"\"\n",
        "        # Compute attention scores: (batch, T, 1)\n",
        "        scores = self.attention(lstm_output)\n",
        "\n",
        "        # Apply softmax over time dimension: (batch, T, 1)\n",
        "        attention_weights = F.softmax(scores, dim=1)\n",
        "\n",
        "        # Compute weighted sum: (batch, hidden_dim)\n",
        "        context = torch.sum(attention_weights * lstm_output, dim=1)\n",
        "\n",
        "        return context, attention_weights.squeeze(-1)\n",
        "\n",
        "\n",
        "class AttentionLSTM(nn.Module):\n",
        "    \"\"\"Bidirectional LSTM with attention mechanism.\n",
        "\n",
        "    Processes temporal sequence of frame features and outputs a fixed-size representation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim=256, num_layers=2, dropout=0.3, bidirectional=True):\n",
        "        super(AttentionLSTM, self).__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.bidirectional = bidirectional\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            bidirectional=bidirectional\n",
        "        )\n",
        "\n",
        "        # Attention mechanism\n",
        "        self.attention = Attention(hidden_dim * self.num_directions)\n",
        "\n",
        "        # Output dimension\n",
        "        self.output_dim = hidden_dim * self.num_directions\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch, T, input_dim)\n",
        "        Returns:\n",
        "            output: Context vector of shape (batch, hidden_dim * num_directions)\n",
        "            attention_weights: Attention weights of shape (batch, T)\n",
        "        \"\"\"\n",
        "        # LSTM forward pass: (batch, T, hidden_dim * num_directions)\n",
        "        lstm_output, (hidden, cell) = self.lstm(x)\n",
        "\n",
        "        # Apply attention\n",
        "        context, attention_weights = self.attention(lstm_output)\n",
        "\n",
        "        return context, attention_weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXNLX5GV-lmj"
      },
      "source": [
        "## 5. Complete Sign2Text Model\n",
        "\n",
        "Combines all components: Vision Layer → Positional Encoding → Attention LSTM → FC Layer → Classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "pCKBEgo2-lmk"
      },
      "outputs": [],
      "source": [
        "class Sign2TextModel(nn.Module):\n",
        "    \"\"\"Complete Sign Language to Text model.\n",
        "\n",
        "    Architecture:\n",
        "    1. Vision Layer (CNN): Extract spatial features from each frame\n",
        "    2. Positional Encoding: Add temporal position information\n",
        "    3. Attention LSTM: Process temporal sequence with attention\n",
        "    4. FC Layer: Final classification\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, feature_dim=512, hidden_dim=256,\n",
        "                 num_lstm_layers=2, dropout=0.3, pretrained_cnn=True,\n",
        "                 freeze_cnn=False, max_frames=100):\n",
        "        super(Sign2TextModel, self).__init__()\n",
        "\n",
        "        # Vision Layer (CNN)\n",
        "        self.vision_layer = VisionLayer(\n",
        "            feature_dim=feature_dim,\n",
        "            pretrained=pretrained_cnn,\n",
        "            freeze_backbone=freeze_cnn\n",
        "        )\n",
        "\n",
        "        # Positional Encoding\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            d_model=feature_dim,\n",
        "            max_len=max_frames,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        # Attention Layer (LSTM)\n",
        "        self.attention_lstm = AttentionLSTM(\n",
        "            input_dim=feature_dim,\n",
        "            hidden_dim=hidden_dim,\n",
        "            num_layers=num_lstm_layers,\n",
        "            dropout=dropout,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        # FC Layer (Classification)\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            nn.Linear(self.attention_lstm.output_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def forward(self, x, return_attention=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input video frames of shape (batch, T, C, H, W)\n",
        "            return_attention: If True, also return attention weights\n",
        "        Returns:\n",
        "            logits: Classification logits of shape (batch, num_classes)\n",
        "            attention_weights (optional): Attention weights of shape (batch, T)\n",
        "        \"\"\"\n",
        "        # Vision Layer: (batch, T, C, H, W) → (batch, T, feature_dim)\n",
        "        features = self.vision_layer(x)\n",
        "\n",
        "        # Positional Encoding: (batch, T, feature_dim) → (batch, T, feature_dim)\n",
        "        features = self.positional_encoding(features)\n",
        "\n",
        "        # Attention LSTM: (batch, T, feature_dim) → (batch, hidden_dim * 2)\n",
        "        context, attention_weights = self.attention_lstm(features)\n",
        "\n",
        "        # FC Layer: (batch, hidden_dim * 2) → (batch, num_classes)\n",
        "        logits = self.fc_layer(context)\n",
        "\n",
        "        if return_attention:\n",
        "            return logits, attention_weights\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srRI42gq-lmk"
      },
      "source": [
        "## 6. Training Utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "6UWY8AUW-lmk"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    \"\"\"Train the model for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
        "    for frames, labels in progress_bar:\n",
        "        frames = frames.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(frames)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Statistics\n",
        "        running_loss += loss.item() * frames.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        progress_bar.set_postfix({\n",
        "            'loss': loss.item(),\n",
        "            'acc': 100 * correct / total\n",
        "        })\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = 100 * correct / total\n",
        "\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    \"\"\"Evaluate the model on a dataset.\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for frames, labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            frames = frames.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(frames)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * frames.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = 100 * correct / total\n",
        "\n",
        "    return epoch_loss, epoch_acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFBEBTg4-lml"
      },
      "source": [
        "## 7. Configuration and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPsZzTQd-lml"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# CONFIGURATION - Modify these paths and hyperparameters\n",
        "# ============================================\n",
        "\n",
        "# Data paths (not needed when download=True, but kept for manual override)\n",
        "JSON_PATH = None  # Will be auto-set by kagglehub download\n",
        "VIDEO_DIR = None  # Will be auto-set by kagglehub download\n",
        "\n",
        "# Model hyperparameters\n",
        "NUM_FRAMES = 16          # Number of frames to sample from each video\n",
        "FEATURE_DIM = 512        # CNN feature dimension\n",
        "HIDDEN_DIM = 256         # LSTM hidden dimension\n",
        "NUM_LSTM_LAYERS = 2      # Number of LSTM layers\n",
        "DROPOUT = 0.3            # Dropout rate\n",
        "\n",
        "# Training hyperparameters\n",
        "BATCH_SIZE = 32           # Batch size (adjust based on GPU memory)\n",
        "LEARNING_RATE = 1e-4     # Learning rate\n",
        "WEIGHT_DECAY = 1e-4      # L2 regularization\n",
        "\n",
        "# ============================================\n",
        "# QUICK TEST MODE - Set to True for 1-2 hour test run\n",
        "# Set to False for full training (~14 days)\n",
        "# ============================================\n",
        "QUICK_TEST_MODE = True\n",
        "\n",
        "if QUICK_TEST_MODE:\n",
        "    NUM_EPOCHS = 3           # Quick test: 3 epochs\n",
        "    MAX_TRAIN_SAMPLES = 500  # Limit training samples for faster iteration\n",
        "    MAX_VAL_SAMPLES = 100    # Limit validation samples\n",
        "else:\n",
        "    NUM_EPOCHS = 30          # Full training: 30 epochs\n",
        "    MAX_TRAIN_SAMPLES = None # Use all samples\n",
        "    MAX_VAL_SAMPLES = None   # Use all samples\n",
        "\n",
        "# Options\n",
        "FREEZE_CNN = False       # Whether to freeze CNN backbone\n",
        "PRETRAINED_CNN = True    # Use pretrained CNN weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "l39WEIDb-lml"
      },
      "outputs": [],
      "source": [
        "# Data transforms for training and validation\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnohaoY9-lml",
        "outputId": "c6a443b4-8fb5-4ee0-9f08-a60dbd755319"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloaded at path:  C:\\Users\\Zach\\.cache\\kagglehub\\datasets\\sttaseen\\wlasl2000-resized\\versions\\1\n",
            "Downloaded at path:  C:\\Users\\Zach\\.cache\\kagglehub\\datasets\\sttaseen\\wlasl2000-resized\\versions\\1\n",
            "Downloaded at path:  C:\\Users\\Zach\\.cache\\kagglehub\\datasets\\sttaseen\\wlasl2000-resized\\versions\\1\n",
            "Number of training samples: 500\n",
            "Number of validation samples: 100\n",
            "Number of test samples: 100\n",
            "Number of classes: 2000\n",
            "\n",
            "⚡ QUICK TEST MODE ENABLED - Using limited samples for faster testing\n"
          ]
        }
      ],
      "source": [
        "# Create datasets (uses kagglehub download by default)\n",
        "train_dataset = WLASLTorchCodec(\n",
        "    download=True,\n",
        "    split=\"train\",\n",
        "    num_frames=NUM_FRAMES,\n",
        "    transform=train_transform,\n",
        "    max_samples=MAX_TRAIN_SAMPLES  # Limit samples in quick test mode\n",
        ")\n",
        "\n",
        "val_dataset = WLASLTorchCodec(\n",
        "    download=True,\n",
        "    split=\"val\",\n",
        "    num_frames=NUM_FRAMES,\n",
        "    transform=val_transform,\n",
        "    max_samples=MAX_VAL_SAMPLES  # Limit samples in quick test mode\n",
        ")\n",
        "\n",
        "test_dataset = WLASLTorchCodec(\n",
        "    download=True,\n",
        "    split=\"test\",\n",
        "    num_frames=NUM_FRAMES,\n",
        "    transform=val_transform,\n",
        "    max_samples=MAX_VAL_SAMPLES  # Limit samples in quick test mode\n",
        ")\n",
        "\n",
        "# Get number of classes from dataset\n",
        "NUM_CLASSES = train_dataset.num_classes\n",
        "\n",
        "print(f\"Number of training samples: {len(train_dataset)}\")\n",
        "print(f\"Number of validation samples: {len(val_dataset)}\")\n",
        "print(f\"Number of test samples: {len(test_dataset)}\")\n",
        "print(f\"Number of classes: {NUM_CLASSES}\")\n",
        "if QUICK_TEST_MODE:\n",
        "    print(f\"\\n⚡ QUICK TEST MODE ENABLED - Using limited samples for faster testing\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "gpPAkm72-lml"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataLoader config: num_workers=0, pin_memory=False\n"
          ]
        }
      ],
      "source": [
        "# Create data loaders\n",
        "# Note: num_workers=0 for Windows compatibility, pin_memory only when CUDA available\n",
        "NUM_WORKERS = 0  # Set to 0 for Windows (multiprocessing issues), increase on Linux\n",
        "PIN_MEMORY = torch.cuda.is_available()\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=PIN_MEMORY\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=PIN_MEMORY\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=PIN_MEMORY\n",
        ")\n",
        "\n",
        "print(f\"DataLoader config: num_workers={NUM_WORKERS}, pin_memory={PIN_MEMORY}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abGiXTmk-lml",
        "outputId": "1297ad28-ca61-45ca-98ce-6ac98f80220a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sign2TextModel(\n",
            "  (vision_layer): VisionLayer(\n",
            "    (backbone): Sequential(\n",
            "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "      (4): Sequential(\n",
            "        (0): BasicBlock(\n",
            "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (1): BasicBlock(\n",
            "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (5): Sequential(\n",
            "        (0): BasicBlock(\n",
            "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): BasicBlock(\n",
            "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (6): Sequential(\n",
            "        (0): BasicBlock(\n",
            "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): BasicBlock(\n",
            "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (7): Sequential(\n",
            "        (0): BasicBlock(\n",
            "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): BasicBlock(\n",
            "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (positional_encoding): PositionalEncoding(\n",
            "    (dropout): Dropout(p=0.3, inplace=False)\n",
            "  )\n",
            "  (attention_lstm): AttentionLSTM(\n",
            "    (lstm): LSTM(512, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
            "    (attention): Attention(\n",
            "      (attention): Sequential(\n",
            "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (1): Tanh()\n",
            "        (2): Linear(in_features=256, out_features=1, bias=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (fc_layer): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.3, inplace=False)\n",
            "    (3): Linear(in_features=256, out_features=2000, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "Total parameters: 15,107,345\n",
            "Trainable parameters: 15,107,345\n"
          ]
        }
      ],
      "source": [
        "# Initialize model\n",
        "model = Sign2TextModel(\n",
        "    num_classes=NUM_CLASSES,\n",
        "    feature_dim=FEATURE_DIM,\n",
        "    hidden_dim=HIDDEN_DIM,\n",
        "    num_lstm_layers=NUM_LSTM_LAYERS,\n",
        "    dropout=DROPOUT,\n",
        "    pretrained_cnn=PRETRAINED_CNN,\n",
        "    freeze_cnn=FREEZE_CNN,\n",
        "    max_frames=NUM_FRAMES\n",
        ").to(device)\n",
        "\n",
        "# Print model summary\n",
        "print(model)\n",
        "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXgIymLR-lmm"
      },
      "source": [
        "# Loss function and optimizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "TIND-0ZnWb5m"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW( model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xrr02DMWXiQ"
      },
      "source": [
        "# Learning rate scheduler\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "1zMGIp0-G9vL"
      },
      "outputs": [],
      "source": [
        "#no verbose\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau( optimizer, mode='min', factor=0.5, patience=3 )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbpkZSb_-lmm"
      },
      "source": [
        "## 8. Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "0jbK9sCf-lmm",
        "outputId": "96f5d1cd-1578-4578-d1bd-b24b9e85e519"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/3\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  12%|█▎        | 2/16 [02:28<17:41, 75.83s/it, loss=7.61, acc=0]"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "best_val_acc = 0.0\n",
        "history = {\n",
        "    'train_loss': [], 'train_acc': [],\n",
        "    'val_loss': [], 'val_acc': []\n",
        "}\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Train\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "\n",
        "    # Validate\n",
        "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "    # Update scheduler\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Save history\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_acc': val_acc,\n",
        "            'label_map': train_dataset.label_map\n",
        "        }, 'best_model.pth')\n",
        "        print(f\"✓ Saved new best model with Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "print(f\"\\nTraining complete! Best Val Acc: {best_val_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0_AbauW-lmm"
      },
      "source": [
        "## 9. Evaluation and Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxMS0YFc-lmm"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training history\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Loss plot\n",
        "axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
        "axes[0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Training and Validation Loss')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True)\n",
        "\n",
        "# Accuracy plot\n",
        "axes[1].plot(history['train_acc'], label='Train Acc', marker='o')\n",
        "axes[1].plot(history['val_acc'], label='Val Acc', marker='s')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Accuracy (%)')\n",
        "axes[1].set_title('Training and Validation Accuracy')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_history.png', dpi=150)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5fEQd22-lmm"
      },
      "outputs": [],
      "source": [
        "# Load best model and evaluate on test set\n",
        "checkpoint = torch.load('best_model.pth')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
        "print(f\"\\nTest Results:\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK5qG01d-lmn"
      },
      "source": [
        "## 10. Attention Visualization\n",
        "\n",
        "Visualize which frames the model attends to most when making predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FY2JOSmH-lmn"
      },
      "outputs": [],
      "source": [
        "def visualize_attention(model, frames, true_label, label_map, device):\n",
        "    \"\"\"Visualize attention weights over video frames.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Get reverse label map\n",
        "    idx_to_label = {v: k for k, v in label_map.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Add batch dimension\n",
        "        frames_batch = frames.unsqueeze(0).to(device)\n",
        "\n",
        "        # Get predictions and attention weights\n",
        "        logits, attention_weights = model(frames_batch, return_attention=True)\n",
        "        pred_label = torch.argmax(logits, dim=1).item()\n",
        "        attention = attention_weights[0].cpu().numpy()\n",
        "\n",
        "    # Create visualization\n",
        "    num_frames = frames.shape[0]\n",
        "    fig, axes = plt.subplots(2, num_frames, figsize=(2 * num_frames, 6))\n",
        "\n",
        "    # Denormalize frames for visualization\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "\n",
        "    for i in range(num_frames):\n",
        "        frame = frames[i].cpu()\n",
        "        frame = frame * std + mean\n",
        "        frame = frame.clamp(0, 1).permute(1, 2, 0).numpy()\n",
        "\n",
        "        # Frame image\n",
        "        axes[0, i].imshow(frame)\n",
        "        axes[0, i].set_title(f\"Frame {i+1}\")\n",
        "        axes[0, i].axis('off')\n",
        "\n",
        "        # Attention weight bar\n",
        "        axes[1, i].bar([0], [attention[i]], color='blue', alpha=0.7)\n",
        "        axes[1, i].set_ylim(0, max(attention) * 1.2)\n",
        "        axes[1, i].set_title(f\"{attention[i]:.3f}\")\n",
        "        axes[1, i].axis('off')\n",
        "\n",
        "    plt.suptitle(\n",
        "        f\"True: {idx_to_label.get(true_label, true_label)} | \"\n",
        "        f\"Predicted: {idx_to_label.get(pred_label, pred_label)}\",\n",
        "        fontsize=14\n",
        "    )\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return pred_label, attention\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pOEoPik-lmn"
      },
      "outputs": [],
      "source": [
        "# Visualize attention for a sample from the test set\n",
        "sample_idx = 0\n",
        "frames, label = test_dataset[sample_idx]\n",
        "pred, attn = visualize_attention(model, frames, label, train_dataset.label_map, device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1vGI8IA-lmn"
      },
      "source": [
        "## 11. Inference Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CXDTtOc-lmn"
      },
      "outputs": [],
      "source": [
        "def predict_video(model, video_path, transform, num_frames, label_map, device):\n",
        "    \"\"\"Predict the sign language class for a video file.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Get reverse label map\n",
        "    idx_to_label = {v: k for k, v in label_map.items()}\n",
        "\n",
        "    # Decode video\n",
        "    decoder = VideoDecoder(video_path)\n",
        "    frames = []\n",
        "\n",
        "    for chunk in decoder:\n",
        "        for frame_tensor in chunk:\n",
        "            if frame_tensor.dim() == 2:\n",
        "                frame_tensor = frame_tensor.unsqueeze(2)\n",
        "            frame_chw = frame_tensor.permute(2, 0, 1)\n",
        "            frame_pil = transforms.ToPILImage()(frame_chw)\n",
        "            frames.append(frame_pil)\n",
        "\n",
        "    # Handle short videos\n",
        "    while len(frames) < num_frames:\n",
        "        frames.extend(frames)\n",
        "\n",
        "    # Sample frames\n",
        "    T = len(frames)\n",
        "    idx = torch.linspace(0, T - 1, num_frames).long()\n",
        "    frames = [frames[i] for i in idx]\n",
        "\n",
        "    # Apply transforms\n",
        "    frames = torch.stack([transform(f) for f in frames])\n",
        "\n",
        "    # Predict\n",
        "    with torch.no_grad():\n",
        "        frames_batch = frames.unsqueeze(0).to(device)\n",
        "        logits, attention = model(frames_batch, return_attention=True)\n",
        "        probabilities = F.softmax(logits, dim=1)\n",
        "        pred_idx = torch.argmax(logits, dim=1).item()\n",
        "        confidence = probabilities[0, pred_idx].item()\n",
        "\n",
        "    predicted_label = idx_to_label.get(pred_idx, f\"Unknown ({pred_idx})\")\n",
        "\n",
        "    return {\n",
        "        'prediction': predicted_label,\n",
        "        'confidence': confidence,\n",
        "        'attention_weights': attention[0].cpu().numpy(),\n",
        "        'all_probabilities': probabilities[0].cpu().numpy()\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SNQvsPo-lmn"
      },
      "outputs": [],
      "source": [
        "# Example inference (uncomment and modify path to use)\n",
        "# result = predict_video(\n",
        "#     model=model,\n",
        "#     video_path=\"/path/to/your/video.mp4\",\n",
        "#     transform=val_transform,\n",
        "#     num_frames=NUM_FRAMES,\n",
        "#     label_map=train_dataset.label_map,\n",
        "#     device=device\n",
        "# )\n",
        "# print(f\"Prediction: {result['prediction']}\")\n",
        "# print(f\"Confidence: {result['confidence']:.2%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Srt60SNL-lmn"
      },
      "source": [
        "## 12. Save Final Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpDygB68-lmo"
      },
      "outputs": [],
      "source": [
        "# Save complete model for deployment\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'label_map': train_dataset.label_map,\n",
        "    'config': {\n",
        "        'num_classes': NUM_CLASSES,\n",
        "        'feature_dim': FEATURE_DIM,\n",
        "        'hidden_dim': HIDDEN_DIM,\n",
        "        'num_lstm_layers': NUM_LSTM_LAYERS,\n",
        "        'num_frames': NUM_FRAMES,\n",
        "        'dropout': DROPOUT\n",
        "    }\n",
        "}, 'sign2text_model_final.pth')\n",
        "\n",
        "print(\"Model saved to sign2text_model_final.pth\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7qt1Yws-lmo"
      },
      "source": [
        "---\n",
        "**Note:** The cells above contain the complete implementation. Make sure to run them in order from top to bottom.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6VTtt4e-lmo"
      },
      "outputs": [],
      "source": [
        "# PositionalEncoding class is defined below cell 7 - this cell can be ignored\n",
        "# The model requires running cells in sequential order\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
