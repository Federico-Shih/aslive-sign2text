{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Luwl8hSq-lmd"
      },
      "source": [
        "# ASLive Sign2Text Model\n",
        "\n",
        "This notebook implements the sign language to text model following the architecture:\n",
        "- **Vision Layer (CNN)**: Extracts spatial features from each frame\n",
        "- **Positional Encoding (PE)**: Adds temporal position information\n",
        "- **Attention Layer (LSTM)**: Processes temporal sequence with attention\n",
        "- **FC Layer**: Final classification layer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kagglehub torchcodec torchvision\n",
        "!pip install git+https://github.com/facebookresearch/pytorchvideo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbsKbY7bA16g",
        "outputId": "228dcc7d-bb49-4ee8-c9a6-4e500e9890d5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.12/dist-packages (0.3.13)\n",
            "Collecting torchcodec\n",
            "  Downloading torchcodec-0.8.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kagglehub) (25.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from kagglehub) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kagglehub) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: torch==2.9.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.9.0+cu126)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.5.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.9.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.9.0->torchvision) (3.0.3)\n",
            "Downloading torchcodec-0.8.1-cp312-cp312-manylinux_2_28_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchcodec\n",
            "Successfully installed torchcodec-0.8.1\n",
            "Collecting git+https://github.com/facebookresearch/pytorchvideo\n",
            "  Cloning https://github.com/facebookresearch/pytorchvideo to /tmp/pip-req-build-5q2n8di7\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/pytorchvideo /tmp/pip-req-build-5q2n8di7\n",
            "  Resolved https://github.com/facebookresearch/pytorchvideo to commit 0f9a5e102e4d84972b829fd30e3c3f78c7c7fd1a\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fvcore (from pytorchvideo==0.1.5)\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting av (from pytorchvideo==0.1.5)\n",
            "  Downloading av-16.0.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
            "Collecting parameterized (from pytorchvideo==0.1.5)\n",
            "  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Collecting iopath (from pytorchvideo==0.1.5)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from pytorchvideo==0.1.5) (3.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from fvcore->pytorchvideo==0.1.5) (2.0.2)\n",
            "Collecting yacs>=0.1.6 (from fvcore->pytorchvideo==0.1.5)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from fvcore->pytorchvideo==0.1.5) (6.0.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from fvcore->pytorchvideo==0.1.5) (4.67.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.12/dist-packages (from fvcore->pytorchvideo==0.1.5) (3.2.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from fvcore->pytorchvideo==0.1.5) (11.3.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from fvcore->pytorchvideo==0.1.5) (0.9.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from iopath->pytorchvideo==0.1.5) (4.15.0)\n",
            "Collecting portalocker (from iopath->pytorchvideo==0.1.5)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Downloading av-16.0.1-cp312-cp312-manylinux_2_28_x86_64.whl (40.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Building wheels for collected packages: pytorchvideo, fvcore, iopath\n",
            "  Building wheel for pytorchvideo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorchvideo: filename=pytorchvideo-0.1.5-py3-none-any.whl size=213017 sha256=82c0fa8d9f7138d128d208881ad01991879af257525c35046ccf5df20b6f87ac\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-yn9t_do1/wheels/98/ed/57/db54871bbb5a7356f816cf5ec47ab2d3ca2a86fea760a0cbd8\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61397 sha256=9b05cec847e3c2727a6a4e819a09576c00110fc42e26f329b03f8e2376bb9035\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/9f/a5/e4f5b27454ccd4596bd8b62432c7d6b1ca9fa22aef9d70a16a\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31527 sha256=b98a3f5c250b1d0678dcf3bb993891fd51b3b37a580ebd983548940eb0e14cca\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/96/04/4f5f31ff812f684f69f40cb1634357812220aac58d4698048c\n",
            "Successfully built pytorchvideo fvcore iopath\n",
            "Installing collected packages: yacs, portalocker, parameterized, av, iopath, fvcore, pytorchvideo\n",
            "Successfully installed av-16.0.1 fvcore-0.1.5.post20221221 iopath-0.1.10 parameterized-0.9.0 portalocker-3.2.0 pytorchvideo-0.1.5 yacs-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Before running, add everything from SQ_dataloader.ipynb into the cell below and run"
      ],
      "metadata": {
        "id": "8q1A9uXSU7qG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtCIKfOn-lmh"
      },
      "source": [
        "## 1. Data Loading (from SQ_dataloader)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#add SQ_dataloader code here\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from torchcodec.decoders import VideoDecoder\n",
        "import kagglehub\n",
        "import os\n",
        "import json\n",
        "\n",
        "class WLASLTorchCodec(Dataset):\n",
        "  download_path = None\n",
        "\n",
        "  def __init__(self, json_path=None, video_dir=None, download=True, max_classes=None, split=\"train\", num_frames=32, transform=None):\n",
        "    print(\"Will download:\", download)\n",
        "    if (json_path is None or video_dir is None) and download == False:\n",
        "      raise ValueError(\"json_path and video_dir must be provided with download false\")\n",
        "    if download:\n",
        "      if WLASLTorchCodec.download_path is None:\n",
        "        path = kagglehub.dataset_download(\"sttaseen/wlasl2000-resized\")\n",
        "        WLASLTorchCodec.download_path = path\n",
        "      else:\n",
        "        path = WLASLTorchCodec.download_path\n",
        "      print(\"Downloaded at path: \", path)\n",
        "\n",
        "      self.video_dir = os.path.join(path, \"wlasl-complete\", \"videos\")\n",
        "      json_path = os.path.join(path, \"wlasl-complete\",\"WLASL_v0.3.json\")\n",
        "      downloaded = True\n",
        "    else:\n",
        "      self.video_dir = video_dir\n",
        "    self.num_frames = num_frames\n",
        "    self.transform = transform\n",
        "\n",
        "    # Read json\n",
        "    with open(json_path, \"r\") as f:\n",
        "      data = json.load(f)\n",
        "    if max_classes is not None:\n",
        "        if isinstance(max_classes, int):\n",
        "            # Keep only the first N entries (Usually the most frequent in WLASL)\n",
        "            data = data[:max_classes]\n",
        "            print(f\"Limiting dataset to top {max_classes} classes.\")\n",
        "        elif isinstance(max_classes, list):\n",
        "            # Keep only entries that match specific glosses\n",
        "            data = [entry for entry in data if entry['gloss'] in max_classes]\n",
        "            print(f\"Limiting dataset to {len(data)} specific classes.\")\n",
        "    self.samples = []\n",
        "    self.label_map = {}\n",
        "    label_id = 0\n",
        "\n",
        "    for entry in data:\n",
        "      gloss = entry[\"gloss\"]\n",
        "\n",
        "      if gloss not in self.label_map:\n",
        "        self.label_map[gloss] = label_id\n",
        "        label_id += 1\n",
        "\n",
        "      label = self.label_map[gloss]\n",
        "\n",
        "      for inst in entry[\"instances\"]:\n",
        "        if inst[\"split\"] != split:\n",
        "          continue\n",
        "\n",
        "        video_id = inst[\"video_id\"]\n",
        "        file_path = os.path.join(self.video_dir, f\"{video_id}.mp4\")\n",
        "\n",
        "        if os.path.isfile(file_path):\n",
        "          self.samples.append((file_path, label))\n",
        "        self.num_classes = label_id\n",
        "  def __len__(self):\n",
        "    return len(self.samples)\n",
        "\n",
        "  def _sample_frames(self, frames):\n",
        "    \"\"\"frames is a list of PIL images or Tensors.\"\"\"\n",
        "    T = len(frames)\n",
        "    idx = torch.linspace(0, T - 1, self.num_frames).long()\n",
        "    return [frames[i] for i in idx]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    video_path, label = self.samples[idx]\n",
        "    decoder = VideoDecoder(video_path)\n",
        "\n",
        "    frames = []\n",
        "\n",
        "    for chunk in decoder:\n",
        "      for frame_tensor in chunk:\n",
        "        # print(f\"Raw frame shape: {frame_tensor.shape}, dtype: {frame_tensor.dtype}\")\n",
        "        # print(frame_tensor.shape)\n",
        "        if frame_tensor.dim() == 2:\n",
        "          frame_tensor = frame_tensor.unsqueeze(2)  # → (H, W, 1)\n",
        "\n",
        "        if frame_tensor.shape[2] == 1:\n",
        "          # Use tensor repeat for grayscale to RGB conversion\n",
        "          frame_tensor = frame_tensor.repeat(1, 1, 3)\n",
        "\n",
        "        # At this point, frame_tensor is (H, W, 3)\n",
        "        if frame_tensor.dim() == 3 and frame_tensor.shape[2] == 3:\n",
        "          # Convert to C x H x W immediately for torchvision compatibility\n",
        "          frame_chw = frame_tensor.permute(2, 0, 1)\n",
        "          frames.append(frame_chw)\n",
        "        else:\n",
        "          raise ValueError(f\"Unexpected frame shape after processing: {frame_tensor.shape}\")\n",
        "\n",
        "    # # Guard for short videos\n",
        "    # if len(frames) < self.num_frames:\n",
        "    #     while len(frames) < self.num_frames:\n",
        "    #         frames.extend(frames)\n",
        "    #     frames = frames[: self.num_frames]\n",
        "\n",
        "    # # Uniform sampling\n",
        "    # frames = self._sample_frames(frames)\n",
        "\n",
        "    # frames is now a list of C x H x W Tensors\n",
        "    frames_tensor = torch.stack(frames) # Shape: (T, C, H, W)\n",
        "\n",
        "    # Apply transform (if provided) directly to the stacked tensor\n",
        "    if self.transform:\n",
        "        frames_tensor = self.transform(frames_tensor)\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "    return frames_tensor, label\n",
        "\n",
        "\n",
        "\n",
        "# once this cell successfully runs, continue with the other cells"
      ],
      "metadata": {
        "id": "vqaVkQmlVJE0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsdOaDCZ-lmh",
        "outputId": "e8090d0e-5f29-458d-f6d3-48e5b018b0eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from torchcodec.decoders import VideoDecoder\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tfGCzOb-lmi"
      },
      "source": [
        "## 2. Vision Layer (CNN Backbone)\n",
        "\n",
        "The Vision Layer extracts spatial features from each video frame using a CNN. We use a pretrained ResNet-18 as the backbone and remove the final classification layer to get feature embeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jkESoYf_-lmi"
      },
      "outputs": [],
      "source": [
        "class VisionLayer(nn.Module):\n",
        "    \"\"\"CNN backbone for extracting spatial features from video frames.\n",
        "\n",
        "    Uses pretrained ResNet-18 as feature extractor.\n",
        "    Input: (batch, T, C, H, W) - batch of T frames\n",
        "    Output: (batch, T, feature_dim) - feature vectors for each frame\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, feature_dim=512, pretrained=True, freeze_backbone=False):\n",
        "        super(VisionLayer, self).__init__()\n",
        "\n",
        "        # Load pretrained ResNet-18\n",
        "        resnet = models.resnet18(weights='IMAGENET1K_V1' if pretrained else None)\n",
        "\n",
        "        # Remove the final FC layer\n",
        "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
        "\n",
        "        # ResNet-18 outputs 512-dim features\n",
        "        self.resnet_feature_dim = 512\n",
        "\n",
        "        # Optional projection layer to adjust feature dimension\n",
        "        if feature_dim != self.resnet_feature_dim:\n",
        "            self.projection = nn.Linear(self.resnet_feature_dim, feature_dim)\n",
        "        else:\n",
        "            self.projection = None\n",
        "\n",
        "        self.feature_dim = feature_dim\n",
        "\n",
        "        # Freeze backbone if specified\n",
        "        if freeze_backbone:\n",
        "            for param in self.backbone.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch, T, C, H, W)\n",
        "        Returns:\n",
        "            Feature tensor of shape (batch, T, feature_dim)\n",
        "        \"\"\"\n",
        "        batch_size, T, C, H, W = x.shape\n",
        "\n",
        "        # Reshape to process all frames together: (batch * T, C, H, W)\n",
        "        x = x.view(batch_size * T, C, H, W)\n",
        "\n",
        "        # Extract features: (batch * T, 512, 1, 1)\n",
        "        features = self.backbone(x)\n",
        "\n",
        "        # Flatten: (batch * T, 512)\n",
        "        features = features.view(batch_size * T, -1)\n",
        "\n",
        "        # Project features if needed\n",
        "        if self.projection is not None:\n",
        "            features = self.projection(features)\n",
        "\n",
        "        # Reshape back: (batch, T, feature_dim)\n",
        "        features = features.view(batch_size, T, self.feature_dim)\n",
        "\n",
        "        return features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIkvk_iX-lmj"
      },
      "source": [
        "## 3. Positional Encoding (PE)\n",
        "\n",
        "Sinusoidal positional encoding adds temporal position information to the frame features before feeding them to the LSTM.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cfVsdetA-lmj"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Sinusoidal positional encoding for temporal sequences.\n",
        "\n",
        "    Adds position information to help the model understand the order of frames.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, max_len=500, dropout=0.1):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Create positional encoding matrix\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Add batch dimension: (1, max_len, d_model)\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        # Register as buffer (not a parameter, but should be saved/loaded)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch, T, d_model)\n",
        "        Returns:\n",
        "            Tensor with positional encoding added: (batch, T, d_model)\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVq1WNB1-lmj"
      },
      "source": [
        "## 4. Attention Layer (LSTM with Attention)\n",
        "\n",
        "Bidirectional LSTM processes the sequence of frame features, followed by an attention mechanism to weight the importance of different time steps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HCrnK27J-lmj"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    \"\"\"Attention mechanism for weighting LSTM outputs.\n",
        "\n",
        "    Computes attention weights over the sequence and returns a weighted sum.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim // 2, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, lstm_output):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            lstm_output: LSTM outputs of shape (batch, T, hidden_dim)\n",
        "        Returns:\n",
        "            context: Weighted sum of shape (batch, hidden_dim)\n",
        "            attention_weights: Attention weights of shape (batch, T)\n",
        "        \"\"\"\n",
        "        # Compute attention scores: (batch, T, 1)\n",
        "        scores = self.attention(lstm_output)\n",
        "\n",
        "        # Apply softmax over time dimension: (batch, T, 1)\n",
        "        attention_weights = F.softmax(scores, dim=1)\n",
        "\n",
        "        # Compute weighted sum: (batch, hidden_dim)\n",
        "        context = torch.sum(attention_weights * lstm_output, dim=1)\n",
        "\n",
        "        return context, attention_weights.squeeze(-1)\n",
        "\n",
        "\n",
        "class AttentionLSTM(nn.Module):\n",
        "    \"\"\"Bidirectional LSTM with attention mechanism.\n",
        "\n",
        "    Processes temporal sequence of frame features and outputs a fixed-size representation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim=256, num_layers=2, dropout=0.3, bidirectional=True):\n",
        "        super(AttentionLSTM, self).__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.bidirectional = bidirectional\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            bidirectional=bidirectional\n",
        "        )\n",
        "\n",
        "        # Attention mechanism\n",
        "        self.attention = Attention(hidden_dim * self.num_directions)\n",
        "\n",
        "        # Output dimension\n",
        "        self.output_dim = hidden_dim * self.num_directions\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch, T, input_dim)\n",
        "        Returns:\n",
        "            output: Context vector of shape (batch, hidden_dim * num_directions)\n",
        "            attention_weights: Attention weights of shape (batch, T)\n",
        "        \"\"\"\n",
        "        # LSTM forward pass: (batch, T, hidden_dim * num_directions)\n",
        "        lstm_output, (hidden, cell) = self.lstm(x)\n",
        "\n",
        "        # Apply attention\n",
        "        context, attention_weights = self.attention(lstm_output)\n",
        "\n",
        "        return context, attention_weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXNLX5GV-lmj"
      },
      "source": [
        "## 5. Complete Sign2Text Model\n",
        "\n",
        "Combines all components: Vision Layer → Positional Encoding → Attention LSTM → FC Layer → Classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pCKBEgo2-lmk"
      },
      "outputs": [],
      "source": [
        "class Sign2TextModel(nn.Module):\n",
        "    \"\"\"Complete Sign Language to Text model.\n",
        "\n",
        "    Architecture:\n",
        "    1. Vision Layer (CNN): Extract spatial features from each frame\n",
        "    2. Positional Encoding: Add temporal position information\n",
        "    3. Attention LSTM: Process temporal sequence with attention\n",
        "    4. FC Layer: Final classification\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, feature_dim=512, hidden_dim=256,\n",
        "                 num_lstm_layers=2, dropout=0.3, pretrained_cnn=True,\n",
        "                 freeze_cnn=False, max_frames=100):\n",
        "        super(Sign2TextModel, self).__init__()\n",
        "\n",
        "        # Vision Layer (CNN)\n",
        "        self.vision_layer = VisionLayer(\n",
        "            feature_dim=feature_dim,\n",
        "            pretrained=pretrained_cnn,\n",
        "            freeze_backbone=freeze_cnn\n",
        "        )\n",
        "\n",
        "        # Positional Encoding\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            d_model=feature_dim,\n",
        "            max_len=max_frames,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        # Attention Layer (LSTM)\n",
        "        self.attention_lstm = AttentionLSTM(\n",
        "            input_dim=feature_dim,\n",
        "            hidden_dim=hidden_dim,\n",
        "            num_layers=num_lstm_layers,\n",
        "            dropout=dropout,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        # FC Layer (Classification)\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            nn.Linear(self.attention_lstm.output_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def forward(self, x, return_attention=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input video frames of shape (batch, T, C, H, W)\n",
        "            return_attention: If True, also return attention weights\n",
        "        Returns:\n",
        "            logits: Classification logits of shape (batch, num_classes)\n",
        "            attention_weights (optional): Attention weights of shape (batch, T)\n",
        "        \"\"\"\n",
        "        # Vision Layer: (batch, T, C, H, W) → (batch, T, feature_dim)\n",
        "        features = self.vision_layer(x)\n",
        "\n",
        "        # Positional Encoding: (batch, T, feature_dim) → (batch, T, feature_dim)\n",
        "        features = self.positional_encoding(features)\n",
        "\n",
        "        # Attention LSTM: (batch, T, feature_dim) → (batch, hidden_dim * 2)\n",
        "        context, attention_weights = self.attention_lstm(features)\n",
        "\n",
        "        # FC Layer: (batch, hidden_dim * 2) → (batch, num_classes)\n",
        "        logits = self.fc_layer(context)\n",
        "\n",
        "        if return_attention:\n",
        "            return logits, attention_weights\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.1 Timesformer"
      ],
      "metadata": {
        "id": "jiW72eN0UdWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import repeat\n",
        "import collections.abc\n",
        "from enum import Enum\n",
        "from typing import *\n",
        "\n",
        "import torch.nn.functional as F\n",
        "def _ntuple(n):\n",
        "    def parse(x):\n",
        "        if isinstance(x, collections.abc.Iterable) and not isinstance(x, str):\n",
        "            return tuple(x)\n",
        "        return tuple(repeat(x, n))\n",
        "    return parse\n",
        "\n",
        "class Format(str, Enum):\n",
        "    NCHW = 'NCHW'\n",
        "    NHWC = 'NHWC'\n",
        "    NCL = 'NCL'\n",
        "    NLC = 'NLC'\n",
        "\n",
        "\n",
        "FormatT = Union[str, Format]\n",
        "\n",
        "to_1tuple = _ntuple(1)\n",
        "to_2tuple = _ntuple(2)\n",
        "to_3tuple = _ntuple(3)\n",
        "to_4tuple = _ntuple(4)\n",
        "to_ntuple = _ntuple\n",
        "\n",
        "\n",
        "# https://github.com/huggingface/pytorch-image-models/blob/main/timm/layers/patch_embed.py\n",
        "class PatchEmbedVideo(nn.Module):\n",
        "    \"\"\" Video to Patch Embedding\n",
        "        (Adapts 2D Conv PatchEmbed to handle B, F, C, H, W inputs)\n",
        "    \"\"\"\n",
        "    dynamic_img_pad: torch.jit.Final[bool]\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            img_size: Optional[Union[int, Tuple[int, int]]] = 224,\n",
        "            patch_size: int = 16,\n",
        "            in_chans: int = 3,\n",
        "            embed_dim: int = 768,\n",
        "            norm_layer: Optional[Callable] = None,\n",
        "            flatten: bool = True,\n",
        "            output_fmt: Optional[str] = None,\n",
        "            bias: bool = True,\n",
        "            strict_img_size: bool = True,\n",
        "            dynamic_img_pad: bool = False,\n",
        "            device=None,\n",
        "            dtype=None,\n",
        "    ):\n",
        "        dd = {'device': device, 'dtype': dtype}\n",
        "        super().__init__()\n",
        "        self.patch_size = to_2tuple(patch_size)\n",
        "        self.img_size, self.grid_size, self.num_patches = self._init_img_size(img_size)\n",
        "\n",
        "        if output_fmt is not None:\n",
        "            self.flatten = False\n",
        "            self.output_fmt = Format(output_fmt)\n",
        "        else:\n",
        "            self.flatten = flatten\n",
        "            self.output_fmt = Format.NCHW\n",
        "\n",
        "        self.strict_img_size = strict_img_size\n",
        "        self.dynamic_img_pad = dynamic_img_pad\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # Standard 2D Conv is sufficient for TimeSformer (frame-wise processing)\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size, bias=bias, **dd)\n",
        "        self.norm = norm_layer(embed_dim, **dd) if norm_layer else nn.Identity()\n",
        "\n",
        "    def _init_img_size(self, img_size: Union[int, Tuple[int, int]]):\n",
        "        if img_size is None:\n",
        "            return None, None, None\n",
        "        img_size = to_2tuple(img_size)\n",
        "        grid_size = tuple([s // p for s, p in zip(img_size, self.patch_size)])\n",
        "        num_patches = grid_size[0] * grid_size[1]\n",
        "        return img_size, grid_size, num_patches\n",
        "\n",
        "    def set_input_size(self, img_size=None, patch_size=None):\n",
        "        # (Same as original code, omitted for brevity)\n",
        "        pass\n",
        "\n",
        "    def feat_ratio(self, as_scalar=True):\n",
        "        return max(self.patch_size) if as_scalar else self.patch_size\n",
        "\n",
        "    def dynamic_feat_size(self, img_size: Tuple[int, int]):\n",
        "        if self.dynamic_img_pad:\n",
        "            return math.ceil(img_size[0] / self.patch_size[0]), math.ceil(img_size[1] / self.patch_size[1])\n",
        "        else:\n",
        "            return img_size[0] // self.patch_size[0], img_size[1] // self.patch_size[1]\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input shape: (Batch, Frames, Channels, Height, Width)\n",
        "        B, F, C, H, W = x.shape\n",
        "\n",
        "        # 1. Fold Frames into Batch dimension to use Conv2d\n",
        "        # (B, F, C, H, W) -> (B * F, C, H, W)\n",
        "        x = x.reshape(B * F, C, H, W)\n",
        "\n",
        "        # 2. Handle Dynamic Padding (if enabled)\n",
        "        if self.dynamic_img_pad:\n",
        "            pad_h = (self.patch_size[0] - H % self.patch_size[0]) % self.patch_size[0]\n",
        "            pad_w = (self.patch_size[1] - W % self.patch_size[1]) % self.patch_size[1]\n",
        "            x = F.pad(x, (0, pad_w, 0, pad_h))\n",
        "\n",
        "        # 3. Apply Projection\n",
        "        # Output: (B*F, Embed_Dim, Grid_H, Grid_W)\n",
        "        x = self.proj(x)\n",
        "\n",
        "        if self.flatten:\n",
        "            # Flatten spatial dims: (B*F, Embed_Dim, Grid_H * Grid_W)\n",
        "            # Transpose: (B*F, Num_Spatial_Patches, Embed_Dim)\n",
        "            x = x.flatten(2).transpose(1, 2)\n",
        "\n",
        "            # 4. Unfold Frames back out\n",
        "            # We want a continuous sequence of tokens: (B, F * Num_Spatial_Patches, Embed_Dim)\n",
        "            # This matches the TimeSformer expectation.\n",
        "            BF, N_spatial, D = x.shape\n",
        "            x = x.reshape(B, F * N_spatial, D)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "class TimeSpatialTokens(nn.Module):\n",
        "  def __init__(self, num_frames, num_patches, embed_dim):\n",
        "    super(TimeSpatialTokens, self).__init__()\n",
        "\n",
        "    self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "    self.spatial_pos_embedding = nn.Parameter(torch.zeros(1,  num_patches, embed_dim))\n",
        "    self.temporal_pos_embedding = nn.Parameter(torch.zeros(1, num_frames, embed_dim))\n",
        "\n",
        "  # Receives patch embeddings of size B, T, D\n",
        "  def forward(self, x):\n",
        "    B, T, D = x.shape\n",
        "    F = self.temporal_pos_embedding.shape[1] # num frames\n",
        "    N = self.spatial_pos_embedding.shape[1] # num patches per frame\n",
        "    cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "    x = torch.cat((cls_tokens, x), dim=1)\n",
        "    spatial_pe = self.spatial_pos_embedding.repeat(1, F, 1)\n",
        "    x[:, 1:, :] = x[:, 1:, :] + spatial_pe\n",
        "    temporal_pe = self.temporal_pos_embedding.unsqueeze(2).repeat(1, 1, N, 1)\n",
        "    temporal_pe = temporal_pe.view(1, F*N, D)\n",
        "    x[:, 1:, :] = x[:, 1:, :] + temporal_pe\n",
        "    return x\n",
        "\n",
        "class DividedSpaceTimeAttention(nn.Module):\n",
        "  def __init__(self, dim, num_heads, num_frames, num_patches, dropout=0.0):\n",
        "    super(DividedSpaceTimeAttention, self).__init__()\n",
        "    self.num_frames = num_frames\n",
        "    self.num_patches = num_patches\n",
        "    self.dim = dim\n",
        "    # 1. Temporal Attention Layer\n",
        "    self.temporal_attn = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, dropout=dropout, batch_first=True)\n",
        "    # 2. Spatial Attention Layer\n",
        "    self.spatial_attn = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, dropout=dropout, batch_first=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, N, D = x.shape\n",
        "    cls_token = x[:, 0:1, :] # (B, 1, D)\n",
        "    patch_tokens = x[:, 1:, :] # (B, T, D)\n",
        "    patch_tokens = patch_tokens.reshape(B, self.num_frames, self.num_patches, D)\n",
        "\n",
        "    # Temporal Attention\n",
        "    # Shape change: (B, F, P, D) -> (B, P, F, D) -> (B * P, F, D)\n",
        "    xt = patch_tokens.permute(0, 2, 1, 3).contiguous().view(B * self.num_patches, self.num_frames, D)\n",
        "    cls_t = cls_token.repeat(1, self.num_patches, 1).view(B * self.num_patches, 1, D)\n",
        "    xt = torch.cat([cls_t, xt], dim=1)\n",
        "    xt, _ = self.temporal_attn(xt, xt, xt)\n",
        "\n",
        "    # Recover original shapes\n",
        "    cls_t_out = xt[:, 0:1, :] # (B*P, 1, D)\n",
        "    xt_out = xt[:, 1:, :]     # (B*P, F, D)\n",
        "\n",
        "    # Average CLS token\n",
        "    cls_token = cls_t_out.view(B, self.num_patches, 1, D).mean(dim=1)\n",
        "\n",
        "    patch_tokens = xt_out.view(B, self.num_patches, self.num_frames, D).permute(0, 2, 1, 3)\n",
        "\n",
        "    # Spatial Attention\n",
        "    xs = patch_tokens.reshape(B * self.num_frames, self.num_patches, D)\n",
        "    cls_s = cls_token.repeat(1, self.num_frames, 1).view(B * self.num_frames, 1, D)\n",
        "    xs = torch.cat([cls_s, xs], dim=1)\n",
        "    xs, _ = self.spatial_attn(xs, xs, xs)\n",
        "    cls_s_out = xs[:, 0:1, :] # (B*F, 1, D)\n",
        "    xs_out = xs[:, 1:, :]     # (B*F, P, D)\n",
        "\n",
        "    cls_token = cls_s_out.view(B, self.num_frames, 1, D).mean(dim=1)\n",
        "    patch_tokens = xs_out.view(B, self.num_frames, self.num_patches, D).flatten(1, 2)\n",
        "    return torch.cat([cls_token, patch_tokens], dim=1)\n",
        "\n",
        "class TimesformerBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads, num_frames, num_patches, mlp_ratio=4., drop=0.0, attn_drop=0.0):\n",
        "        super(TimesformerBlock, self).__init__()\n",
        "\n",
        "        # --- Normalization Layers (Step 3) ---\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "\n",
        "        # --- Attention Layer (Step 4) ---\n",
        "        self.attn = DividedSpaceTimeAttention(\n",
        "            dim=dim,\n",
        "            num_heads=num_heads,\n",
        "            num_frames=num_frames,\n",
        "            num_patches=num_patches,\n",
        "            dropout=attn_drop\n",
        "        )\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, 4*dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(drop),\n",
        "            nn.Linear(4*dim, dim),\n",
        "            nn.Dropout(drop)\n",
        "        )\n",
        "\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        shortcut = x\n",
        "        x_norm = self.norm1(x)\n",
        "        attn_out = self.attn(x_norm)\n",
        "\n",
        "        # 3. Add Residual (Skip Connection)\n",
        "        x = shortcut + attn_out\n",
        "        shortcut = x\n",
        "        x_norm = self.norm2(x)\n",
        "\n",
        "        mlp_out = self.mlp(x_norm)\n",
        "\n",
        "        x = shortcut + mlp_out\n",
        "        return x\n",
        "\n",
        "class SignTimeSformer(nn.Module):\n",
        "  # L=amount of encoder blocks\n",
        "  def __init__(self, num_classes, img_size=224, num_frames=12, L=20, heads=12, dropout=0.1,):\n",
        "      super(SignTimeSformer, self).__init__()\n",
        "\n",
        "      self.patcher = PatchEmbedVideo(img_size=img_size)\n",
        "      embed_dim = self.patcher.embed_dim\n",
        "      self.time_spatial_tokens = TimeSpatialTokens(\n",
        "          num_frames=num_frames,\n",
        "          num_patches=self.patcher.num_patches,\n",
        "          embed_dim=embed_dim\n",
        "      )\n",
        "      self.blocks = nn.ModuleList(\n",
        "        [\n",
        "            TimesformerBlock(\n",
        "                dim=embed_dim,\n",
        "                num_heads=heads,\n",
        "                num_frames=num_frames,\n",
        "                num_patches=self.patcher.num_patches,\n",
        "            )\n",
        "            for _ in range(L)\n",
        "        ]\n",
        "      )\n",
        "\n",
        "      self.layer_norm = nn.LayerNorm(embed_dim)\n",
        "      self.layer_norm_mlp = nn.LayerNorm(embed_dim)\n",
        "\n",
        "      self.norm = nn.LayerNorm(embed_dim)\n",
        "      self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "        # Patching: (batch, T, C, H, W) → (batch, T, feature_dim)\n",
        "    x = self.patcher(x) # (batch, C, T, H, W) → (batch, T, feature_dim)\n",
        "    x = self.time_spatial_tokens(x) # (batch, T, feature_dim) -> (B, 1+T, feature_dim)\n",
        "\n",
        "    for block in self.blocks:\n",
        "      x = block(x)\n",
        "    x = self.norm(x)\n",
        "    cls_token_out = x[:, 0] # extract CLS\n",
        "    logits = self.head(cls_token_out)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "mtyvIN2NUGkQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srRI42gq-lmk"
      },
      "source": [
        "## 6. Training Utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6UWY8AUW-lmk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d23107a6-26ee-4cfc-fc1f-99dee233b114"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3302298169.py:2: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        }
      ],
      "source": [
        "from torch.cuda.amp import autocast, GradScaler\n",
        "scaler = GradScaler()\n",
        "\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    \"\"\"Train the model for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
        "    for frames, labels in progress_bar:\n",
        "        frames = frames.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # print(frames.shape)\n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        with autocast():\n",
        "          outputs = model(frames)\n",
        "          loss = criterion(outputs, labels)\n",
        "        # Backward pass\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        # loss.backward()\n",
        "        # optimizer.step()\n",
        "\n",
        "        # Statistics\n",
        "        running_loss += loss.item() * frames.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        progress_bar.set_postfix({\n",
        "            'loss': loss.item(),\n",
        "            'acc': 100 * correct / total\n",
        "        })\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = 100 * correct / total\n",
        "\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    \"\"\"Evaluate the model on a dataset.\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for frames, labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            frames = frames.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            with autocast():\n",
        "                outputs = model(frames)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * frames.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = 100 * correct / total\n",
        "\n",
        "    return epoch_loss, epoch_acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFBEBTg4-lml"
      },
      "source": [
        "## 7. Configuration and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "kPsZzTQd-lml"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# CONFIGURATION - Modify these paths and hyperparameters\n",
        "# ============================================\n",
        "\n",
        "# Data paths\n",
        "JSON_PATH = \"/content/drive/MyDrive/wlasl_resized/wlasl-complete/WLASL_v0.3.json\"  # Path to WLASL JSON\n",
        "VIDEO_DIR = \"/content/drive/MyDrive/wlasl_resized/wlasl-complete/videos\"  # Path to video directory\n",
        "\n",
        "# Model hyperparameters\n",
        "NUM_FRAMES = 8          # Number of frames to sample from each video\n",
        "FEATURE_DIM = 512        # CNN feature dimension\n",
        "HIDDEN_DIM = 256         # LSTM hidden dimension\n",
        "NUM_LSTM_LAYERS = 2      # Number of LSTM layers\n",
        "DROPOUT = 0.1            # Dropout rate\n",
        "\n",
        "# Training hyperparameters\n",
        "BATCH_SIZE = 5           # Batch size (adjust based on GPU memory)\n",
        "LEARNING_RATE = 1e-4     # Learning rate\n",
        "NUM_EPOCHS = 30          # Number of training epochs\n",
        "WEIGHT_DECAY = 1e-4      # L2 regularization\n",
        "IMG_SIZE=224\n",
        "# Options\n",
        "FREEZE_CNN = False       # Whether to freeze CNN backbone\n",
        "PRETRAINED_CNN = True    # Use pretrained CNN weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "l39WEIDb-lml"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import v2\n",
        "# Data transforms for training and validation\n",
        "# train_transform = transforms.Compose([\n",
        "#     v2.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "#     # v2.RandomHorizontalFlip(),\n",
        "#     # v2.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "#     v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]),\n",
        "# ])\n",
        "\n",
        "# val_transform = transforms.Compose([\n",
        "#     v2.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "#     v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]),\n",
        "# ])\n",
        "\n",
        "from torchvision.transforms import Compose\n",
        "import pytorchvideo.transforms as ptv_transforms\n",
        "from pytorchvideo.transforms import functional as ptv_functional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# --- Parameters ---\n",
        "MEAN = [0.45, 0.45, 0.45] # Common means for ImageNet-trained models (Adjust if needed)\n",
        "STD = [0.225, 0.225, 0.225] # Common STDs for ImageNet-trained models (Adjust if needed)\n",
        "NUM_FRAMES = 32 # The number of frames you sample in your DataLoader\n",
        "\n",
        "# Note: The transforms below expect the video tensor to be in the range [0.0, 1.0]\n",
        "# and of shape (T, C, H, W). The `WLASLTorchCodec` implementation already ensures\n",
        "# the shape is (T, C, H, W), but you must ensure the pixel values are converted\n",
        "# to float and normalized to [0, 1] before applying the standard normalization.\n",
        "\n",
        "\n",
        "train_transform = Compose(\n",
        "    [\n",
        "        # 1. Spatial Resize: Scale the shortest edge to SIDE_SIZE\n",
        "        ptv_transforms.UniformTemporalSubsample(num_samples=NUM_FRAMES, temporal_dim=0),\n",
        "        ptv_transforms.ConvertUint8ToFloat(),\n",
        "        ptv_transforms.ShortSideScale(size=IMG_SIZE),\n",
        "\n",
        "        # 2. Random Crop: Take a random CROP_SIZE x CROP_SIZE crop\n",
        "        # ptv_transforms.RandomCrop(size=112),\n",
        "        # 3. Horizontal Flip (Data Augmentation)\n",
        "        ptv_transforms.RandAugment(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "test_transform = Compose(\n",
        "    [\n",
        "        ptv_transforms.UniformTemporalSubsample(num_samples=NUM_FRAMES, temporal_dim=0),\n",
        "        ptv_transforms.ConvertUint8ToFloat(),\n",
        "        ptv_transforms.ShortSideScale(size=IMG_SIZE),\n",
        "    ]\n",
        ")\n",
        "val_transform =test_transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnohaoY9-lml",
        "outputId": "a1268d90-7789-46a5-a95c-3f1d078e3914"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Will download: True\n",
            "Downloaded at path:  /kaggle/input/wlasl2000-resized\n",
            "Limiting dataset to top 10 classes.\n",
            "Will download: True\n",
            "Downloaded at path:  /kaggle/input/wlasl2000-resized\n",
            "Limiting dataset to top 10 classes.\n",
            "Will download: True\n",
            "Downloaded at path:  /kaggle/input/wlasl2000-resized\n",
            "Limiting dataset to top 10 classes.\n",
            "Number of training samples: 200\n",
            "Number of validation samples: 45\n",
            "Number of test samples: 35\n",
            "Number of classes: 10\n"
          ]
        }
      ],
      "source": [
        "# Create datasets\n",
        "train_dataset = WLASLTorchCodec(\n",
        "    download=True,\n",
        "    split=\"train\",\n",
        "    max_classes=10,\n",
        "    num_frames=NUM_FRAMES,\n",
        "    transform=train_transform\n",
        ")\n",
        "\n",
        "val_dataset = WLASLTorchCodec(\n",
        "    download=True,\n",
        "    split=\"val\",\n",
        "    max_classes=10,\n",
        "    num_frames=NUM_FRAMES,\n",
        "    transform=val_transform\n",
        ")\n",
        "\n",
        "test_dataset = WLASLTorchCodec(\n",
        "    download=True,\n",
        "    split=\"test\",\n",
        "    max_classes=10,\n",
        "    num_frames=NUM_FRAMES,\n",
        "    transform=val_transform\n",
        ")\n",
        "\n",
        "# Get number of classes from dataset\n",
        "NUM_CLASSES = train_dataset.num_classes\n",
        "\n",
        "print(f\"Number of training samples: {len(train_dataset)}\")\n",
        "print(f\"Number of validation samples: {len(val_dataset)}\")\n",
        "print(f\"Number of test samples: {len(test_dataset)}\")\n",
        "print(f\"Number of classes: {NUM_CLASSES}\")\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=8,           # Start high. The optimal value is often 4 to 12.\n",
        "                             # Since video decoding is CPU-heavy, 8 is a good starting point.\n",
        "    pin_memory=True,         # Accelerates the transfer of data from CPU to GPU VRAM.\n",
        "    persistent_workers=True, # Recommended for PyTorch multi-process workers to save epoch setup time.\n",
        "    prefetch_factor=2\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=8,           # Start high. The optimal value is often 4 to 12.\n",
        "                             # Since video decoding is CPU-heavy, 8 is a good starting point.\n",
        "    pin_memory=True,         # Accelerates the transfer of data from CPU to GPU VRAM.\n",
        "    persistent_workers=True, # Recommended for PyTorch multi-process workers to save epoch setup time.\n",
        "    prefetch_factor=2\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=8,           # Start high. The optimal value is often 4 to 12.\n",
        "                             # Since video decoding is CPU-heavy, 8 is a good starting point.\n",
        "    pin_memory=True,         # Accelerates the transfer of data from CPU to GPU VRAM.\n",
        "    persistent_workers=True, # Recommended for PyTorch multi-process workers to save epoch setup time.\n",
        "    prefetch_factor=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abGiXTmk-lml",
        "outputId": "77133656-43e8-4b1a-c87b-b2b13030b257"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 240MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total parameters: 14,595,915\n",
            "Trainable parameters: 14,595,915\n"
          ]
        }
      ],
      "source": [
        "# Initialize model\n",
        "# model = Sign2TextModel(\n",
        "#     num_classes=NUM_CLASSES,\n",
        "#     feature_dim=FEATURE_DIM,\n",
        "#     hidden_dim=HIDDEN_DIM,\n",
        "#     num_lstm_layers=NUM_LSTM_LAYERS,\n",
        "#     dropout=DROPOUT,\n",
        "#     pretrained_cnn=PRETRAINED_CNN,\n",
        "#     freeze_cnn=FREEZE_CNN,\n",
        "#     max_frames=NUM_FRAMES\n",
        "# ).to(device)\n",
        "\n",
        "model = SignTimeSformer(\n",
        "    num_classes=NUM_CLASSES,\n",
        "    img_size=IMG_SIZE,\n",
        "    num_frames=NUM_FRAMES,\n",
        "    heads=8,\n",
        "    L=5,\n",
        "    dropout=0\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW( model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY )\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau( optimizer, mode='min', factor=0.5, patience=3 )\n",
        "\n",
        "# Print model summary\n",
        "# print(model)\n",
        "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbpkZSb_-lmm"
      },
      "source": [
        "## 8. Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "id": "0jbK9sCf-lmm",
        "outputId": "59527da6-87e8-4969-dd00-c342d74a53f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/30\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   0%|          | 0/40 [00:05<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "DataLoader worker (pid(s) 2896, 2897, 2898, 2899, 2900, 2901, 2906, 2907) exited unexpectedly",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1276\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mEmpty\u001b[0m: ",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2484656764.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Validate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3302298169.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprogress_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    489\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_reset\u001b[0;34m(self, loader, first_iter)\u001b[0m\n\u001b[1;32m   1252\u001b[0m             \u001b[0mresume_iteration_cnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_workers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mresume_iteration_cnt\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m                 \u001b[0mreturn_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ResumeIteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m                     \u001b[0;32massert\u001b[0m \u001b[0mreturn_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1434\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1435\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1286\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m                 \u001b[0mpids_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1288\u001b[0;31m                 raise RuntimeError(\n\u001b[0m\u001b[1;32m   1289\u001b[0m                     \u001b[0;34mf\"DataLoader worker (pid(s) {pids_str}) exited unexpectedly\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m                 ) from e\n",
            "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 2896, 2897, 2898, 2899, 2900, 2901, 2906, 2907) exited unexpectedly"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "best_val_acc = 0.0\n",
        "history = {\n",
        "    'train_loss': [], 'train_acc': [],\n",
        "    'val_loss': [], 'val_acc': []\n",
        "}\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Train\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "\n",
        "    # Validate\n",
        "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "    # Update scheduler\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Save history\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_acc': val_acc,\n",
        "            'label_map': train_dataset.label_map\n",
        "        }, 'best_model.pth')\n",
        "        print(f\"✓ Saved new best model with Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "print(f\"\\nTraining complete! Best Val Acc: {best_val_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0_AbauW-lmm"
      },
      "source": [
        "## 9. Evaluation and Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxMS0YFc-lmm"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training history\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Loss plot\n",
        "axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
        "axes[0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Training and Validation Loss')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True)\n",
        "\n",
        "# Accuracy plot\n",
        "axes[1].plot(history['train_acc'], label='Train Acc', marker='o')\n",
        "axes[1].plot(history['val_acc'], label='Val Acc', marker='s')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Accuracy (%)')\n",
        "axes[1].set_title('Training and Validation Accuracy')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_history.png', dpi=150)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5fEQd22-lmm"
      },
      "outputs": [],
      "source": [
        "# Load best model and evaluate on test set\n",
        "checkpoint = torch.load('best_model.pth')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
        "print(f\"\\nTest Results:\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK5qG01d-lmn"
      },
      "source": [
        "## 10. Attention Visualization\n",
        "\n",
        "Visualize which frames the model attends to most when making predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FY2JOSmH-lmn"
      },
      "outputs": [],
      "source": [
        "def visualize_attention(model, frames, true_label, label_map, device):\n",
        "    \"\"\"Visualize attention weights over video frames.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Get reverse label map\n",
        "    idx_to_label = {v: k for k, v in label_map.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Add batch dimension\n",
        "        frames_batch = frames.unsqueeze(0).to(device)\n",
        "\n",
        "        # Get predictions and attention weights\n",
        "        logits, attention_weights = model(frames_batch, return_attention=True)\n",
        "        pred_label = torch.argmax(logits, dim=1).item()\n",
        "        attention = attention_weights[0].cpu().numpy()\n",
        "\n",
        "    # Create visualization\n",
        "    num_frames = frames.shape[0]\n",
        "    fig, axes = plt.subplots(2, num_frames, figsize=(2 * num_frames, 6))\n",
        "\n",
        "    # Denormalize frames for visualization\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "\n",
        "    for i in range(num_frames):\n",
        "        frame = frames[i].cpu()\n",
        "        frame = frame * std + mean\n",
        "        frame = frame.clamp(0, 1).permute(1, 2, 0).numpy()\n",
        "\n",
        "        # Frame image\n",
        "        axes[0, i].imshow(frame)\n",
        "        axes[0, i].set_title(f\"Frame {i+1}\")\n",
        "        axes[0, i].axis('off')\n",
        "\n",
        "        # Attention weight bar\n",
        "        axes[1, i].bar([0], [attention[i]], color='blue', alpha=0.7)\n",
        "        axes[1, i].set_ylim(0, max(attention) * 1.2)\n",
        "        axes[1, i].set_title(f\"{attention[i]:.3f}\")\n",
        "        axes[1, i].axis('off')\n",
        "\n",
        "    plt.suptitle(\n",
        "        f\"True: {idx_to_label.get(true_label, true_label)} | \"\n",
        "        f\"Predicted: {idx_to_label.get(pred_label, pred_label)}\",\n",
        "        fontsize=14\n",
        "    )\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return pred_label, attention\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pOEoPik-lmn"
      },
      "outputs": [],
      "source": [
        "# Visualize attention for a sample from the test set\n",
        "sample_idx = 0\n",
        "frames, label = test_dataset[sample_idx]\n",
        "pred, attn = visualize_attention(model, frames, label, train_dataset.label_map, device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1vGI8IA-lmn"
      },
      "source": [
        "## 11. Inference Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CXDTtOc-lmn"
      },
      "outputs": [],
      "source": [
        "def predict_video(model, video_path, transform, num_frames, label_map, device):\n",
        "    \"\"\"Predict the sign language class for a video file.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Get reverse label map\n",
        "    idx_to_label = {v: k for k, v in label_map.items()}\n",
        "\n",
        "    # Decode video\n",
        "    decoder = VideoDecoder(video_path)\n",
        "    frames = []\n",
        "\n",
        "    for chunk in decoder:\n",
        "        for frame_tensor in chunk:\n",
        "            if frame_tensor.dim() == 2:\n",
        "                frame_tensor = frame_tensor.unsqueeze(2)\n",
        "            frame_chw = frame_tensor.permute(2, 0, 1)\n",
        "            frame_pil = transforms.ToPILImage()(frame_chw)\n",
        "            frames.append(frame_pil)\n",
        "\n",
        "    # Handle short videos\n",
        "    while len(frames) < num_frames:\n",
        "        frames.extend(frames)\n",
        "\n",
        "    # Sample frames\n",
        "    T = len(frames)\n",
        "    idx = torch.linspace(0, T - 1, num_frames).long()\n",
        "    frames = [frames[i] for i in idx]\n",
        "\n",
        "    # Apply transforms\n",
        "    frames = torch.stack([transform(f) for f in frames])\n",
        "\n",
        "    # Predict\n",
        "    with torch.no_grad():\n",
        "        frames_batch = frames.unsqueeze(0).to(device)\n",
        "        logits, attention = model(frames_batch, return_attention=True)\n",
        "        probabilities = F.softmax(logits, dim=1)\n",
        "        pred_idx = torch.argmax(logits, dim=1).item()\n",
        "        confidence = probabilities[0, pred_idx].item()\n",
        "\n",
        "    predicted_label = idx_to_label.get(pred_idx, f\"Unknown ({pred_idx})\")\n",
        "\n",
        "    return {\n",
        "        'prediction': predicted_label,\n",
        "        'confidence': confidence,\n",
        "        'attention_weights': attention[0].cpu().numpy(),\n",
        "        'all_probabilities': probabilities[0].cpu().numpy()\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SNQvsPo-lmn"
      },
      "outputs": [],
      "source": [
        "# Example inference (uncomment and modify path to use)\n",
        "# result = predict_video(\n",
        "#     model=model,\n",
        "#     video_path=\"/path/to/your/video.mp4\",\n",
        "#     transform=val_transform,\n",
        "#     num_frames=NUM_FRAMES,\n",
        "#     label_map=train_dataset.label_map,\n",
        "#     device=device\n",
        "# )\n",
        "# print(f\"Prediction: {result['prediction']}\")\n",
        "# print(f\"Confidence: {result['confidence']:.2%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Srt60SNL-lmn"
      },
      "source": [
        "## 12. Save Final Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpDygB68-lmo"
      },
      "outputs": [],
      "source": [
        "# Save complete model for deployment\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'label_map': train_dataset.label_map,\n",
        "    'config': {\n",
        "        'num_classes': NUM_CLASSES,\n",
        "        'feature_dim': FEATURE_DIM,\n",
        "        'hidden_dim': HIDDEN_DIM,\n",
        "        'num_lstm_layers': NUM_LSTM_LAYERS,\n",
        "        'num_frames': NUM_FRAMES,\n",
        "        'dropout': DROPOUT\n",
        "    }\n",
        "}, 'sign2text_model_final.pth')\n",
        "\n",
        "print(\"Model saved to sign2text_model_final.pth\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7qt1Yws-lmo"
      },
      "source": [
        "---\n",
        "**Note:** The cells above contain the complete implementation. Make sure to run them in order from top to bottom.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6VTtt4e-lmo"
      },
      "outputs": [],
      "source": [
        "# PositionalEncoding class is defined below cell 7 - this cell can be ignored\n",
        "# The model requires running cells in sequential order\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}